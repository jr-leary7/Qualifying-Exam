\documentclass{report}

% margins
\usepackage[letterpaper, top=2cm, bottom=2cm, left=2cm, right=2cm, marginparwidth=1.75cm]{geometry}
% insertion of images
\usepackage{graphicx}
% image resizing to fit page dimensions as needed
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
\setkeys{Gin}{width=\maxwidth, height=\maxheight, keepaspectratio}
\makeatletter
\def\fps@figure{htbp}
\makeatother
% math typesetting -- requires XeLaTeX instead of pdfLaTeX
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{unicode-math}
% algorithm blocks
\usepackage{algorithm}
\usepackage{algorithmic}
% code blocks with syntax highlighting
\usepackage{minted}
% number source code listings & algorithms by chapter
\usepackage{chngcntr}
% change listing title from "Listing" to "Source Code"
\renewcommand{\listingscaption}{Source Code}
% correctly format captions
\usepackage[labelfont=bf, labelsep=period]{caption}
% place page number in top left header & section title in top right
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\thepage}
\fancyhead[R]{\textit\rightmark}
% set indents to 0 and include extra space between paragraphs
\usepackage{parskip}
% references
\usepackage[backend=biber, citestyle=authoryear-comp, bibstyle=numeric]{biblatex}
\addbibresource{references.bib}
% add notation appendix to table of contents
\usepackage[toc]{appendix}
% table row height
\renewcommand{\arraystretch}{1.5}
% combine rows in tables
\usepackage{multirow}
% links
\usepackage[colorlinks]{hyperref}
% create list of acronyms for appendix -- must be loaded *after* hyperref for links to work 
\usepackage[acronym, nopostdot, toc, section, numberedsection=autolabel, nogroupskip, symbols]{glossaries-extra}
\glsdisablehyper
% format glossaries correctly based on width of longest term 
\usepackage{glossary-tree}
\makeglossaries
% make sure first instance of an acronym includes its definition
\setabbreviationstyle[acronym]{long-short}

% list of all acronyms used for glossary
\newacronym{rv}{RV}{random variable}
\newacronym{iid}{IID}{independent and identically distributed}
\newacronym{cdf}{CDF}{cumulative distribution function}
\newacronym{pmf}{PMF}{probability mass function}
\newacronym{pdf}{PDF}{probability density function}
\newacronym{mgf}{MGF}{moment-generating function}
\newacronym{iff}{IFF}{if and only if}
\newacronym{em}{EM}{expectation-maximization}
\newacronym{umvue}{UMVUE}{unique minimum variance unbiased estimator}
\newacronym{ump}{UMP}{uniformly most powerful}
\newacronym{ols}{OLS}{ordinary least-squares}
\newacronym{gls}{GLS}{generalized least-squares}
\newacronym{wls}{WLS}{weighted least-squares}
\newacronym{ss}{SS}{sum-of-squares}
\newacronym{rss}{RSS}{residual sum-of-squares}
\newacronym{tss}{TSS}{total sum-of-squares}
\newacronym{mse}{MSE}{mean-square error}
\newacronym{df}{DF}{degrees of freedom}
\newacronym{se}{SE}{standard error}
\newacronym{blue}{BLUE}{best linear unbiased estimator}
\newacronym{ci}{CI}{confidence interval}
\newacronym{mht}{MHT}{multiple hypothesis testing}
\newacronym{lasso}{LASSO}{least absolute shrinkage and selection operator}
\newacronym{gcv}{GCV}{generalized cross-validation}
\newacronym{mle}{MLE}{maximum likelihood estimate}
\newacronym{bma}{BMA}{Bayesian model averaging}
\newacronym{reml}{REML}{restricted maximum likelihood}
\newacronym{lmm}{LMM}{linear mixed model}
\newacronym{lrt}{LRT}{likelihood ratio test}
\newacronym{rlrt}{RLRT}{restricted likelihood ratio test}
\newacronym{mcmc}{MCMC}{Markov Chain Monte Carlo}
\newacronym{blup}{BLUP}{best linear unbiased predictor}
\newacronym{nls}{NLS}{nonlinear least-squares}
\newacronym{glm}{GLM}{generalized linear model}
\newacronym{irwls}{IRWLS}{iteratively-reweighted least-squares}
\newacronym{or}{OR}{odds ratio}
\newacronym{ic}{IC}{information criterion}
\newacronym{bic}{BIC}{Bayesian Information Criterion}
\newacronym{aic}{AIC}{Akaike Information Criterion}
\newacronym{dic}{DIC}{Deviance Information Criterion}
\newacronym{gee}{GEE}{generalized estimating equation}
\newacronym{ar}{AR}{autoregressive}
\newacronym{qic}{QIC}{quasi-likelihood under the independence model criterion}
\newacronym{glmm}{GLMM}{generalized linear mixed model}
\newacronym{agq}{AGQ}{adaptive Gauss-Hermite quadrature}
\newacronym{pql}{PQL}{penalized quasi-likelihood}
\newacronym{gam}{GAM}{generalized additive model}
\newacronym{edf}{EDF}{effective degrees of freedom}
\newacronym{if}{IF}{influence function}
\newacronym{wlog}{WLOG}{without loss of generality}
\newacronym{ui}{UI}{uniformly integrable}

% list of all symbols used for glossary
\glsxtrnewsymbol[description={Sample space of a probability space}]{sample-space}{\ensuremath{\Omega}}
\glsxtrnewsymbol[description={Sigma-algebra of a probability space}]{sigma-algebra}{\ensuremath{\mathcal{B}}}
\glsxtrnewsymbol[description={Probability function of a probability space}]{prob-function}{\ensuremath{\mathcal{P}}}
\glsxtrnewsymbol[description={The support of random variable \ensuremath{X}}]{support}{\ensuremath{\mathcal{X}}}
\glsxtrnewsymbol[description={The set of real numbers}]{reals}{\ensuremath{\mathbb{R}}}
\glsxtrnewsymbol[description={Derivative of \ensuremath{f} with respect to \ensuremath{x}}]{derivative}{\ensuremath{f^\prime(x)}}
\glsxtrnewsymbol[description={Partial derivative of \ensuremath{f} with respect to \ensuremath{x}}]{partial-derivative}{\ensuremath{\frac{\partial f(x, y)}{\partial x}}}
\glsxtrnewsymbol[description={Natural logarithm of \ensuremath{x}}]{natural-log}{\ensuremath{\log(x)}}
\glsxtrnewsymbol[description={Overall sample size}]{sample-size}{\ensuremath{n}}
\glsxtrnewsymbol[description={Index of observations or random variables (depending on context)}]{index}{\ensuremath{i = 1, \dots, n}}
\glsxtrnewsymbol[description={Transpose of matrix \ensuremath{A}}]{transpose}{\ensuremath{A^\intercal}}
\glsxtrnewsymbol[description={Inverse or pseudo-inverse of matrix \ensuremath{A}}]{inverse}{\ensuremath{A^{-1}}}
\glsxtrnewsymbol[description={Design matrix in a regression model}]{design-matrix}{\ensuremath{X}}
\glsxtrnewsymbol[description={Index of covariates in a regression model}]{index-covariates}{\ensuremath{j = 1, \dots, p}}
\glsxtrnewsymbol[description={Response variable in a regression model}]{regress-response}{\ensuremath{y}}
\glsxtrnewsymbol[description={Error term in a regression model}]{regress-error}{\ensuremath{\epsilon}}
\glsxtrnewsymbol[description={Dispersion parameter in a regression model}]{regress-dispersion}{\ensuremath{\phi}}
\glsxtrnewsymbol[description={True coefficients in a regression model}]{regress-coefs}{\ensuremath{\beta}}
\glsxtrnewsymbol[description={Link function in a generalized linear (mixed) model}]{regress-link}{\ensuremath{g}}
\glsxtrnewsymbol[description={Residual for observation \ensuremath{i} in a regression model}]{regress-residual}{\ensuremath{r_i}}
\glsxtrnewsymbol[description={Weight for observation \ensuremath{i} in a regression model}]{regress-weight}{\ensuremath{w_i}}
\glsxtrnewsymbol[description={The projection or hat matrix in a regression model}]{regress-hat-matrix}{\ensuremath{H}}
\glsxtrnewsymbol[description={The leverage for observation \ensuremath{i} in a regression model}]{regress-leverage}{\ensuremath{h_i}}
\glsxtrnewsymbol[description={Maximum likelihood estimate of \ensuremath{\theta} given \ensuremath{n} observations}]{mle-symbol}{\ensuremath{\hat{\theta}_n}}
\glsxtrnewsymbol[description={Sample mean of random variable \ensuremath{X} given \ensuremath{n} observations}]{sample-mean}{\ensuremath{\bar{X}_n}}
\glsxtrnewsymbol[description={Unbiased sample variance of a random variable given \ensuremath{n} observations}]{sample-variance-unbiased}{\ensuremath{\hat{s}_n^2}}
\glsxtrnewsymbol[description={Cumulative distribution function for random variable \ensuremath{X}}]{cdf-symbol}{\ensuremath{F_X}}
\glsxtrnewsymbol[description={The probability mass or density function of random variable \ensuremath{Y} given parameter(s) \ensuremath{\theta}}]{pdf-or-pmf}{\ensuremath{f_Y(y|\theta)}}
\glsxtrnewsymbol[description={Mean of a random variable}]{mu}{\ensuremath{\mu}}
\glsxtrnewsymbol[description={Variance of a random variable}]{sigma-squared}{\ensuremath{\sigma^2}}
\glsxtrnewsymbol[description={Expectation operator}]{expectation}{\ensuremath{\mathbb{E}}}
\glsxtrnewsymbol[description={Probability operator}]{probability}{\ensuremath{\mathbb{P}}}
\glsxtrnewsymbol[description={Likelihood function of \ensuremath{\theta}}]{likelihood}{\ensuremath{\mathcal{L}(\theta)}}
\glsxtrnewsymbol[description={Log-likelihood function of \ensuremath{\theta}}]{log-likelihood}{\ensuremath{\mathcal{l}(\theta)}}
\glsxtrnewsymbol[description={Score function of \ensuremath{\theta} (usually set equal to 0 and solved for \ensuremath{\theta})}]{score-function}{\ensuremath{\mathcal{S}(\theta) = \frac{\partial\mathcal{l}(\theta)}{\partial\theta}}}
\glsxtrnewsymbol[description={Fisher's information matrix for \ensuremath{\theta}}]{fisher-information}{\ensuremath{\mathcal{I}(\theta)}}
\glsxtrnewsymbol[description={Posterior distribution of \ensuremath{\theta} given observed data \ensuremath{y}}]{posterior-dist}{\ensuremath{p(\theta|y)}}
\glsxtrnewsymbol[description={Prior distribution for \ensuremath{\theta} given optional hyperparameters \ensuremath{\lambda}}]{prior-dist}{\ensuremath{\pi(\theta|\lambda)}}
\glsxtrnewsymbol[description={Jacobian of function \ensuremath{f} with respect to \ensuremath{\theta}}]{jacobian}{\ensuremath{\mathfrak{J}_f(\theta)}}
\glsxtrnewsymbol[description={\ensuremath{k^{\text{th}}} raw moment of a random variable}]{raw-moment}{\ensuremath{m_k}}
\glsxtrnewsymbol[description={\ensuremath{k^{\text{th}}} central moment of a random variables}]{central-moment}{\ensuremath{\mu_k}}
\glsxtrnewsymbol[description={Denotes removal of the \ensuremath{i^{\text{th}}} observation, the \ensuremath{i^{\text{th}}} derivative, or iteration \ensuremath{i} (depending on context)}]{i-removal}{\ensuremath{(i)}}
\glsxtrnewsymbol[description={A Wald test statistic}]{wald-test-stat}{\ensuremath{Z}}
\glsxtrnewsymbol[description={Pearson's \ensuremath{\chi}-squared test statistic}]{chi-squared-test-stat}{\ensuremath{\chi^2}}

\title{\Huge \textbf{Qualifying Exam Study Guide} \\ ~ \\ \large University of Florida \\ Department of Biostatistics}
\author{Jack R. Leary \\ \href{mailto:j.leary@ufl.edu}{j.leary@ufl.edu}}

\begin{document}

% enumerate code blocks and algorithms within each chapter like is done for tables & figures
\counterwithin{listing}{chapter}
\counterwithin{algorithm}{chapter}

\maketitle

\chapter*{Introduction}

This document is a personalized study guide for the University of Florida's Department of Biostatistics 2024 qualifying exam. The general structure is as follows: Chapter \ref{chap:required-reading} lists out references for every major topic of study, Chapter \ref{chap:tools-of-the-trade} provides formulae and notes that should be known, and Chapter \ref{chap:worked-examples} contains a variety of worked examples. Each chapter is split into three subsections, one each for linear models, generalized linear models, and large sample asymptotic theory. Appendix \ref{chap:appendix} contain lists of acronyms and notational conventions that I use throughout. Lastly, Appendix \ref{chap:software} contains a list of useful software packages beyond those used for modeling. All references used in creating the content of the guide are cited, and are listed on the final pages. I have attempted to use an even mix of original sources, course textbooks, lecture notes, and recently-published articles. In the special case of lecture notes, I cite the relevant professor as the author, and can provide the original notes upon request. The Zotero library containing all the sources can be accessed \href{https://www.zotero.org/groups/5529617/qualifying-exam/library}{here}. When referencing R and other software packages, the first occurrence of the package name includes a link to the source code (whether on CRAN or elsewhere). I have also included a series of R code examples with the goal of showing how to apply the theoretical results discussed throughout. The R code used in the examples is included in the Overleaf project directory under \texttt{Example-Code/}. Finally, throughout I have tried my best to simplify and make consistent the (widely) varying forms of notation used by different authors; if anything appears incorrect please feel free to contact me and I will make the necessary changes. 

\tableofcontents

% print lists of tables, algorithms, and code blocks
\renewcommand\listoflistingscaption{List of Code Examples}
\listoftables
\begingroup
\let\clearpage\relax
\listofalgorithms
\listoflistings 
\endgroup

\chapter{Required Reading} \label{chap:required-reading}

\section{Linear models}\label{sec:reading-ols}

\subsection{Maximum likelihood estimation}

\begin{itemize}
    \item \cite[Chapter~2]{faraway_linear_2015} 
    \item \cite[Chapter~2]{wakefield_bayesian_2013}
    \item \cite[Chapter~2.1]{agresti_foundations_2015}
    \item \cite[Chapter~2]{dunn_generalized_2018}
\end{itemize}

\subsection{Quasi-likelihood estimation}

\begin{itemize}
    \item \cite[Chapter~2.5]{wakefield_bayesian_2013}
\end{itemize}

\subsection{Hypothesis testing}

\begin{itemize}
    \item \cite[Chapter~3]{faraway_linear_2015}
    \item \cite[Chapter~2.9]{wakefield_bayesian_2013}
    \item \cite[Chapter~3.2]{agresti_foundations_2015}
    \item \cite[Chapter~2.8]{dunn_generalized_2018}
\end{itemize}

\subsection{Bootstrap methods}

\begin{itemize}
    \item \cite[Chapter~3.6]{faraway_linear_2015}
    \item \cite[Chapter~2.7]{wakefield_bayesian_2013}
\end{itemize}

\subsection{Bayesian linear models}

\begin{itemize}
    \item \cite[Chapter~3]{wakefield_bayesian_2013}
    \item \cite[Chapter~10.2]{agresti_foundations_2015}
\end{itemize}

\subsection{Model diagnostics}

\begin{itemize}
    \item \cite[Chapter~6]{faraway_linear_2015}
    \item \cite[Chapter~5.11]{wakefield_bayesian_2013}
    \item \cite[Chapter~2.5]{agresti_foundations_2015}
    \item \cite[Chapter~3]{dunn_generalized_2018}
\end{itemize}

\subsection{Model selection}

\begin{itemize}
    \item \cite[Chapter~10]{faraway_linear_2015}
    \item \cite[Chapter~4.8]{wakefield_bayesian_2013}
\end{itemize}

\subsection{Penalized regression methods}

\begin{itemize}
    \item \cite[Chapter~11]{faraway_linear_2015}
    \item \cite[Chapter~10.5]{wakefield_bayesian_2013}
    \item \cite[Chapter~3.4]{hastie_elements_2009}
    \item \cite[Chapter~11.1]{agresti_foundations_2015}
\end{itemize}

\subsection{Generalized least-squares}

\begin{itemize}
    \item \cite[Chapter~8.1]{faraway_linear_2015}
    \item \cite[Chapter~2.7.2]{agresti_foundations_2015}
    \item \cite[Chapter~5.6.2]{wakefield_bayesian_2013}
\end{itemize}

\section{Generalized linear models}\label{sec:reading-glm}

\subsection{Exponential family distributions}

\begin{itemize}
    \item \cite[Chapter~3.2]{dobson_introduction_2018}
    \item \cite[Chapter~8.1]{faraway_extending_2016}
    \item \cite[Chapter~4.1]{agresti_foundations_2015}
    \item \cite[Chapter~2.3]{hardin_generalized_2018}
    \item \cite[Chapter~5.3]{dunn_generalized_2018}
\end{itemize}

\subsection{Model fitting}

\begin{itemize}
    \item \cite[Chapter~8.2]{faraway_extending_2016}
    \item \cite[Chapter~2]{dobson_introduction_2018}
    \item \cite[Chapter~4.5]{agresti_foundations_2015}
    \item \cite[Chapter~3]{hardin_generalized_2018}
    \item \cite[Chapter~6]{dunn_generalized_2018}
\end{itemize}

\subsection{Model diagnostics}

\begin{itemize}
    \item \cite[Chapter~8.4]{faraway_extending_2016}
    \item \cite[Chapter~6.9]{wakefield_bayesian_2013}
    \item \cite[Chapter~4.4]{agresti_foundations_2015}
    \item \cite[Chapter~4]{hardin_generalized_2018}
    \item \cite[Chapter~8]{dunn_generalized_2018}
\end{itemize}

\subsection{Logistic regression}

\begin{itemize}
    \item \cite[Chapter~2]{faraway_extending_2016}
    \item \cite[Chapter~7]{dobson_introduction_2018}
    \item \cite[Chapter~7]{wakefield_bayesian_2013}
    \item \cite[Chapter~5]{agresti_foundations_2015}
    \item \cite[Chapters~9-11]{hardin_generalized_2018}
    \item \cite[Chapter~9]{dunn_generalized_2018}
\end{itemize}

\subsection{Count regression}

\begin{itemize}
    \item \cite[Chapter~9]{dobson_introduction_2018}
    \item \cite[Chapter~5]{faraway_extending_2016}
    \item \cite[Chapter~7]{agresti_foundations_2015}
    \item \cite[Chapters~12-14]{hardin_generalized_2018}
    \item \cite[Chapter~10]{dunn_generalized_2018}
\end{itemize}

\subsection{Nominal and ordinal regression}

\begin{itemize}
    \item \cite[Chapter~8]{dobson_introduction_2018}
    \item \cite[Chapter~6]{agresti_foundations_2015}
    \item \cite[Chapters~15-16]{hardin_generalized_2018}
\end{itemize}

\subsection{Hypothesis testing}

\begin{itemize}
    \item \cite[Chapter~8.4]{faraway_extending_2016}
    \item \cite[Chapter~6.5.3]{wakefield_bayesian_2013}
    \item \cite[Chapter~5.7]{dobson_introduction_2018}
    \item \cite[Chapter~4.3]{agresti_foundations_2015}
    \item \cite[Chapter~7]{dunn_generalized_2018}
\end{itemize}

\subsection{Generalized estimating equations}

\begin{itemize}
    \item \cite[Chapter~13.5]{faraway_extending_2016}
    \item \cite[Chapter~8.7]{wakefield_bayesian_2013}
    \item \cite[Chapter~11.4]{dobson_introduction_2018}
    \item \cite[Chapter~9.6]{agresti_foundations_2015}
    \item \cite[Chapter~18.6]{hardin_generalized_2018}
    \item \cite[Chapters~3-4]{hardin_generalized_2012}
\end{itemize}

\subsection{Generalized linear mixed models}

\begin{itemize}
    \item \cite[Chapter~11.5]{dobson_introduction_2018}
    \item \cite[Chapter~13]{faraway_extending_2016}
    \item \cite[Chapter~9]{wakefield_bayesian_2013}
    \item \cite[Chapter~9]{agresti_foundations_2015}
    \item \cite[Chapter~18.4]{hardin_generalized_2018}
\end{itemize}

\subsection{Generalized additive models}

\begin{itemize}
    \item \cite[Chapter~15]{faraway_extending_2016}
    \item \cite[Chapters~4-7]{wood_generalized_2017}
    \item \cite[Chapters~4-7]{hastie_generalized_1990}
    \item \cite[Chapter~9.1]{hastie_elements_2009}
    \item \cite{rigby_generalized_2005}
\end{itemize}

\section{Large sample theory}\label{sec:reading-lst}

\subsection{Modes of convergence}

\begin{itemize}
    \item \cite[Chapter~2.1]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~1.1]{dasgupta_asymptotic_2008}
    \item \cite[Chapter 5.5]{casella_statistical_2002}
    \item \cite[Chapter~2]{lehmann_elements_1999}
    \item \cite[Chapter~1.2-6]{serfling_approximation_1980}
\end{itemize}

\subsection{``Oh'' notation}

\begin{itemize}
    \item \cite[Chapter~2.2]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~1.4]{lehmann_elements_1999}
    \item \cite[Chapter~1.1]{serfling_approximation_1980}
\end{itemize}

\subsection{The continuous mapping theorem}

\begin{itemize}
    \item \cite[Chapter~2.1]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~1.3]{dasgupta_asymptotic_2008}
\end{itemize}

\subsection{Slutsky's theorem}

\begin{itemize}
    \item \cite[Chapter~2.1]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~1.1]{dasgupta_asymptotic_2008}
\end{itemize}

\subsection{The central limit theorem}

\begin{itemize}
    \item \cite[Chapter~2.1]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~1.3]{dasgupta_asymptotic_2008}
    \item \cite[Chapter~5.5]{casella_statistical_2002}
    \item \cite[Chapter~2.4]{lehmann_elements_1999}
    \item \cite[Chapter~1.9]{serfling_approximation_1980}
\end{itemize}

\subsection{The delta method}

\begin{itemize}
    \item \cite[Chapter~3.1]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~3.4]{dasgupta_asymptotic_2008}
    \item \cite[Chapter~5.5]{casella_statistical_2002}
    \item \cite[Chapter~2.5]{lehmann_elements_1999}
    \item \cite[Chapter~3.1]{serfling_approximation_1980}
\end{itemize}

\subsection{Quantiles}

\begin{itemize}
    \item \cite[Chapter~21]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~7]{dasgupta_asymptotic_2008}
    \item \cite[Chapter~10]{david_order_2003}
    \item \cite[Chapter~2.3]{serfling_approximation_1980}
\end{itemize}

\subsection{Influence functions}

\begin{itemize}
    \item \cite[Chapter~2.1]{hampel_robust_2011}
    \item \cite[Chapter~6.3]{lehmann_elements_1999}
\end{itemize}

\subsection{Maximum likelihood estimation and estimating equations}

\begin{itemize}
    \item \cite[Chapter~5]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapters~16-17]{dasgupta_asymptotic_2008}
    \item \cite[Chapter~10.2]{casella_statistical_2002}
    \item \cite[Chapter~2.3]{hampel_robust_2011}
    \item \cite[Chapter~7]{serfling_approximation_1980}
\end{itemize}

\subsection{U- and V-statistics}

\begin{itemize}
    \item \cite[Chapter~12]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~15]{dasgupta_asymptotic_2008}
    \item \cite[Chapter~6.1]{lehmann_elements_1999}
    \item \cite[Chapter~5]{serfling_approximation_1980}
\end{itemize}

\subsection{The jackknife}

\begin{itemize}
    \item \cite[Chapter~30]{dasgupta_asymptotic_2008}
\end{itemize}

\subsection{The bootstrap}

\begin{itemize}
    \item \cite[Chapter~23]{van_der_vaart_asymptotic_1998}
    \item \cite[Chapter~29]{dasgupta_asymptotic_2008}
    \item \cite[Chapter~10.1]{casella_statistical_2002}
\end{itemize}


\chapter{Tools of the Trade} \label{chap:tools-of-the-trade}

\section{Calculus review}\label{sec:tools-calculus}

\subsection{The fundamental theorem of calculus}

\begin{equation}\label{eq:fundamental-theorem-of-calculus}
    \int_a^b f^\prime(t) dt = f(b) - f(a)
\end{equation}

\subsection{The sum and difference rule}

\begin{equation}\label{eq:sum-and-difference-rule}
    (f(x) \pm g(x))^\prime = f^\prime(x) \pm g^\prime(x)
\end{equation}

\subsection{The product rule}

For a given function $h(x) = f(x)g(x)$ we have:

\begin{equation}\label{eq:product-rule}
    h^\prime(x) = f^\prime(x)g(x) + f(x)g^\prime(x)
\end{equation}

\subsection{The chain rule}

The chain rule states that for a given function $h(x) = f(g(x))$, where $f$ and $g$ are both differentiable, the following is true:

\begin{equation}\label{eq:chain-rule}
    h^\prime(x) = f^\prime(g(x))g^\prime(x)
\end{equation}

\subsection{Logarithmic differentiation}

The rule of logarithmic differentiation can sometimes make it easier to derive functions:

\begin{equation}\label{eq:logarithmic-differentiation}
    f^\prime(x) = f(x) \log(f(x))^\prime
\end{equation}

\subsection{The quotient rule}

The quotient rule states that the derivative of a ratio of two differentiable functions $h(x) = \frac{f(x)}{g(x)}$ is:

\begin{equation}\label{eq:quotient-rule}
    h^\prime(x) = \frac{f^\prime(x)g(x) - f(x)g^\prime(x)}{\left(g(x)\right)^2}
\end{equation}

\subsection{L'Hopital's rule}

L'Hopital's rule states the following for differentiable functions $f$ and $g$:

\begin{equation}\label{eq:lhopitals-rule}
    \lim_{x\to c} \frac{f(x)}{g(x)} = \lim_{x\to c} \frac{f^\prime(x)}{g^\prime(x)}
\end{equation}

\subsection{Differentiation under the integral sign}

The general form of this rule is as follows, for a continuous and continuously differentiable function $f$:

\begin{equation}\label{eq:diff-under-integral-general}
    \frac{d}{dx} \int_{a(x)}^{b(x)} f(x, t) dt = f(x, b(x))b^\prime(x) - f(x, a(x))a^\prime(x) + \int_{a(x)}^{b(x)} \frac{\partial}{\partial x} f(x, t) dt
\end{equation}

When $a(x)$ and $b(x)$ are constants, this simpler form holds:

\begin{equation}\label{eq:diff-under-integral-simple}
    \frac{d}{dx} \int_a^b f(x, t) dt = \int_a^b \frac{\partial}{\partial x} f(x, t) dt
\end{equation}

\subsection{Differentiation under summation}

The following is adapted from \cite[Chapter~2.4]{casella_statistical_2002}. Suppose we have a series $\sum_{x=0}^\infty h(\theta, x)$ that converges $\forall \: \theta$ in an interval $(a, b)$. If $\frac{\partial}{\partial\theta} h(\theta, x)$ is continuous in $\theta \: \forall \: x$, and $\sum_{x=0}^\infty \frac{\partial}{\partial\theta} h(\theta, x)$ converges uniformly on every closed, bounded subinterval of $(a, b)$, then:

\begin{equation}\label{eq:diff-under-summation}
    \frac{d}{d\theta} \sum_{x=0}^\infty h(\theta, x) = \sum_{x=0}^\infty \frac{\partial}{\partial\theta} h(\theta, x)
\end{equation}

\subsection{Squeeze theorem}

Suppose we have the following functions defined on an interval $I$ containing the point $a$:

\begin{equation}\label{eq:squeez-assumption-1}
    g(x) \le f(x) \le h(x)
\end{equation}

Suppose as well that:

\begin{equation}\label{eq:squeeze-assumption-2}
    \lim_{x\to a} g(x) = \lim_{x\to a} h(x) = L
\end{equation}

Then:

\begin{equation}\label{eq:squeeze}
    \lim_{x\to a} f(x) = L
\end{equation}

\subsection{Integration by parts}

Integration by parts allows us to find the integral of a product of functions; its general form is:

\begin{equation}\label{eq:integral-by-parts-general}
    \int_a^b u(x) v^\prime(x) dx = u(b)v(b) - u(a)v(a) - \int_a^b u^\prime(x) v(x) dx
\end{equation}

More compactly:

\begin{equation}\label{eq:integral-by-parts-simple}
   \int u dv = uv - \int v du 
\end{equation}

\subsection{Integration by substitution}

The technique of u-substitution allows the evaluation of definite integrals via a kind of reverse chain rule. The rule states that the following holds for a continuous function $f$ and a differentiable function $g$ that has a continuous derivative:

\begin{equation}\label{eq:integral-by-sub}
    \int_a^b f(g(x))g^\prime(x) dx = \int_{g(a)}^{g(b)} f(u) du
\end{equation}

\subsection{The binomial theorem}

\begin{equation}\label{eq:binomial-theorem}
    (x + y)^n = \sum_{k=0}^n \binom{n}{k} x^ky^{n-k}
\end{equation}

\section{Probability review}\label{sec:tools-probability}

\subsection{The probability space}

Also referred to as a probability triple, a probability space consists of 3 elements: the sample space, denoted $\Omega$, a sigma-algebra (also called a Borel field), denoted $\mathcal{B}$, and a probability function, denoted $\mathcal{P}$. 

\subsubsection{The sample space}

The sample space $\Omega$ is simply the set of all possible outcomes of an experiment. Events in $\Omega$ follow several laws. The three basic set operations - union, intersection, and complementation - are defined as follows for events $A$ and $B$ in $\Omega$:

\begin{equation}\label{eq:set-union-intersection}
    \begin{aligned}
        A \cup B &= \{x: x \in A \text{ or } x \in B\} \\
        A \cap B &= \{x: x \in A \text{ and } x \in B\} \\
        A^c &= \{x: x \notin A\}
    \end{aligned}
\end{equation}

For any three events in $\Omega$, which we'll denote $A$, $B$, and $C$, the following are true:

\begin{table}[h!]
\centering
\begin{tabular}{||l l||} 
 \hline
 \textbf{Law} & \textbf{Definition} \\ [0.5ex] 
  \hline\hline
  \multirow{2}*{Commutativity} & $A \cup B = B \cup A$ \\
  & $A \cap B = B \cap A$ \\
  \hline
  \multirow{2}*{Associativity} & $A \cup (B \cup C) = (A \cup B) \cup C$ \\
  & $A \cap (B \cap C) = (A \cap B) \cap C$ \\
  \hline
  \multirow{2}*{Distributive} & $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$ \\
  & $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$ \\
  \hline 
  \multirow{2}*{DeMorgan's} & $(A \cup B)^c = A^c \cap B^c$\\
  & $(A \cap B)^c = A^c \cup B^c $\\
  \hline 
\end{tabular}
\caption{Laws governing set operations on a sample space.}
\label{table:set-laws}
\end{table}

\subsubsection{The sigma-algebra}

\cite[Chapter~1.2.1]{casella_statistical_2002} defines a sigma-algebra as having the following properties:

\begin{enumerate}
    \item $\emptyset \in \mathcal{B}$
    \item $A \in \mathcal{B} \implies A^c \in \mathcal{B}$ ($\mathcal{B}$ is closed under complementation)
    \item $A_1, A_2, \dots \in \mathcal{B} \implies \cup_{i=1}^\infty A_i \in \mathcal{B}$ ($\mathcal{B}$ is closed under countable unions)
\end{enumerate}

As a consequence of DeMorgan's Law we also have the following:

\begin{equation}\label{eq:demorgans-law-consequence}
    \left(\bigcup_{i=1}^\infty A_i^c\right)^c = \bigcap_{i=1}^\infty A_i
\end{equation}

From this definition we can see that the sample space $\Omega$ can have many associated sigma-algebras. In general, the sigma-algebra we care about is the smallest one that contains all of the open sets in $\Omega$, as per \cite[Chapter~1.2.1]{casella_statistical_2002}. 

\subsubsection{The probability function}

Now that we've defined $\Omega$ and $\mathcal{B}$ we are able to define the probability function $\mathcal{P}$. We do so by laying out the so-called axioms of probability (also known as the Kolmogorov axioms) for a given sample space and associated sigma-algebra:

\begin{enumerate}
    \item $\mathbb{P}(A) \geq 0 \quad \forall A \in \mathcal{B}$
    \item $\mathbb{P}(\Omega) = 1$
    \item If $A_1, A_2, \dots \in \mathcal{B}$ are pairwise disjoint, then $\mathbb{P} \left(\cup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty \mathbb{P}(A_i)$
\end{enumerate}

Multiple probability functions can be defined on the same sample space, but in general $\mathcal{P}$ serves to map events in the sample space to numeric probabilities bounded between 0 and 1. $\mathcal{P}$ has the following additional properties when applied to a single event, which we denote $A$:

\begin{enumerate}
    \item $\mathbb{P}(\emptyset) = 0$
    \item $\mathbb{P}(A) \leq 1$
    \item $\mathbb{P}(A^c) = 1 - \mathbb{P}(A)$
\end{enumerate}

When applied to two events $A$ and $B$, the following hold:

\begin{enumerate}
    \item $\mathbb{P}(A \cap A^c) = \mathbb{P}(B) - \mathbb{P}(A \cap B)$
    \item $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$
    \item $A \subset B \implies \mathbb{P}(A) \leq \mathbb{P}(B)$
\end{enumerate}

If two events $A$ and $B$ are independent, we have:

\begin{equation}\label{eq:independence-1}
    \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)
\end{equation}

If $A$ and $B$ are not independent, we define the conditional probability of $A$ given $B$ as follows:

\begin{equation}\label{eq:conditional-prob}
    \mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\end{equation}

This definition leads us to Bayes' Rule:

\begin{equation}\label{eq:bayes-rule-for-sets}
    \mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)} 
\end{equation}

Lastly, if $A$ and $B$ are independent, the following are true:

\begin{equation}\label{eq:independence-2}
    \begin{aligned}
        \mathbb{P}(A|B) &= \mathbb{P}(A) \\
        A &\perp B^c \\
        A^c &\perp B \\
        A^c &\perp B^c \\
    \end{aligned}
\end{equation}

Mutual independence of multiple events $A_1, \dots, A_n$ is defined as follows, for any subcollection of events $A_{i_1}, \dots, A_{i_k}$:

\begin{equation}\label{eq:mutual-independence}
    \mathbb{P}\left(\bigcap_{j=1}^k A_{i_j}\right) = \prod_{j=1}^k \mathbb{P}(A_{i_j})
\end{equation}

\subsection{Counting}

The following is adapted from \cite[Chapter~1.2.3]{casella_statistical_2002}. Counting discrete outcomes is often useful when it comes to defining probabilities. Problems like this often take the form of enumerating how many ways there are to select a sample of size $r$ from a population of size $n$, the means of which are shown below:

\begin{equation}\label{eq:counting}
    \text{Number of possible arrangements} = \begin{cases}
        \text{Ordered without replacement} & \frac{n!}{(n-r)!} \\
        \text{Ordered with replacement} & n^r \\
        \text{Unordered without replacement} & \binom{n}{r} \\
        \text{Unordered with replacement} & \binom{n+r-1}{r} \\
    \end{cases}
\end{equation}

\subsection{Random variables}

A \gls{rv} is a mapping from $\Omega$ to $\mathbb{R}$. \glspl{rv} have what's called a \textit{support}, which is the range of values they can take. For example, if our \gls{rv} is the sum of two dice that are rolled, the support is $[2, 12]$.  Every \gls{rv} has a corresponding \gls{cdf}, which is defined as:

\begin{equation}\label{eq:cdf}
    F_X(x) = \mathbb{P}(X \leq x) \quad \forall x
\end{equation}

\Glspl{cdf} have the following necessary properties:

\begin{enumerate}
    \item $\lim_{x \to -\infty} F_X = 0$
    \item $\lim_{x \to \infty} F_X = 1$
    \item $F_X$ is a nondecreasing function of $x$ 
    \item $F_X$ is right-continuous i.e., $\lim_{x \to a} F_X(x) = F_X(a) \quad \forall a$
\end{enumerate}

The \gls{cdf} allows us to define whether or not the corresponding \gls{rv} is \textit{discrete} or \textit{continuous} i.e., the \gls{rv} is continuous if $F_X$ is a continuous function of $x$, and discrete if $F_X$ is a step function of $x$. Lastly, as noted by \cite[Chapter~1.5]{casella_statistical_2002}, the probability distribution of a \gls{rv} is completely determined by its \gls{cdf}. This allows us to defined the concept of \glspl{rv} being \textit{identically distributed}. Two \glspl{rv} $X$ and $Y$ are identically distributed if:

\begin{equation}\label{eq:identically-distributed}
    F_X(x) = F_Y(x) \quad \forall x
\end{equation}

The definition of the \gls{cdf} brings us to the concept of the \gls{pmf} (discrete case) and the \gls{pdf} (continuous case), both of which are concerned with providing probabilities of \glspl{rv} taking specific values. For a discrete \gls{rv}, the \gls{pmf} is defined as:

\begin{equation}\label{eq:pmf}
    f_X(x) = \mathbb{P}(X = x) \quad \forall x
\end{equation}

For a continuous \gls{rv}, the \gls{pdf} satisfies the following:

\begin{equation}\label{eq:pdf-1}
    F_X(x) = \int_{-\infty}^x f_X(t) dt \quad \forall x
\end{equation}

In the usual case that $f_X$ is continuous we have:

\begin{equation}\label{eq:pdf-2}
    f_X(x) = \frac{d}{dx} F_X(x) 
\end{equation}

Density or mass functions must have the following properties:

\begin{enumerate}
    \item $f_X(x) \geq 0 \quad \forall x$
    \item $\sum_x f_X(x) = 1$ (discrete case) or $\int_{\mathbb{R}} f_X(x) dx = 1$ (continuous case) 
\end{enumerate}

Oftentimes we are interested in the behavior of functions of \glspl{rv}, which are themselves \glspl{rv}. Suppose we define $Y = g(X)$ as a monotone function of the continuous \gls{rv} $X$. $X$ and $Y$ have supports $\mathcal{X}$ and $\mathcal{Y}$, respectively. Depending on whether $g$ is increasing or decreasing on $\mathcal{X}$ we can define the \gls{cdf} of $Y$ as follows:

\begin{equation}\label{eq:cdf-of-transformation}
    F_Y(y) = \begin{cases}
        \int_{-\infty}^{g^{-1}(y)} f_X(x) dx = F_X\left(g^{-1}(y)\right) & g \text{ increasing} \\
        \int_{g^{-1}(y)}^\infty f_X(x) dx = 1 - F_X\left(g^{-1}(y)\right) & g \text{ decreasing} \\
    \end{cases}
\end{equation}

If instead $X$ is a discrete \gls{rv}, we have:

\begin{equation}\label{eq:pmf-of-transformation}
    f_Y(y) = \mathbb{P}(Y = y) = \sum_{x \in g^{-1}(y)} f_X(x) \quad \forall y \in \mathcal{Y}
\end{equation}

From the \gls{cdf} of $Y$ we can obtain its \gls{pdf}, assuming again that $X$ is a continuous \gls{rv}, $g$ is a monotone function on $\mathcal{X}$, and $g^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$:

\begin{equation}\label{eq:pdf-of-transformation}
    f_Y(y) = \begin{cases}
        f_X\left(g^{-1}(y)\right) \left|\frac{d}{dy}g^{-1}(y)\right| & y \in \mathcal{Y} \\
        0 & \text{otherwise} \\
    \end{cases}
\end{equation}

\subsubsection{Expected values and moments}

The \textit{expected value} of a \gls{rv} is a measure of its central tendency. To generalize, we define the expected value of a function of a \gls{rv} $g(X)$ as follows:

\begin{equation}\label{eq:expected-value}
    \mathbb{E}[g(X)] = \begin{cases}
        \sum_{x \in \mathcal{X}} g(x) f_X(x) & \text{discrete } X \\
        \int_{\mathbb{R}} g(x) f_X(f) dx & \text{continuous } X \\
    \end{cases}
\end{equation}

In some cases e.g., Cauchy \glspl{rv}, the expected value is equal to $\infty$ and thus we say that it does not exist. In the usual case that the expected value does exist it has the following properties, where $a$ and $b$ are arbitrary constants:

\begin{enumerate}
    \item $\mathbb{E}[ag(X) + bh(X) + c] = a\mathbb{E}[g(X)] + b\mathbb{E}[h(X)] + c$
    \item $g(x) \geq 0 \: \forall x \implies \mathbb{E}[g(X)] \geq 0$ 
    \item $g(x) \geq h(x) \: \forall x \implies \mathbb{E}[g(X)] \geq \mathbb{E}[h(X)]$
    \item $a \leq g(x) \leq b \: \forall x \implies a \leq \mathbb{E}[g(X)] \leq b$
    \item $\underset{b}{\text{min }} \mathbb{E}\left[(X - b)^2\right] = \mathbb{E}\left[(X - \mathbb{E}[X])^2\right]$
\end{enumerate}

Expectation can be expanded to the concept of \textit{moments} of \glspl{rv}. The $k^{\text{th}}$ raw moment of $X$ is given by:

\begin{equation}\label{eq:raw-moment}
    m_k = \mathbb{E}\left[X^k\right]
\end{equation}

whereas the $k^{\text{th}}$ central moment of $X$ is:

\begin{equation}\label{eq:central-moment}
    \mu_k = \mathbb{E}\left[(X - \mathbb{E}[X])^k\right]
\end{equation}

The second central moment of a \gls{rv} is known as the variance, and its square root is denoted the standard deviation. The variance can be alternatively defined as:

\begin{equation}\label{eq:variance}
    \text{Var}(X) = \mathbb{E}\left[X^2\right] - \left(\mathbb{E}[X]\right)^2
\end{equation}

Similarly, the covariance between two random variables $X$ and $Y$ is defined as the product of their individual deviations from their respective expected values: 

\begin{equation}
    \begin{aligned}
        \text{Cov}(X, Y) 
          &= \mathbb{E}\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right] \\
          &= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] \\
    \end{aligned}
\end{equation}

With the covariance thusly defined, we can enumerate the properties of the variance, where $a$ and $b$ are arbitrary constants:

\begin{enumerate}
    \item $\text{Var}(X + a) = \text{Var}(X)$
    \item $\text{Var}(aX) = a^2\text{Var}(X)$
    \item $\text{Var}(aX \pm bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) \pm 2ab\text{Cov}(X, Y)$
    \item $\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i) + \sum_{i\neq j} \text{Cov}(X_i, X_j)$
\end{enumerate}

The concept of moments leads us to the so-called \gls{mgf}, which is defined as follows:

\begin{equation}\label{eq:mgf-1}
    M_X(t) = \mathbb{E}\left[e^{tX}\right] = \begin{cases}
        \int_{\mathbb{R}} e^{tx} f_X(x) dx & \text{continuous } X \\
        \sum_x e^{tx} f_X(x) & \text{discrete } X \\
    \end{cases}
\end{equation}

With the \gls{mgf} defined, we can say that the $k^{\text{th}}$ raw moment of $X$ is given by the $k^{\text{th}}$ derivative of $M_X(t)$ evaluated at $t = 0$ i.e.:

\begin{equation}\label{eq:mgf-2}
    \mathbb{E}\left[X^k\right] = M_X^{(k)}(0) = \frac{d^k}{dt^k} M_X(t) \bigg |_{t=0}
\end{equation}

As per \cite[Chapter~2.3]{casella_statistical_2002}, the usefulness of the \gls{mgf} comes not from its ability to generate the moments of a \gls{rv}, but from the fact that it can uniquely determine a distribution under certain conditions. The following theorem lays out that property for \glspl{rv} $X \sim F_X$ and $Y \sim F_Y$, all of whose moments exist:

\begin{enumerate}
    \item If $X$ and $Y$ have bounded support and $\mathbb{E}[X^k] = \mathbb{E}[Y^k] \: \forall r = 0, 1, 2, \dots$, then $F_X(u) = F_Y(u) \: \forall u$
    \item If the \glspl{mgf} exist and $M_X(t) = M_Y(t) \: \forall t$ in some neighborhood of 0, then $F_X(u) = F_Y(u) \: \forall u$
\end{enumerate}

Under certain conditions, the convergence of a sequence of \glspl{mgf} implies the convergence of \glspl{cdf}. To show this, suppose we observe a sequence of \glspl{rv} $X_i, i = 1, 2, \dots$, each of which has \gls{mgf} $M_{X_i}(t)$. If we suppose as well that:

\begin{equation}\label{eq:mgf-convergence-1}
    \lim_{i \to \infty} M_{X_i}(t) = M_X(t) \quad \forall t \text{ in a neighborhood of } 0
\end{equation}

then there exists a unique \gls{cdf} $F_X$ whose moments are determined by $M_X(t)$. We then have:

\begin{equation}\label{eq:mgf-convergence-2}
    \lim_{i\to\infty} F_{X_i}(x) = F_X(x) \quad \forall x \text{ where } F_X(x) \text{ is continuous}
\end{equation}

Finally, a useful property of \glspl{mgf} is that, for arbitrary constants $a$ and $b$:

\begin{equation}\label{eq:mgf-of-combination}
    M_{aX + b}(t) = e^{bt}M_X(at)
\end{equation}

\subsubsection{Characteristic functions}

Clearly, a limitation of the \gls{mgf} is that it cannot always uniquely characterize a distribution. Enter the \textit{characteristic function}, which 1) always exists 2) can be used to generate moments (when they exist) and 3) completely determines the distribution. The characteristic function is defined simply as:

\begin{equation}\label{eq:characteristic-function-1}
    \varphi_X(t) = \mathbb{E}\left[e^{itX}\right] = \int_{-\infty}^\infty e^{itx} f_X(x) dx
\end{equation}

The relationship of $\varphi_X(t)$ to the \gls{mgf} is given by:

\begin{equation}\label{characteristic-function-2}
    \varphi_X(-it) = M_X(t)
\end{equation}

A few more properties of the characteristic function are found in \cite{oberhettinger_fourier_1973}. Two in particular are of use to us. First, if $\varphi_X(t)$ has a $k^{\text{th}}$ derivative at $t = 0$, then all the moments of $F_X$ exist up to order $k$ if $k$ is even, and up to order $k-1$ if $k$ is odd. Second, two distribution functions $F_X$ and $F_Y$ are identical \gls{iff} their characteristic functions $\varphi_X(t)$ and $\varphi_Y(t)$ are identical. Lastly, the characteristic function can be used to prove the Central Limit Theorem. 

The following table is adapted from \cite{oberhettinger_fourier_1973} and shows the characteristic functions for several widely-used distributions. 

% make table rows larger
\renewcommand{\arraystretch}{2}

\begin{table}[h!]
\centering
\begin{tabular}{||l l l||} 
 \hline
 \textbf{Distribution} & \textbf{Parameters} & \textbf{Characteristic Function} \\ [0.5ex] 
  \hline\hline
  Normal & $\mu, \sigma^2$ & $\text{exp}\left(it\mu -\frac{1}{2}\sigma^2t^2\right)$ \\
  \hline
  Poisson & $\lambda$ & $\text{exp}\left(\lambda(e^{it} - 1)\right)$ \\
  \hline
  Exponential & $\lambda$ & $\left(1 - it\lambda^{-1}\right)^{-1}$ \\
  \hline 
  Gamma & $\alpha, \beta$ & $\left(1 - it\beta\right)^{-\alpha}$ \\
  \hline 
  Binomial & $n, p$ & $\left(1 - p + pe^{it}\right)^n$ \\
  \hline 
  Chi-squared & $p$ & $(1 - 2it)^{-\frac{p}{2}}$ \\
  \hline 
  Negative-binomial & $r, p$ & $\left(\frac{p}{1 - e^{it} + pe^{it}}\right)^r$ \\
  \hline 
\end{tabular}
\caption{Characteristic functions for several common distributions.}
\label{table:characteristic-functions}
\end{table}

% reset table row height
\renewcommand{\arraystretch}{1.5}

\subsubsection{Joint and marginal distributions}

Given two discrete \glspl{rv} $X$ and $Y$, the joint \gls{pmf} is defined as:

\begin{equation}\label{eq:joint-pmf}
    \mathbb{P}((X, Y) \in A) = \sum_{(x,y) \in A} f_{X, Y}(x, y)
\end{equation}

We then have the following definition of expectation:

\begin{equation}\label{eq:joint-expectation-discrete}
    \mathbb{E}[g(X,Y)] = \sum_{(x,y) \in \mathbb{R}^2} g(x,y)f_{X, Y}(x,y)
\end{equation}

From the joint \gls{pmf} we can define the marginal \glspl{pmf} as follows:

\begin{equation}
    \begin{aligned}
        f_X(x) &= \sum_{y \in \mathbb{R}} f_{X, Y}(x,y) \\
        f_Y(x) &= \sum_{x \in \mathbb{R}} f_{X, Y}(x,y) \\
    \end{aligned}
\end{equation}

Similarly, for continuous \glspl{rv} $X$ and $Y$, the joint \gls{pdf} is defined as:

\begin{equation}\label{eq:joint-pdf}
    \mathbb{P}((X,Y) \in A) = \iint_A f_{X, Y}(x,y) dxdy
\end{equation}

Expectation is defined as follows:

\begin{equation}\label{eq:joint-expectation-continuous}
    \mathbb{E}[g(X,Y)] = \iint_{\mathbb{R}^2} g(x,y)f_{X, Y}(x,y) dxdy
\end{equation}

The marginal \glspl{pdf} for $X$ and $Y$ can be had by integrating out the other variable:

\begin{equation}
    \begin{aligned}
        f_X(x) &= \int_{\mathbb{R}} f_{X, Y}(x,y) dy \quad y \in \mathbb{R} \\
        f_Y(y) &= \int_{\mathbb{R}} f_{X, Y}(x,y) dx \quad x \in \mathbb{R} \\
    \end{aligned}
\end{equation}

Lastly, we can define the joint \gls{cdf}:

\begin{equation}\label{eq:joint-cdf}
    F_{X, Y}(x,y) = \mathbb{P}(X \leq x, Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f(s,t)dtds
\end{equation}

\subsubsection{Conditional distributions}

The conditional \gls{pmf} of \gls{rv} $Y$ given \gls{rv} $X = x$ is defined as:

\begin{equation}\label{eq:conditional-pmf}
    f(y|x) = \mathbb{P}(Y = y | X = x) = \frac{f_{X, Y}(x,y)}{f_X(x)}
\end{equation}

The conditional \gls{pmf} of $X$ given $Y = y$ is defined similarly. For continuous \glspl{rv} $X$ and $Y$, the conditional \gls{pdf} of $X$ given $Y = y$ is given by:

\begin{equation}\label{eq:conditional-pdf}
    f(x|y) = \frac{f_{X, Y}(x,y)}{f_Y(y)}
\end{equation}

Conditional expected values can be defined as follows:

\begin{equation}\label{eq:conditional-ev}
    \mathbb{E}[g(Y) | x] = \begin{cases}
        \sum_y g(y)f(y|x) & \text{discrete case} \\
        \int_{\mathbb{R}} g(y) f(y|x) dy & \text{continuous case} \\
    \end{cases}
\end{equation}

This leads to the definition of the conditional variance:

\begin{equation}\label{eq:conditional-variance-1}
    \text{Var}(Y|x) = \mathbb{E}\left[Y^2|x\right] - \left(\mathbb{E}[Y|x]\right)^2
\end{equation}

\glspl{rv} $X$ and $Y$ are called independent if the following is true $\forall \: x, y \in \mathbb{R}$:

\begin{equation}\label{eq:independent-distributions}
    f_{X, Y}(x,y) = f_X(x)f_Y(y)
\end{equation}

The expectation of the product of functions of independent \glspl{rv} is the product of their expectations:

\begin{equation}\label{eq:independent-expectation}
    \mathbb{E}[g(X)h(Y)] = \mathbb{E}[g(X)]\mathbb{E}[h(Y)]
\end{equation}

In a similar vein, the following is true as regards the \gls{mgf} of the sum of independent \glspl{rv}, denoted $Z = X + Y$:

\begin{equation}\label{eq:mgf-of-sum-of-independent-rvs}
    M_Z(t) = M_X(t)M_Y(t)
\end{equation}

Expectation can be redefined as follows for any two \glspl{rv} $X$ and $Y$:

\begin{equation}\label{eq:conditional-expectation}
    \mathbb{E}[X] = \mathbb{E}\left[\mathbb{E}[X|Y]\right]
\end{equation}

Similarly, the variance can be defined in conditional terms:

\begin{equation}\label{eq:conditional-variance-2}
    \text{Var}(X) = \mathbb{E}\left[\text{Var}(X|Y)\right] + \text{Var}\left(\mathbb{E}[X|Y]\right)
\end{equation}

For a relatively simple proof see \cite[Chapter~4.4]{casella_statistical_2002}. 

\subsubsection{Correlation}

Correlation is a measure bounded between -1 and 1 that describes the strength of the linear relationship between two \glspl{rv}. It is defined as:

\begin{equation}\label{eq:correlation}
    \rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X\sigma_Y}
\end{equation}

If $X \perp Y$, then their covariance, and thus their correlation, is equal to 0. However the inverse is not true i.e., a covariance of zero does not imply that $X$ and $Y$ are independent.  

\subsubsection{The covariance inequality}

Given \glspl{rv} $X$ and $Y$, the following inequality holds as a consequence of Holder's inequality as defined in Equation \ref{eq:holders-inequality}:

\begin{equation}\label{eq:covariance-inequality}
    \left(\text{Cov}(X,Y)\right)^2 \leq \sigma^2_X\sigma^2_Y
\end{equation}

\subsubsection{Chebychev's inequality}

For any \gls{rv} $X$, nonnegative function $g$, and $r > 0$ the following holds:

\begin{equation}\label{eq:chebychevs-inequality}
    \mathbb{P}(g(X) \geq r) \leq \frac{\mathbb{E}[g(X)]}{r}
\end{equation}

For a simple proof see \cite[Chapter~3.6.1]{casella_statistical_2002}. 

\subsubsection{Holder's inequality}

For any two \glspl{rv} $X$ and $Y$ and positive numbers $p$ and $q$ satisfying $p^{-1} + q^{-1} = 1$, the following is true:

\begin{equation}\label{eq:holders-inequality}
    |\mathbb{E}[XY]| \leq \mathbb{E}[|XY|] \leq \left(\mathbb{E}[|X|^p]\right)^{\frac{1}{p}}\left(\mathbb{E}[|Y|^q]\right)^{\frac{1}{q}}
\end{equation}

A proof can be found in \cite[Chapter~4.7.1]{casella_statistical_2002}. A special case of Holder's inequality when $p = q = 2$ is called the Cauchy-Schwarz inequality. 

\subsubsection{Minkowski's inequality}

For any two \glspl{rv} $X$ and $Y$ and $1 \leq p \leq \infty$, the following is true:

\begin{equation}\label{eq:minkowskis-inequality}
    \left(\mathbb{E}[|X + Y|^p]\right)^{\frac{1}{p}} \leq \left(\mathbb{E}[|X|^p]\right)^{\frac{1}{p}} + \left(\mathbb{E}[|Y|^p]\right)^{\frac{1}{p}}
\end{equation}

A proof following from the application of the triangle inequality (see Equation \ref{eq:lst-triangle-inequalities}) and Holder's inequality can be found in \cite[Chapter~4.7.1]{casella_statistical_2002}. 

\subsubsection{Jensen's inequality}

For any \gls{rv} $X$ and convex function $g$ ($g$ is convex \gls{iff} $g^{\prime\prime}(x) \geq 0$), the following holds:

\begin{equation}\label{eq:jensens-inequality-1}
    \mathbb{E}[g(X)] \geq g(\mathbb{E}[X])
\end{equation}

If instead $g$ is concave, the reverse holds:

\begin{equation}\label{eq:jensens-inequality-2}
    \mathbb{E}[g(X)] \leq g(\mathbb{E}[X])
\end{equation}

A proof can be found in \cite[Chapter~4.7.2]{casella_statistical_2002}. 

\subsection{Families of distributions}

When considering probability distributions an \textit{exponential family} distribution is of the following form (note that this definition is similar but distinct from the exponential family distributions used in fitting \glspl{glm}, see Section \ref{sec:exponential-family-glms} for that definition):

\begin{equation}\label{eq:exponential-family-dist}
    f(x | \theta) = h(x) c(\theta) \text{exp} \left(\sum_{i=1}^k w_i(\theta) t_i(x)\right) \quad c(\theta), h(x) \geq 0 
\end{equation}

From \cite[Chapter~3.4]{casella_statistical_2002}, the following is true for any \gls{rv} $X$ with an exponential family \gls{pmf} or \gls{pdf}:

\begin{equation}\label{eq:exponential-family-properties}
    \begin{aligned}
        \mathbb{E}\left[\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial\theta_j}t_i(X)\right] &= -\frac{\partial}{\partial\theta_j} \log(c(\theta)) \\
        \text{Var}\left(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial\theta_j}t_i(X)\right) &= -\frac{\partial^2}{\partial\theta_j^2} \log(c(\theta)) - \mathbb{E}\left[\sum_{i=1}^k \frac{\partial^2w_i(\theta)}{\partial\theta_j^2} t_i(X)\right] \\
    \end{aligned}
\end{equation}

These identities can sometimes make it easier to derive the expected value and variance of a \gls{rv}. 

\subsubsection{Location and scale families}

Suppose we have a \gls{pdf} $f(x)$. Then, for any $\mu \in \mathbb{R}$ and $\sigma > 0$, the following is also a \gls{pdf}:

\begin{equation}\label{eq:pdf-transformation}
    g(x | \mu, \sigma) = \sigma^{-1} f\left(\frac{x - \mu}{\sigma}\right)
\end{equation}

A distribution belongs to the \textit{location family} if it follows the form $f(x - \mu)$ for any $\mu \in \mathbb{R}$. We then say that the \gls{pdf} belongs to the location family having standard \gls{pdf} $f(x)$ and location parameter $\mu$. If instead the \gls{pdf} has the form $\sigma^{-1} f\left(\frac{x}{\sigma}\right)$ for any $\sigma > 0$, we say that it belongs to the \textit{scale family} having standard \gls{pdf} $f(x)$ and location parameter $\sigma$. Finally, distributions of the form shown in Equation \ref{eq:pdf-transformation} are said to belong to the \textit{location-scale} family with standard \gls{pdf} $f(x)$, location parameter $\mu$, and scale parameter $\sigma$. 

\subsection{Principles of data reduction}

Suppose we observe some data $X = X_1, \dots, X_n$, and we are interested in making inference about some parameter $\theta$ of the data generating process. We might choose to define a statistic, $T(X)$, that summarizes the data. Some goals in defining this statistic might be 1) defining it such that it does not discard any useful information about $\theta$ 2) making sure it contains all available information about $\theta$ and 3) that it retains features of our model of the data generating process. These desires lead us to the principles of \textit{sufficiency}, \textit{likelihood}, and \textit{equivariance}, respectively. 

\subsubsection{Sufficiency}

\cite[Chapter~6.2]{casella_statistical_2002} defines a sufficient statistic as one that ``captures all available information about $\theta$ contained in the sample.'' That is, any inference concerning $\theta$ should depend on the sample through $T(X)$, and thus inference for a given value of $T(X)$ should be consistent regardless of the sample used to generate it. More concretely, the conditional distribution of $X$ given $T(X)$ should not depend on $\theta$, and $T(X)$ is only sufficient for $\theta$ if the following ratio is constant as a function of $\theta$, where $q$ is the \gls{pmf} or \gls{pdf} of $T(X)$:

\begin{equation}\label{eq:sufficient-stat}
    \frac{p(x|\theta)}{g(T(x)|\theta)}
\end{equation}

An alternate definition is as follows: $T(X)$ is sufficient for $\theta$ \gls{iff} the \gls{pmf} or \gls{pdf} can be factorized like so:

\begin{equation}\label{eq:pdf-pmf-factorization}
    f(x|\theta) = g(T(X)|\theta)h(x)
\end{equation}

For the exponential family of distributions defined in Equation \ref{eq:exponential-family-dist}, the sufficient statistic can be defined as follows, where $\theta$ is a vector of dimension $d \leq k$:

\begin{equation}\label{eq:sufficient-stat-exponential-family}
    T(X) = \left(\sum_{j=1}^n t_1(X_j), \dots, \sum_{j=1}^n t_k(X_k)\right)
\end{equation}

Clearly, a given sample may have many sufficient statistics. The question then arises of how to find the so-called \textit{minimal sufficient statistic} i.e., the one that achieves the greatest reduction in the data while still retaining all information about $\theta$. \cite[Chapter~6.2.2]{casella_statistical_2002} defines this statistic, rather confusingly, as one that is a function of any other sufficient statistic $T^\prime(X)$. A more concrete definition is found later in the text, where a minimal sufficient statistic is defined as one that satisfies the following: for any two sample points $x$ and $y$ the ratio of the \glspl{pmf} or \glspl{pdf} $\frac{f(x|\theta)}{f(y|\theta)}$ is constant in $\theta$ \gls{iff} $T(X) = T(y)$. Any one-to-one function of a minimal sufficient statistic is also a minimal sufficient statistic. 

A related concept is that of the \textit{ancillary statistic}, which is defined as any statistic $S(X)$ such that its distribution does not depend on $\theta$. Despite containing no information about $\theta$, ancillary statistics are not necessarily orthogonal to sufficient statistics, and in fact are oftentimes related. 

Finally, this brings us to the concept of the \textit{complete statistic}, which is defined as follows. Suppose $f(t|\theta)$ is the \gls{pmf} or \gls{pdf} of a statistic $T(X)$. Tee family of distributions is then called complete \gls{iff} the following holds:

\begin{equation}\label{eq:complete-statistic}
    \mathbb{E}_\theta[g(T)] = 0 \: \forall \theta \implies \mathbb{P}_\theta(g(T) = 0) = 1 \: \forall \theta 
\end{equation}

Next, Basu's theorem states the following: a complete and minimal sufficient statistic $(TX)$ is orthogonal to every ancillary statistic. For a proof see \cite[Chapter~6.2.4]{casella_statistical_2002}. 

For exponential family distributions as defined in Equation \ref{eq:exponential-family-dist}, the following statistic is complete \gls{iff} the parameter space $\Theta$ contains an open set in $\mathbb{R^k}$:

\begin{equation}\label{eq:complete-statistic-exponential-family}
    T(X) = \left(\sum_{j=1}^n t_1(X_j), \dots, \sum_{j=1}^n t_k(X_k)\right)
\end{equation}

Lastly, we can say that if a minimal sufficient statistic exists, any complete statistic is also a minimal sufficient statistic. 

\subsubsection{Likelihood}

We define the \textit{likelihood function} of $\theta$ given that $x$ was observed as:

\begin{equation}\label{eq:likelihood-function}
    \mathcal{L}(\theta|x) = \prod_{i=1}^n f(x_i|\theta)
\end{equation}

For discrete $X$, we have:

\begin{equation}\label{eq:likelihood-function-discrete}
    \mathcal{L}(\theta|x) = \mathbb{P}_\theta(X = x)
\end{equation}

whereas for continuous $X$:

\begin{equation}\label{eq:likelihood-function-continuous}
    \mathcal{L}(\theta|x) \approx \mathbb{P}_\theta(x - \epsilon < X < x + \epsilon) \quad \forall \epsilon > 0
\end{equation}

If $x$ and $y$ have been observed such that $\mathcal{L}(\theta|x) \propto \mathcal{L}(\theta|y)$, there must exist some constant $C(x,y)$ such that the following holds:

\begin{equation}\label{eq:likelihood principle}
    \mathcal{L}(\theta|x) = C(x,y) \mathcal{L}(\theta|y) \quad \forall \theta 
\end{equation}

and the conclusions drawn from the samples $x$ and $y$ should be identical. This is known as the \textit{likelihood principle}. 

\subsubsection{Equivariance}

The \textit{equivariance principle} states that if two statistics are equal i.e., $T(x) = T(y)$ for observed data $x$ and $y$, then the inference based on $x$ should have some sort of relationship to the inference made if $y$ was observed instead - though the inferences may not be exactly the same. Equivariance can take two forms, the first being what's called \textit{measurement equivariance}, which posits that the inference made should not depend on the measurement scale used during the experiment. The second form is \textit{formal equivariance}, which is more complex. It states that if two experiments have the same mathematical modeling structure, then the same procedure of inference should be used in both experiments. Combining the two forms of equivariance, we can say that for any change in measurement scale $Y = g(X)$ such that the model structure for $Y$ is the same as that of $X$, the inference procedure should be both measurement and formally equivariant. For examples of these two forms, see \cite[Chapter~6.4]{casella_statistical_2002}. 

\subsection{Estimation}

So far we have mostly described distributions as functions of some parameter(s) $\theta$, but we have not discussed how to obtain estimates of $\theta$. There are several means of doing so, which we'll discuss in detail in the following sections. In general, we define any statistic as a \textit{point estimator} of the parameter(s) of interest. The difference between an estimator and an estimate is simple: an estimator is a function of the sample, while the estimate is the actual realized value. 

\subsubsection{The method of moments}

Suppose we observe data $X_1, \dots, X_n$ with a corresponding \gls{pmf} or \gls{pdf} $f(x|\theta)$, where $\theta$ is a $k$-dimensional vector. The method of moments is a series of simultaneous equations where the first $k$ raw sample moments are equated to their population counterparts, after which the system is solved. The $k^{\text{th}}$ raw sample moment is defined as:

\begin{equation}\label{eq:kth-sample-moment}
    \hat{m}_k = n^{-1} \sum_{i=1}^n X_i^k
\end{equation}

Going forward, we denote method of moments estimates as $\Tilde{\theta}_n$ in order to distinguish them from other types of estimates. A key concern with using the method of moments is that the range of the estimator(s) might not coincide with the range of the parameter(s) of interest e.g., one might obtain a negative estimate of $p$ for a Binomial \gls{rv}. This is of course not desirable, and might lead us to consider an alternate class of estimator. 

\subsubsection{Maximum likelihood estimation}

The technique of maximum likelihood is far and away the most widely-used method for deriving estimators. Suppose we observe \gls{iid} \glspl{rv} $X_1, \dots X_n$ with corresponding \gls{pmf} or \gls{pdf} $f(x|\theta)$. The likelihood function is then defined as:

\begin{equation}\label{eq:likelihood-function-of-theta}
    \mathcal{L}(\theta; x_1, \dots, x_n) = \prod_{i=1}^n f(x_i|\theta)
\end{equation}

With $x$ held fixed, the \gls{mle} of $\theta$ is obtained by maximizing the likelihood as a function of $\theta$, which we'll denote henceforth as $\hat{\theta}_n$. In order to find a global maximum of the likelihood, one generally takes the partial derivative with respect to $\theta$, sets the resulting function equal to 0, and solves for $\theta$. To verify that the solution is in fact a global maximum, check that the following is true:

\begin{equation}\label{eq:likelihood-function-global-max-check}
    \frac{\partial^2}{\partial\theta^2} \mathcal{L}(\theta; x_1, \dots, x_n)|_{\theta = \hat{\theta}_n} < 0 
\end{equation}

In most cases we can make this process much easier in terms of differentiation by working instead with the log-likelihood, which is defined below. This is possible because the natural logarithm is a strictly increasing function, and thus the maximum of the log-likelihood occurs at the same values of $\theta$ as that of the likelihood. 

\begin{equation}\label{eq:log-likelihood}
    \mathcal{l}(\theta; x_1, \dots, x_n) = \log\left(\mathcal{L}(\theta; x_1, \dots, x_n)\right)
\end{equation}

Maximum likelihood estimation carries several attractive properties. First, unlike the method of moments, the range of the \gls{mle} is the same as that of $\theta$. Such estimators also have what's known as the \textit{invariance property}, which means that the \gls{mle} of a given function $\tau(\theta)$ is simply $\tau\left(\hat{\theta}\right)$. This property holds regardless of whether $\theta$ is a scalar or a vector. In the multivariate case, one verifies that the solution is a global maximum by 1) verifying that the first-order partial derivatives evaluated at the \glspl{mle} are zero 2) checking that at least one of the second-order partial derivatives is negative and 3) ensuring that the Jacobian $\mathfrak{J}(\theta)$ is positive. Lastly, in cases where it is difficult or impossible to derive a solution analytically, numerical methods can be used to obtain an approximate solution. One such technique is the \gls{em} algorithm, which has the unique property of being guaranteed to converge to the \gls{mle}. The basic steps of the \gls{em} algorithm are shown in Algorithm \ref{alg:expectation-maximization}. Given observed data $X$, a set of unobserved or \textit{latent} variables $Z$, and parameter(s) $\theta$, we define the likelihood function of $\theta$ as follows, where the \gls{mle} of $\theta$ is found by maximizing the marginal likelihood of the observed data:

\begin{equation}\label{eq:expectation-maximization-likelihood}
    \begin{aligned}
        \mathcal{L}(\theta;  X_1, \dots, X_n) 
          &= p(X | \theta) \\
          &= \int p(X, Z | \theta) dZ \\
          &= \int p(X | Z, \theta) p(Z|\theta) dZ 
    \end{aligned}
\end{equation}

However, Equation \ref{eq:expectation-maximization-likelihood} is intractable because $Z$ are unobserved. As such, we turn to the \gls{em} algorithm, which consists simply of two steps: the \textit{expectation} step and the \textit{maximization} step. 

\begin{algorithm}[h!]
    \caption{The expectation-maximization algorithm.}\label{alg:expectation-maximization}
    \begin{algorithmic}[1]
        \STATE Provide an initial estimate denoted $\theta^{(0)}$
        \FOR{iterations $i = 1$ to $m$}
            \STATE Compute the expected value of the log-likelihood of $\theta$ i.e., $Q(\theta | \theta^{(i)}) = \mathbb{E}_{Z\sim p(\cdot | X, \theta^{(i)}}\left[\log(p(X,Z|\theta)\right]$
            \STATE Maximize $Q$ over the parameter space i.e., $\theta^{(i+1)} = \underset{\theta}{\text{arg max }} Q(\theta | \theta^{(i)})$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

For expanded details on the properties of Algorithm \ref{alg:expectation-maximization} refer to either the summary given in \cite[Chapter~7.2.4]{casella_statistical_2002}, or \cite{dempster_maximum_1977}, which is the original paper. An example application for a simple liner model with missing data in $y$ is shown below:

\begin{listing}[H]
\inputminted{r}{Example-Code/em_algorithm.R}
\caption{Estimating regression parameters via the EM algorithm.}
\label{listing:em-algorithm-example}
\end{listing}

\subsubsection{Evaluating estimators}

With multiple options available, an important question arises - how does one select which estimator to use? It's clearly necessary to derive some means of evaluating an estimator's performance. A widely used criterion is the \gls{mse}, which is defined for unbiased estimators as follows:

\begin{equation}\label{eq:mse}
    \text{MSE}\left(\hat{\theta}_n\right) = \mathbb{E}\left[\left(\hat{\theta}_n - \theta\right)^2\right] = \text{Var}\left(\hat{\theta}_n\right)
\end{equation}

For biased estimators refer to Section \ref{sec:bias-variance-tradeoff}, which explains the decomposition of the \gls{mse} into variance- and bias-based components. However, suppose now that we have two candidate estimators, each unbiased. How then do we choose between them? The simplest solution is to choose the unbiased estimator with the smallest variance i.e., the \gls{umvue}. Any unbiased estimator $W^*$ is denoted an \gls{umvue} of any differentiable function of $\theta$ denoted $\tau(\theta)$ when to compared to any other unbiased estimator $W$ \gls{iff} the following conditions hold:

\begin{equation}\label{eq:umvue-properties}
    \begin{aligned}
        \mathbb{E}_\theta[W^*] &= \tau(\theta) \\
        \text{Var}_\theta(W^*) &\leq \text{Var}_\theta(W) \quad \forall \: W, \theta \\
    \end{aligned}
\end{equation}

The Cramer-Rao inequality, defined in Equation \ref{eq:cramer-rao-inequality-2}, provides a lower bound on the variance of any estimator $W(X) = W(X_1, \dots, X_n$ for a random variable $X \sim f(x|\theta)$, given that $W$ has the following properties:

\begin{equation}\label{eq:cramer-rao-inequality-1}
    \begin{aligned}
        \frac{d}{d\theta} E_\theta[W(X)] &= \int_{\mathcal{X}} \frac{\partial}{\partial\theta} (W(X)f(x|\theta) dx \\
        \text{Var}_\theta(W(X)) &< \infty \\
    \end{aligned}
\end{equation}

The inequality then follows from the Cauchy-Schwarz inequality as defined in Equations \ref{eq:covariance-inequality} and \ref{eq:holders-inequality}:

\begin{equation}\label{eq:cramer-rao-inequality-2}
    \text{Var}_\theta(W(X)) \geq \frac{\left(\frac{d}{d\theta}\mathbb{E}_\theta[W(X)]\right)^2}{\mathbb{E}_\theta\left[\left(\frac{\partial}{\partial\theta}\log(f(X|\theta))\right)^2\right]}
\end{equation}

The denominator in Equation \ref{eq:cramer-rao-inequality-2} is called \textit{Fisher's information} of the estimator. Clearly, as Fisher's information grows, the lower bound on the variance of the estimator shrinks. A corollary follows: if the \gls{pdf} or \gls{pmf} is of the exponential family as defined in Equation \ref{eq:exponential-family-dist}, the following is true:

\begin{equation}\label{eq:fisher-information}
    \mathbb{E}_\theta\left[\left(\frac{\partial}{\partial\theta}\log(f(X|\theta))\right)^2\right] = -\mathbb{E}_\theta\left[\frac{\partial^2}{\partial\theta^2} \log(f(X|\theta))\right]
\end{equation}

The Cramer-Rao lower bound cannot always be obtained; in fact, it can only be obtained under the following circumstances:

\begin{equation}\label{eq:cramer-rao-attainment}
    a(\theta) \left(W(x) - \tau(\theta)\right) = \frac{\partial}{\partial\theta} \log\left(\mathcal{L}(\theta|x)\right)
\end{equation}

As an example, consider a sequence of \glspl{rv} $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$, where we are interested in estimating $\sigma^2$ and $\mu$ is unknown i.e., $\tau(\theta) = \sigma^2$. The likelihood is then:

\begin{equation}\label{eq:normal-likelihood}
    \mathcal{L}(\mu, \sigma^2; x_1, \dots, x_n) = (2\pi\sigma^2)^{-\frac{n}{2}} \text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(x_i - \mu)^2}{\sigma^2}\right)
\end{equation}

Deriving the log-likelihood with respect to $\sigma^2$ gives us:

\begin{equation}\label{eq:normal-log-likelihood-derived}
    \frac{\partial}{\partial\sigma^2} \mathcal{l}(\mu, \sigma^2; x_1, \dots, x_n) = n(2\sigma)^{-4} \sum_{i=1}^n \frac{(x_i - \mu)^2}{n} - \sigma^2
\end{equation}

Taking $a(\theta) = n(2\sigma)^{-4}$ gives us $W(x) = \frac{(x_i - \mu)^2}{n}$, which is course only calculable if $\mu$ is known. In our case, since $\mu$ is unknown, the lower bound cannot be obtained. 

\subsubsection{Rao-Blackwellization}

Consider an unbiased estimator $W$ of $\tau(\theta)$. If $T$ is a sufficient statistic for $\theta$, we can define $\phi(T) = \mathbb{E}[W|T]$. We then have $\mathbb{E}_\theta[\phi(T)] = \tau(\theta)$ and $\text{Var}_\theta(\phi(T)) \leq \text{Var}_\theta(W)$, thus confirming that $\phi(T)$ is a uniformly better unbiased estimator of $\tau(\theta)$. For a proof see \cite[Chapter~7.3.3]{casella_statistical_2002}. The concrete utility of this theorem is that conditioning any unbiased estimator on a sufficient statistic improves the estimator uniformly. Thus, in our search for a best unbiased estimator, we should only consider estimators that are functions of sufficient statistics. For example, method of moments estimators are generally not functions of sufficient statistics, and thus can be improved via conditioning. For a detailed explanation see \cite[Chapter~7.5.1]{casella_statistical_2002}.

\subsubsection{Profile likelihood}

Oftentimes we are not interested in every parameter of a distribution; these uninteresting variables are referred to as \textit{nuisance parameters}. In other cases it might be exorbitantly difficult to maximize the full likelihood. In either event, suppose we observe data $x_1, \dots, x_n$, and suppose as well that can partition our vector of total parameters into two parts $\theta = [\delta, \xi]$, where $\xi$ is the set of nuisance parameters and $\delta$ is what we're really interested in. The overall likelihood function is thus:

\begin{equation}\label{eq:profile-likelihood-1}
    \mathcal{L}(\theta; x_1, \dots, x_n) = \mathcal{L}(\delta, \xi; x_1, \dots, x_n) = \prod_{i=1}^n f(x_i|\delta, \xi) 
\end{equation}

We can then define the profile likelihood as follows:

\begin{equation}\label{eq:profile-likelihood-2}
    \mathcal{L}_P(\delta; x_1, \dots, x_n) = \sup_{\xi} \mathcal{L}(\delta, \xi; x_1, \dots, x_n)
\end{equation}

The primary benefit or application of the profile likelihood approach is the estimation of \glspl{ci}, which can perform better than those based on the asymptotic Normal distribution of the \gls{mle} when the sample size is small or the \gls{mle} has a non-Gaussian distribution. The profile likelihood \gls{ci} is defined as follows, using the notation from \cite{venzon_method_1988} where $\chi^2_k(1-\alpha)$ is the $1-\alpha$ quantile of a $\chi$-squared distribution with $k$ \gls{df}:

\begin{equation}\label{eq:profile-likelihood-ci}
    \left[\theta \colon 2\left(\mathcal{l}\left(\hat{\theta}_n\right) - \mathcal{l}(\theta)\right) \leq \chi^2_k(1-\alpha)\right]
\end{equation}

For a brief overview of profile likelihood as regards logistic regression, see \cite{cole_maximum_2014}. \cite{royston_profile_2007} provides an overview of profile likelihood-based confidence intervals with example Stata code. The R package \href{https://cran.r-project.org/web/packages/ProfileLikelihood/}{\texttt{ProfileLikelihood}} implements functionality to estimate profile likelihood \glspl{ci} for a variety of commonly-used statistical models. 

\subsection{Hypothesis testing}

Statistical experiments often take the form of a \textit{hypothesis test}. A hypothesis is a statement about the population value of some parameter of interest. The two parts of a hypothesis test are the \textit{null} and \textit{alternative} hypothesis, which we'll denote $H_0$ and $H_1$, respectively. Commonly, the null hypothesis takes the form of assuming no effect, whereas the alternative hypothesis assumes some effect. Mathematically, for some parameter of interest $\theta$ we have the following forms of the hypotheses:

\begin{equation}\label{eq:hypotheses}
    \begin{aligned}
        H_0 &\colon \theta \in \Theta_0 \\
        H_1 &\colon \theta \in \Theta_0^c \\
    \end{aligned}
\end{equation}

A hypothesis test then defines a rule that specifies which values of a given \textit{test statistic} lead to the rejection of the null hypothesis. This region of the sample space, which we've denoted $\Theta_0^c$, is also known as the \textit{rejection region}. We note that if the test statistic does not fall inside the rejection region, we say that we fail to reject the null hypothesis - \textit{not} that we accept it. The question then arises: what kind of test should we perform?

\subsubsection{Likelihood ratio tests}

The \gls{lrt} is often used in scenarios when maximum likelihood estimation is applied. The \gls{lrt} is defined as the ratio of the likelihood under the null versus the empirically maximized likelihood:

\begin{equation}\label{eq:lrt-form}
    \lambda(x) = \frac{\sup_{\Theta_0} \mathcal{L}(\theta|x)}{\sup_{\Theta} \mathcal{L}(\theta|x)}
\end{equation}

The rejection region for an \gls{lrt} is then:

\begin{equation}\label{eq:lrt-rejection-region}
    \{x \colon \lambda(x) \leq c\} \quad \forall c \in [0, 1]
\end{equation}

As a motivating example, consider $X_1, \dots, X_n \sim \mathcal{N}(\mu, 1)$. We'd like to test the null hypothesis $H_0 \colon \mu = \mu_0$ versus the alternative hypothesis $H_1 \colon \mu \neq \mu_0$. The numerator of the \gls{lrt} is then $\mathcal{L}(\mu_0|x)$ whereas the denominator is the likelihood given the sample mean, which is the unrestricted \gls{mle} of $\mu$ i.e., $\mathcal{L}(\bar{x}_n|x)$. Thus the \gls{lrt} is:

\begin{equation}\label{eq:lrt-normal-rvs}
    \begin{aligned}
        \lambda(x) 
          &= \frac{(2\pi)^{-\frac{n}{2}} \text{exp}\left(-\frac{1}{2} \sum_{i=1}^n (x_i - \mu_0)^2\right)}{(2\pi)^{-\frac{n}{2}} \text{exp}\left(-\frac{1}{2} \sum_{i=1}^n (x_i - \bar{x}_n)^2\right)} \\
          &= \text{exp}\left(\frac{1}{2} \left(-\sum_{i=1}^n (x_i - \mu_0)^2 + \sum_{i=1}^n (x_i - \bar{x}_n)^2\right)\right) \\
          &= \text{exp}\left(-\frac{n(\bar{x}_n - \mu_0)^2}{2}\right)
    \end{aligned}
\end{equation}

With some finagling the rejection region can be formulated as:

\begin{equation}\label{eq:lrt-normal-rvs-rejection-region}
    \left\{x \colon |\bar{x}_n - \mu_0| \geq \sqrt{-n^{-1}2\log(c)}\right\}
\end{equation}

As mentioned previously, $T(X) = \bar{X}_n$ is a sufficient statistic for $\mu$. This motivates the following theorem from \cite[Chapter~8.2.1]{casella_statistical_2002}: if $T(X)$ is a sufficient statistic for $\theta$ and $\lambda^*(t)$ and $\lambda(x)$ are the \glspl{lrt} based on $T$ and $X$, then $\lambda^*(T(x)) = \lambda(x) \: \forall x \in \Omega$. 

\subsubsection{Intersections and unions of tests}

In some cases we might desire to test more then one null hypothesis simultaneously. These more complex tests can take two forms, the first being \textit{union-intersection} tests and the second being \textit{intersection-union} tests. The null hypothesis in the first case is defined like so, where we reject the null if \textit{any} of the individual elements of $H_0$ are rejected:

\begin{equation}\label{eq:union-intersection-test}
    H_0 \colon \theta \in \bigcap_{\gamma \in \Gamma} \Theta_\gamma 
\end{equation}

The intersection-union test is defined similarly, though in this case we only reject $H_0$ if \textit{all} of the individual null hypotheses are rejected: 

\begin{equation}\label{eq:intersection-union-test}
    H_0 \colon \theta \in \bigcup_{\gamma \in \Gamma} \Theta_\gamma 
\end{equation}

\subsubsection{Evaluating tests}

Hypothesis tests can make two types of errors, the rather unhelpfully-named Type I and Type II errors. Type I errors occur when the null hypothesis is incorrectly rejected, while Type II errors happen when we fail to reject the null when the alternative hypothesis is true. For a tabular description of these errors see \cite[Chapter~8.3.1]{casella_statistical_2002}. The two circumstances lead to the definition of the \textit{power function}, which is defined as follows for any test with rejection region $R$:

\begin{equation}\label{eq:power-function}
    \beta(\theta) = \mathbb{P}_\theta(X \in R)
\end{equation}

Ideally, $\beta(\theta)$ should be near 0 for all $\theta \in \Theta_0$, and near 1 for all $\theta \in \Theta_0^c$.  We refer to a given test as being of level $\alpha \in [0, 1]$ if $\sup_{\theta \in \Theta_0} \leq \alpha$. We call tests \textit{unbiased} if the following is true:

\begin{equation}\label{eq:unbiased-hypothesis-test}
    \beta(\theta^\prime) \geq \beta(\theta^{\prime\prime}) \quad \forall \: \theta^\prime \in \Theta_0 \text{ and } \theta^{\prime\prime} \in \Theta_0^c
\end{equation}

Lastly, we denote a test among a class of tests $\mathcal{C}$ having power functions $\beta^\prime(\theta)$ \gls{ump} if the following holds:

\begin{equation}\label{eq:ump-hypothesis-test}
    \beta(\theta) \geq \beta^\prime(\theta) \quad \forall \: \theta \in \Theta_0^c \text{ and } \beta^\prime(\theta) \in \mathcal{C}
\end{equation}

\subsubsection{The Neyman-Pearson lemma}

Suppose we would like to test a null hypothesis $H_0 \colon \theta = \theta_0$ versus an alternative $H_1 \colon \theta = \theta_1$. Any test satisfying the following conditions for some $k \geq 0$ is then a \gls{ump} test having level $\alpha$:

\begin{equation}\label{eq:neyman-pearson-lemma}
    \begin{aligned}
        f(x|\theta_1) > k f(x|\theta_0) &\implies x \in R \\
        f(x|\theta_1) < k f(x|\theta_0) &\implies x \in R^c \\
        \mathbb{P}_{\theta_0}(X \in R) &= \alpha \\
    \end{aligned}
\end{equation}

There are several implications of the Neyman-Pearson lemma which can be found in \cite[Chapter~8.3.2]{casella_statistical_2002}. 

\subsection{Confidence intervals}

When estimating parameters, it is often of interest to estimate as well an interval within which the estimated parameter is likely to fall. We define an interval estimate as follows:

\begin{equation}\label{eq:interval-estimator}
    L(x) \leq \theta \leq U(x) \quad \forall \: x \in \mathcal{X}
\end{equation}

The random interval $[L(X), U(X)]$ is called the interval estimator. The utility of the interval estimator over the point estimate is that we gain a positive probability of being correct about our estimate, since e.g., $\mathbb{P}(\bar{X}_n = \mu) = 0$ by definition. This positive chance of being correct is called the \textit{coverage probability}, and is computed as $\mathbb{P}(\theta \in [L(X), U(X)] | \theta)$. A common method of computing the \gls{ci} of an estimate is the inversion of the acceptance region of a test statistic. This is best explained by an example: suppose we observe $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ where the variance is known, and we are interested in testing $H_0 \colon \mu = \mu_0$ versus $H_1 \colon \mu \neq \mu_0$. The most powerful, unbiased, level $\alpha$ test has rejection region:

\begin{equation}\label{eq:normal-hypothesis-test-rejection-region}
    \left\{x \colon |\bar{x}_n - \mu| > z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right\}
\end{equation}

Ergo, we would fail to reject $H_0$ if the following were true:

\begin{equation}\label{eq:normal-hypothesis-test-acceptance-region}
    \bar{x}_n - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu_0 \leq \bar{x}_n + z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}
\end{equation}

Thus we have computed a \gls{ci} of level $1 - \alpha$ for $\mu$. :

\begin{equation}\label{eq:normal-hypothesis-ttest-acceptance-probability}
    \mathbb{P}\left(\bar{X}_n - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha 
\end{equation}

An alternate method for the estimation of \glspl{ci} is the method of \textit{pivots}. Pivots are generally defined in Table \ref{table:distribution-pivots} for each type of location-scale distribution. Mathematically, a \gls{rv} $Q(X, \theta) = Q(X_1, \dots, X_n, \theta)$ is a pivot \gls{iff} its distribution is independent of all parameters.

% make table rows larger
\renewcommand{\arraystretch}{2}

\begin{table}[h!]
\centering
\begin{tabular}{||l l l||} 
 \hline
 \textbf{PDF Type} & \textbf{PDF Form} & \textbf{Pivot} \\ [0.5ex] 
  \hline\hline
  Location & $f(x - \mu)$ & $\bar{X}_n - \mu$ \\
  \hline 
  Scale & $\sigma^{-1} f \left(\frac{x}{\sigma}\right)$ & $\sigma^{-1}\bar{X}_n$ \\
  \hline 
  Location-scale & $\sigma^{-1} f\left(\frac{x - \mu}{\sigma}\right)$ & $\frac{\bar{X}_n - \mu}{S_n}$ \\
  \hline 
\end{tabular}
\caption{Pivots for each type of location-scale family distribution.}
\label{table:distribution-pivots}
\end{table}

% reset table row height
\renewcommand{\arraystretch}{1.5}

For example, given $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$, the pivot is the t-statistic, which is a pivot because the t distribution does not depend on $\mu$ or $\sigma^2$:

\begin{equation}\label{eq:t-statistic}
    \frac{\bar{X}_n - \mu}{\frac{S_n}{\sqrt{n}}}
\end{equation}

Pivots can be used to construct \glspl{ci} by selecting $a$ and $b$ independent of $\theta$ such that the following is true:

\begin{equation}\label{eq:pivot-confidence-interval}
    \mathbb{P}_\theta\left(a \leq Q(X,\theta) \leq b\right) \geq 1 - \alpha 
\end{equation}

In general, to find a pivot we can use the following technique from \cite[Chapter~9.2.2]{casella_statistical_2002}. Suppose the \gls{pdf} of a statistic $T$ can be formulated as:

\begin{equation}\label{eq:pivot-pdf}
    f(t|\theta) = g(Q(t, \theta)) \left|\frac{\partial}{\partial\theta} Q(t, \theta)\right|
\end{equation}

for arbitrary function $g$ and monotone function $Q$, then $Q(T, \theta)$ is a pivot. For example, consider a $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ where both parameters are unknown. We can use the location-scale pivot from Table \ref{table:distribution-pivots}, which has a t distribution with $n - 1$ \gls{df}. The \gls{ci} for $\mu$ is then:

\begin{equation}\label{eq:normal-ci}
    \left\{\mu \colon \bar{x}_n - t_{n-1,\frac{\alpha}{2}} \frac{\hat{s}_n}{\sqrt{n}} \leq \mu \leq \bar{x}_n + t_{n-1,\frac{\alpha}{2}} \frac{\hat{s}_n}{\sqrt{n}}\right\}
\end{equation}

Intervals can also be estimated under a Bayesian paradigm, for details on this approach refer to Section \ref{sec:bayes}. 

\section{Linear models}\label{sec:tools-ols}

\subsection{Model formulation}

Suppose we have a linear model of the following form:

\begin{equation}\label{eq:ols-model}
    y_i = \beta X_i + \epsilon_i
\end{equation}

The \gls{ols} estimator of $\beta$ is given by:

\begin{equation}\label{eq:ols-beta-est}
    \hat{\beta} = (X^\intercal X)^{-1}X^\intercal y
\end{equation}

The so-called ``hat matrix'', the orthogonal projection of $y$ onto the space spanned by $X$, is given by:

\begin{equation}\label{eq:ols-hat-matrix}
    H = X(X^\intercal X)^{-1} X^\intercal
\end{equation}

The fitted values from the linear model are defined as: 

\begin{equation}\label{eq:ols-fitted-vals}
    \hat{y} = Hy = X\hat{\beta}
\end{equation}

The model residuals are given by:

\begin{equation}\label{eq:ols-residuals}
    \begin{aligned}
        \hat{\epsilon}
          &= y - X\hat{\beta} \\
          &= y - \hat{y} \\
          &= (I - H)y \\
    \end{aligned}
\end{equation}

The \gls{rss} is defined as:

\begin{equation}\label{eq:ols-rss}
    \begin{aligned}
        \text{RSS}
          &= \hat{\epsilon}^\intercal\hat{\epsilon} \\
          &= y^\intercal (I - H)^\intercal (I - H) y \\
          &= y^\intercal (I - H) y \\
    \end{aligned}
\end{equation}

The estimated variance of $\epsilon$ is as follows, where $n - p$ is the \gls{df} of the model ($p$ being the number of covariates):

\begin{equation}\label{eq:ols-sigma-squared}
    \hat{\sigma}^2 = \frac{\hat{\epsilon}^\intercal\hat{\epsilon}}{n - p}
\end{equation}

The (unadjusted) R-squared value for our regression is estimated like so; in the case of simple linear regression $R^2$ is equal to the square of the correlation between the covariate and the response:

\begin{equation}\label{ols-r-squared}
    R^2 = 1 - \frac{\sum_{i=1}^n (\hat{y}_i - y_i)^2}{\sum_{i=1}^n (y_i - \bar{y}_i)^2}
\end{equation}

This is equivalent to 1 minus the \gls{rss} divided by the \gls{tss}. 

Lastly, the estimated \gls{se} of a given coefficient $\hat{\beta}_j$ is given by:

\begin{equation}\label{ols-beta-se}
    \widehat{\text{se}}\left(\hat{\beta}_{j-1}\right) = \sqrt{(X^\intercal X)^{-1}_{jj}\hat{\sigma}^2}
\end{equation}

\subsection{The Gauss-Markov theorem}

Considering a regression model of the form shown in Equation \ref{eq:ols-model} the Gauss-Markov assumptions are as follows:

\begin{enumerate}
    \item $X$ is full-rank i.e., $X^\intercal X$ is invertible
    \item $\mathbb{E}[\epsilon_i] = 0$
    \item $\text{Var}(\epsilon_i) = \sigma^2 < \infty$
    \item Error terms are uncorrelated i.e., $\text{Cov}(\epsilon_i, \epsilon_j) = 0, \forall i \neq j$
\end{enumerate}

Under these assumptions, the \gls{ols} solution is the \gls{blue}.

\subsection{The bias-variance tradeoff}\label{sec:bias-variance-tradeoff}

The \gls{mse} of a regression estimator can be decomposed as follows:

\begin{equation}\label{eq:bias-variance-tradeoff}
    \mathbb{E}\left[\left(y - \hat{y}\right)^2\right] = \text{Var}\left(\hat{y}\right) + \left(\text{Bias}\left(\hat{y}\right)\right)^2
\end{equation}

Bias is defined as:

\begin{equation}\label{eq:bias}
    \text{Bias}\left(\hat{\theta}\right) = \mathbb{E}\left[\hat{\theta}\right] - \theta
\end{equation}

The bias-variance tradeoff states simply that low complexity models (high bias towards the assumed model structure) will have lower variance, while high complexity models (lower bias, more flexible model structure) will have higher variance. Somewhere in the middle of the two extremes lies a parsimonious model that minimizes the loss function, in this case the \gls{mse}. For a more detailed discussion see \cite[Chapter~2.9]{hastie_elements_2009}. 

\subsection{Model estimation}

While the closed-form \gls{ols} solution discussed above does exist, it is not at all the most efficient way of estimating coefficients. This is typically done using the QR decomposition as in e.g., the \texttt{stats} package in R. The following description is adapted from \cite[Chapter~2.7]{faraway_linear_2015}. We rewrite the design matrix $X$ as follows:

\begin{equation}\label{eq:ols-qr-decomp}
    X = Q \begin{bmatrix}
        R \\
        \symbf{0} \\
    \end{bmatrix} = Q_fR 
\end{equation}

The matrix $Q$ is an $n\times n$ orthogonal matrix i.e., $Q^\intercal Q = QQ^\intercal$, $R$ is a $p\times p$ upper triangular matrix, and $\symbf{0}$ is an $(n-p)\times p$ matrix of zeroes. Thus, $Q_f$ is the first $p$ columns of $Q$. Next, the \gls{rss} can be rewritten like so:

\begin{equation}\label{eq:ols-rss-qr-decomp-1}
    \text{RSS} = \left(y - X\hat{\beta}\right)^\intercal \left(y - X\hat{\beta}\right) = \left|\left|y - Z\hat{\beta}\right|\right|^2
\end{equation}

We then have the following, where vector $f$ is of length $p$ and vector $r$ is of length $n-p$:

\begin{equation}\label{eq:ols-rss-qr-decomp-2}
    \text{RSS} = \left|\left|Q^\intercal y - Q^\intercal X\beta\right|\right|^2 = \left|\left|\begin{bmatrix}
        f \\
        r \\
    \end{bmatrix} - \begin{bmatrix}
        R \\
        \symbf{0} \\
    \end{bmatrix}\right|\right|^2
\end{equation}

Lastly, the following can be minimized by solving for $f = R\beta$:

\begin{equation}\label{eq:ols-rss-qr-decomp-3}
    \text{RSS} = ||f - R\beta||^2 + ||r||^2
\end{equation}

\subsubsection{Example code}

The QR decomposition method can be rather easily implemented in R (this is what the \texttt{lm()} function uses internally). We simulate 5 covariates, each having a defined relationship to the response $y$. We next fit a linear model using \texttt{lm()}, then do the same using the \texttt{qr()} function to perform the decomposition. Comparing the coefficients derived from each approach shows their equality. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/qr_decomp_linear_model.R}
\caption{Using the QR decomposition to estimate linear regression coefficients.}
\label{listing:qr-regression-example}
\end{listing}

\subsection{Hypothesis tests}

Going forward we'll compare two nested models, where the larger model is denoted $\Omega$ with dimension $p$, and the smaller model is denoted $\omega$ with dimension $q$. The \gls{lrt} statistic for the null hypothesis that the models have similar performance is given by:

\begin{equation}\label{eq:ols-lrt-stat}
    \frac{\text{max}_{\beta, \sigma \in \Omega} \mathcal{L}(\beta, \sigma; y)}{\text{max}_{\beta, \sigma \in \omega} \mathcal{L}(\beta, \sigma; y)}
\end{equation}

For each model, the following holds:

\begin{equation}\label{eq:likelihood-prop-to-sigma}
    \mathcal{L}\left(\hat{\beta}, \hat{\sigma}; y\right) \propto \hat{\sigma}^{-n}
\end{equation}

This leads to the following F-statistic, which under the null hypothesis is asymptotically distributed $F \sim F_{p-q, n-p}$:

\begin{equation}\label{eq:ols-f-stat-nested}
    F = \frac{\frac{\text{RSS}_\omega - \text{RSS}_\Omega}{p-q}}{\frac{\text{RSS}_\Omega}{n-p}}
\end{equation}

To test whether any of the covariates are useful for predicting the response we set the null hypothesis to $H_0: \beta_1, \dots \beta_p = 0$ and compute the following F-statistic, where the \gls{tss} is equal to $(y - \bar{y})^\intercal (y - \bar{y})$:

\begin{equation}\label{eq:ols-f-stat-overall}
    F = \frac{\frac{\text{TSS} - \text{RSS}}{p-1}}{\frac{\text{RSS}}{n-p}}
\end{equation}

We can also test whether any individual covariate is informative, which corresponds to the null hypothesis $H_0: \beta_j = 0$. A t-statistic for this test is shown below, and has an asymptotic t distribution with \gls{df} equal to $n-p$:

\begin{equation}\label{eq:ols-t-stat}
    t_j = \frac{\hat{\beta}_j}{\widehat{\text{se}}\left(\hat{\beta}_j\right)}
\end{equation}

\Glspl{ci} of level $\alpha$ for coefficients can be constructed like so:

\begin{equation}\label{eq:ols-beta-confidence-interval}
    \hat{\beta}_j \pm t^{\frac{\alpha}{2}}_{n-p} \widehat{\text{se}}\left(\hat{\beta}_j\right)
\end{equation}

An alternative approach to computing coefficient \glspl{ci} is to utilize bootstrap resampling, the basic procedure for which is detailed below:

\begin{algorithm}[h!]
    \caption{The bootstrap confidence interval procedure for linear regression.}\label{alg:linearBS}
    \begin{algorithmic}[1]
        \STATE Fit initial model $\hat{y} = X \hat{\beta}$
        \FOR{iterations $b = 1$ to $B$}
            \STATE Resample from $\hat{\epsilon}$ to generate $\epsilon^*$
            \STATE Formulate resampled model $y^* = X \hat{\beta} + \epsilon^*$
            \STATE Estimate $\hat{\beta}^*$ from the resampled model
        \ENDFOR
    \STATE Compute empirical 2.5\% and 97.5\% quantiles to form the bootstrap CI
    \end{algorithmic}
\end{algorithm}

\subsection{Model diagnostics}

One of the more important steps in model development is checking whether or not assumptions concerning the error are violated. This is typically done by examining the residuals $\hat{\epsilon}$, as it is impossible to directly observe the error. The constant variance assumption is interrogated by plotting $\hat{\epsilon}$ against $\hat{y}$, then examining whether heteroskedasticity or nonlinearity exists. Next, $\hat{\epsilon}$ should be checked to determine whether they significantly deviate from a Normal distribution. This can be performed by examining a plot of the theoretical quantiles versus the sorted residuals; this is often called a QQ-plot. Histograms or density plots should not be used to check normality of $\hat{\epsilon}$. In the case that a test statistic is necessary, the Shapiro-Wilk test of normality can be (cautiously) used. We note that with large sample sizes the test is rather sensitive to even small deviations from normality, and thus its results should be used in conjunction with visual methods such as the QQ-plot. The Shapiro-Wilk test statistic does not have a large sample asymptotic distribution, thus \textit{p}-values are calculated using Monte-Carlo sampling. This process is implemented in the \texttt{shapiro.test()} function that ships with base R. The formula for the Shapiro-Wilk test statistic is shown below; the subscript $(i)$ denotes the $i^{\text{th}}$ order statistic:

\begin{equation}\label{eq:ols-shapiro-wilk-stat-1}
    W = \frac{\left(\sum_{i=1}^n a_i \hat{\epsilon}_{(i)}\right)^2}{\sum_{i=1}^n \left(\hat{\epsilon}_i - \bar{\hat{\epsilon}}\right)^2}
\end{equation}

The coefficients $a_i$ are estimated like so, where $m$ is composed of the expected values of order statistics of \gls{iid} standard normal random variables and $V$ is the covariance matrix of the same:

\begin{equation}\label{eq:ols-shapiro-wilk-stat-2}
    a_i = \frac{m^\intercal V^{-1}}{\left(m^\intercal V^{-1}V^{-1}m\right)^{\frac{1}{2}}}
\end{equation}

The final check to be performed on $\hat{\epsilon}$ is to determine whether the residuals are correlated. Doing so is a difficult task due to the wide variety of correlations that may occur, and it is usually necessary to guide the analysis using domain knowledge concerning the data generating process. For example, time-series data may display correlation between sequential errors, and spatial data may exhibit error correlations within geographical regions. In general, plotting pairs of residuals against one another, or plotting residuals against covariates of interest are likely to be of help. In addition, to formally test whether autocorrelation exists in $\hat{\epsilon}$ the Durbin-Watson test statistic may be computed like so; this functionality is implemented in the \texttt{durbinWatsonTest()} function of the \texttt{car} R package. 

\begin{equation}\label{eq:durbin-watson-stat}
    DW = \frac{\sum_{i=2}^n \left(\hat{\epsilon}_i - \hat{\epsilon}_{i-1}\right)^2}{\sum_{i=1}^n \hat{\epsilon}_i^2}
\end{equation}

After examining the error assumptions, it's often necessary to determine whether or not certain observations have an outsize impact on the model fit. These are often referred to as ``influential'' observations, in contrast to observations that do not fit the model well (denoted ``outliers''), or points that are extreme in the predictor space (denoted ``leverage'' points). Leverage points have the potential to impact model fit, but might not actually do so. The leverage value for each observation is denoted $h_i$, and is estimated using the diagonal of the projection matrix, which we defined in Equation \ref{eq:ols-hat-matrix}. This leads to the following definition:

\begin{equation}\label{eq:ols-residual-variance-est}
    \widehat{\text{Var}}\left(\hat{\epsilon}_i\right) = \sigma^2(1 - h_i)
\end{equation}

The leverages also sum to the number of covariates $p$ i.e., $\sum_{i=1}^n h_i = p$. According to \cite[Chapter~6.2]{faraway_linear_2015}, a general rule is that observations with a leverage of greater than $\frac{2p}{n}$ is worthy of further examination, as the average leverage value is $n^{-1}p$. The leverage values also lead us to the definition of the standardized residual:

\begin{equation}\label{eq:ols-dtandardized-resid}
    r_i = \frac{\hat{\epsilon}_i}{\sqrt{\hat{\sigma}^2(1 - h_i)}}
\end{equation}

The standardized residuals and leverages can be used to compute the Cook's distance for each observation, which combines the two sources of information:

\begin{equation}\label{eq:ols-cooks-distance}
    \begin{aligned}
        D_i 
          &= \frac{\left(\hat{y} - \hat{y}_{(i)}\right)^\intercal\left(\hat{y} - \hat{y}_{(i)}\right)}{p\hat{\sigma}^2} \\
          &= p^{-1}r_i^2 \frac{h_i}{1 - h_i} \\
    \end{aligned}
\end{equation}

We can use the aforementioned leverage values to help identify outliers, which we defined prior as observations that do not fit the model well. To identify such points we can refit the model without each observation $i$, leading to new fitted values:

\begin{equation}\label{eq:ols-yhat-refitted}
    \hat{y}_{(i)} = X_i\hat{\beta}_{(i)}
\end{equation}

This leads to:

\begin{equation}\label{eq:ols-yhat-refitted-variance-est}
    \widehat{\text{Var}}\left(\hat{y}_i - \hat{y}_{(i)}\right) = \hat{\sigma}^2_{(i)}\left(1 + X_i^\intercal \left(X_{(i)}^\intercal X_{(i)}\right)^{-1}X_i\right)
\end{equation}

We can then define the studentized (or jackknife) residual for observation $i$ as follows:

\begin{equation}\label{eq:ols-jackknife-resid}
    \begin{aligned}
        t_i 
          &= \frac{y_i -\hat{y}_{(i)}}{\hat{\sigma}_{(i)}\left(1 + X_i^\intercal \left(X_{(i)}^\intercal X_{(i)}\right)^{-1}X_i\right)^{\frac{1}{2}}} \\
          &= \frac{\hat{\epsilon}_i}{\hat{\sigma}_{(i)}\sqrt{1 - h_i}} \\
          &= r_i\left(\frac{n-p-1}{n-p-r_i^2}\right)^{\frac{1}{2}} \\
    \end{aligned}
\end{equation}

Thus, we can compute the studentized residuals without computing $n$ different regression models. The studentized residuals have an approximate asymptotic distribution of $t_i \sim t_{n-p-1}$, which allows us to estimate a \textit{p}-value for the null hypothesis that a given observation is not an outlier given that the model is correct and $\epsilon \sim \mathcal{N}(0, \sigma^2I)$. However, this approach runs up against the \gls{mht} problem, and thus a \textit{p}-value adjustment method such as the Bonferroni or Holm correction should be used. 

\subsubsection{Example code}

Here we demonstrate how to extract several different kinds of residuals from an \texttt{lm} object in R, after which we extract the per-observation leverages and Cook's distance values. These can all be inspected graphically using histograms, plotting versus the fitted values, and QQ-plots. 

\begin{listing}[H]
\inputminted{r}{Example-Code/residual_diagnostics_gaussian.R}
\caption{Performing residual diagnostics after fitting a linear regression model.}
\label{listing:residuals-example}
\end{listing}

\subsection{Model selection and regularization}

One issue that arises during model selection is multicollinearity, which occurs when a variable has little effect on the response due to its being well-predicted by other covariates i.e., they are highly correlated. Two methods for penalization that can help reduce multicollinearity are ridge regression and the \gls{lasso}. The ridge regression problem is formulated like so:

\begin{equation}\label{eq:ols-ridge-regressor-lagrangian}
    \hat{\beta}^{\text{ridge}} = \underset{\beta}{\text{arg min}} \left\{\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^n x_{ij} \beta_j\right)^2 + \lambda \sum_{j=1}^p \beta_j^2\right\}
\end{equation}

When $\lambda = 0$ the ridge regression problem reduces to the \gls{ols} solution. This can be seen when writing the optimization problem in matrix form:

\begin{equation}\label{eq:ols-ridge-regressor-matrix}
    \hat{\beta}^{\text{ridge}} = \left(X^\intercal X + \lambda I\right)^{-1}X^\intercal y
\end{equation}

The ``trick'' of ridge regression is that adding the ridge to the diagonal of $X^\intercal X$ makes the matrix invertible - even if $X$ is not of full rank. Under the ridge penalty, the variance of the coefficients is given by:

\begin{equation}\label{eq:ridge-coef-variance}
    \text{Var}\left(\hat{\beta}^{\text{ridge}}\right) = \sigma^2\left(X^\intercal X + \lambda I\right)^{-1} X^\intercal X \left(X^\intercal X + \lambda I\right)^{-1}
\end{equation}

Similarly, the \gls{lasso} optimization problem takes the following form:

\begin{equation}\label{eq:lasso-regressor-lagrangian}
    \hat{\beta}^{\text{LASSO}} = \underset{\beta}{\text{arg min}} \left\{\frac{1}{2} \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j\right)^2 + \lambda \sum_{j=1}^n |\beta_j|\right\}
\end{equation}

Unlike ridge regression, which simply shrinks the coefficients towards zero, the \gls{lasso} method is able to actually set the coefficients of unimportant covariates equal to zero, achieving sparsity. The ridge and \gls{lasso} problems are also referred to as the L2-norm and L1-norm penalties, respectively. When using either approach it is customary to center and scale the covariates $X$ and center the response $y$. Lastly, both approaches require the selection of a suitable value of the penalization parameter $\lambda$. This is usually done via \gls{gcv}. 

\subsubsection{Example code}

The \texttt{glmnet} package is the most widely-used option for fitting regularized linear models in R. The ridge penalty is specified using $\alpha = 0$ and the \gls{lasso} by $\alpha = 1$, with values in between 0 and 1 specifying the elastic net penalty. Here we simulate a dataset with $p = 100$ covariates, with $X_j, j = 1, 2, 3$ having a true relationship with $y$ and all others being just random Gaussian noise. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/ridge_and_lasso_gaussian.R}
\caption{Fitting ridge and LASSO models to simulated data using \texttt{glmnet}.}
\label{listing:glmnet-example}
\end{listing}

\subsection{Sandwich variance estimation}\label{sec:sandwich-variance-estimation}

To start we define an estimating function as follows, where $\mathbb{E}[G_n(\theta)] = 0$ and $\theta$ is a $p$-dimensional vector:

\begin{equation}\label{eq:ols-sandwich-est-function}
    G_n(\theta) = n^{-1} \sum_{i=1}^n G(\theta, y_i)
\end{equation}

The estimating equation is thus the following, to which the estimator $\hat{\theta}_n$ is the solution:

\begin{equation}\label{eq:ols-sandwich-est-equation}
    G_n\left(\hat{\theta}_n\right) = n^{-1} \sum_{i=1}^n G\left(\hat{\theta}_n, y_i\right) = 0
\end{equation}

Then $\hat{\theta}_n \overset{p}{\to} \theta$ and:

\begin{equation}\label{eq:ols-sandwich-asymptotic-dist}
    \sqrt{n}\left(\hat{\theta}_n - \theta\right) \overset{d}{\to} \mathcal{N}\left(0, B^{-1}M\left(B^\intercal\right)^{-1}\right)
\end{equation}

Above $B$ and $M$ are defined as follows; this variable naming convention is derived from treating $B$ as the ``bread'' and $M$ as the ``meat'' of the ``sandwich'':

\begin{equation}\label{eq:ols-sandwich-pieces}
    \begin{aligned}
        B &= \mathbb{E}\left[\frac{\partial}{\partial\theta}G(\theta, y)\right] \\
        M &= \text{Var}(G(\theta, y)) \\
    \end{aligned}
\end{equation}

The sandwich variance is then:

\begin{equation}\label{eq:ols-sandwich-variance}
    \text{Var}\left(\hat{\theta}_n\right) = n^{-1} B^{-1} M \left(B^\intercal\right)^{-1}
\end{equation}

The variance estimator is generated empirically from the data:

\begin{equation}\label{eq:ols-sandwich-variance-estimator}
    \begin{aligned}
        \hat{B}_n &= n^{-1} \sum_{i=1}^n \frac{\partial}{\partial\theta} G \left(\hat{\theta}_n y_i\right) \\
        \hat{M}_n &= n^{-1} \sum_{i=1}^n G\left(\hat{\theta}_n, y_i\right)G\left(\hat{\theta}_n, y_i\right)^\intercal \\
        \implies \widehat{\text{Var}}\left(\hat{\theta}_n\right) &= n^{-1} \hat{B}_n^{-1} \hat{M}_n \left(\hat{B}_n^\intercal\right)^{-1}
    \end{aligned}
\end{equation}

Often the estimating function is based on the score function i.e.:

\begin{equation}\label{eq:ols-sandwich-score-function}
    G_n(\theta) = n^{-1} \sum_{i=1}^n \frac{\partial}{\partial\theta} \mathcal{l}_i(\theta)
\end{equation}

The variance estimator is then computed using:

\begin{equation}\label{eq:ols-sandwich-score-function-solutions}
    \begin{aligned}
        \hat{B}_n &= n^{-1} \sum_{i=1}^n \frac{\partial^2}{\partial\theta\partial\theta^\intercal} l_i(\theta)\bigg|_{\hat{\theta}} \\
        \hat{M}_n &= n^{-1} \sum_{i=1}^n \left(\frac{\partial}{\partial\theta}l_i(\theta)\right)\left(\frac{\partial}{\partial\theta}l_i(\theta)\right)^\intercal \bigg|_{\hat{\theta}} \\
    \end{aligned}
\end{equation}

As a motivating example, consider a linear regression model of the form shown in Equation \ref{eq:ols-model}. If the model is correct, then the variance of the estimated coefficients is given by $\text{Var}\left(\hat{\beta}\right) = \sigma^2X^\intercal X$. However, suppose instead that the true variance model is $\text{Var}(y | X) = \sigma^2 V$ i.e., that the assumed variance model is incorrect. The correct variance estimator is then:

\begin{equation}\label{eq:ols-sandwich-wrong-variance-1}
    \text{Var}\left(\hat{\beta}\right) = \sigma^2\left(X^\intercal X\right)^{-1} X^\intercal VX\left(X^\intercal X\right)^{-1}
\end{equation}

This is derived from the estimating function:

\begin{equation}\label{eq:ols-sandwich-wrong-variance-2}
    G(\beta) = X^\intercal(y - \beta X)
\end{equation}

The ``pieces'' of the sandwich are then:

\begin{equation}\label{eq:ols-sandwich-wrong-variance-3}
    \begin{aligned}
        \hat{B}_n &= X^\intercal X \\
        \hat{M}_n &= \sum_{i=1}^n X_i^\intercal X_i \left(y_i - \hat{\beta} X_i\right)^2 \\
    \end{aligned}
\end{equation}

The above estimator of $M$ is biased; an alternate estimator is given by:

\begin{equation}\label{eq:ols-sandwich-wrong-variance-4}
    \hat{M}_n = \frac{n}{n-p-1} \sum_{i=1}^n X_i^\intercal X_i \left(y_i - \hat{\beta} X_i\right)^2
\end{equation}

We then finally have:

\begin{equation}\label{eq:ols-sandwich-wrong-variance-5}
    \widehat{\text{Var}}\left(\hat{\beta}\right) = \hat{B}_n^{-1} \hat{M}_n \left(\hat{B}^\intercal\right)^{-1}
\end{equation}

\subsubsection{Example code}

In Source Code \ref{listing:sandwich-example} we showcase the utility of the \href{https://cran.r-project.org/web/packages/sandwich/index.html}{\texttt{sandwich}} R package, the main purpose of which is to generate a sandwich variance-covariance matrix for a model's coefficients. This matrix can then be used as input to functions from the \href{https://cran.r-project.org/web/packages/lmtest/index.html}{\texttt{lmtest}} package for performing inference. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/sandwich_linear_regression.R}
\caption{Generating a sandwich variance-covariance matrix for robust inference using \texttt{sandwich}.}
\label{listing:sandwich-example}
\end{listing}

\subsection{Generalized least-squares}

Sometimes the assumptions of \gls{ols} regression are violated; in practice, one of the bigger issues is if the errors are somehow dependent or correlated. In this case we should opt to use the method of \gls{gls}. Under the \gls{ols} assumptions we take it for granted that $\text{Var}(\epsilon) = \sigma^2 I$. Under the \gls{gls} paradigm we assume instead that $\text{Var}(\epsilon) = \sigma^2 \Sigma$, where $\Sigma$ is some known positive definite covariance matrix. We can represent $\Sigma$ as $\Sigma = SS^\intercal$, which is equivalent to taking the square root of a matrix. Our regression problem then takes the following form:

\begin{equation}\label{eq:ols-gls-model-form}
    \begin{aligned}
        y &= \beta X + \epsilon \\
        S^{-1} y &= S^{-1} \beta X + S^{-1} \epsilon \\
        \Tilde{y} &= \beta \Tilde{X} + \Tilde{\epsilon} \\
    \end{aligned}
\end{equation}

The variance of the transformed error term is as follows, making use of the fact that $\left(S^{-1}\right)^\intercal = \left(S^\intercal\right)^{-1}$ (\href{https://math.stackexchange.com/questions/340233/transpose-of-inverse-vs-inverse-of-transpose}{proof}):

\begin{equation}\label{eq:ols-gls-error-variance-transformed}
    \begin{aligned}
        \text{Var}(\Tilde{\epsilon})
          &= \text{Var}\left(S^{-1} \epsilon\right) \\
          &= S^{-1}\text{Var}(\epsilon)\left(S^\intercal\right)^{-1} \\
          &= S^{-1} \sigma^2 SS^\intercal \left(S^\intercal\right)^{-1} \\
          &= \sigma^2 I \\
    \end{aligned}
\end{equation}

The \gls{tss} is given by:

\begin{equation}\label{eq:ols-gls-tss}
    (y - \beta X)^\intercal \Sigma^{-1} (y - \beta X)
\end{equation}

and is minimized by:

\begin{equation}\label{eq:ols-gls-solution}
    \hat{\beta}^{\text{GLS}} = \left(X^\intercal \Sigma^{-1} X\right)^{-1} X^\intercal \Sigma^{-1} y
\end{equation}

The variance of the \gls{gls} coefficients is:

\begin{equation}\label{eq:ols-gls-coef-variance}
    \text{Var}\left(\hat{\beta}^{\text{GLS}}\right) = \left(X^\intercal \Sigma^{-1} X\right)^{-1} \sigma^2
\end{equation}

As per \cite[Chapter~2.7.2]{agresti_foundations_2015} the \gls{gls} coefficients are unbiased i.e., $\mathbb{E}\left[\hat{\beta}^{\text{GLS}}\right] = \beta$. The fitted values are computed as follows:

\begin{equation}\label{eq:ols-gls-fitted-values}
    \hat{\mu} = X \left(X^\intercal \Sigma^{-1} X\right)^{-1} X^\intercal \Sigma^{-1} y
\end{equation}

and we estimate $\sigma^2$ via:

\begin{equation}\label{eq:ols-gls-sigma2-estimate}
    \hat{\sigma}^2 = \frac{\left(y - \hat{\mu}\right)^\intercal \Sigma^{-1} \left(y - \hat{\mu}\right)}{n - p}
\end{equation}

\subsubsection{Software implementations}

There aren't a lot of options for fitting \gls{gls} models in R. The primary two that I'm aware of are the \href{https://cran.r-project.org/web/packages/nlme/index.html}{\texttt{nlme}} and \href{https://cran.r-project.org/web/packages/car/index.html}{\texttt{car}} packages, both of which export a \texttt{gls()} function. We'll demonstrate this functionality in the next section. 

\subsubsection{Example code}

Of course, the trickiest part of \gls{gls} is that one must know the variance-covariance matrix $\Sigma$. In practice, $\Sigma$ is often determined by a combination of intuition based on the experimental design and empirical estimation from the data. We demonstrate this in Source Code \ref{listing:gls-example}, where we fit \gls{ols} and \gls{gls} models to the global warming dataset, which contains proxy global warming covariates from several areas that are used to predict temperature in the Northern Hemisphere across time. Since the units are evenly-spaced (years), we choose an \gls{ar}-1 correlation structure. We then compare the two models, which are technically nested, using an F test. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/generalized_least_squares.R}
\caption{Fitting a generalized least-squares model using \texttt{nlme}.}
\label{listing:gls-example}
\end{listing}

\subsection{Weighted least-squares}

Similar to \gls{gls} is the concept of \gls{wls} regression, which as per \cite[Chapter~8.2]{faraway_linear_2015} is to be used when the errors do not exhibit constant variance. In fact, \gls{wls} is a special case of \gls{gls}. In this case, we define $\Sigma$ as a diagonal matrix of weights i.e.:

\begin{equation}\label{eq:ols-wls-sigma-form}
    \Sigma = \text{Diag}\left(\frac{1}{w_1}, \dots, \frac{1}{w_n}\right)
\end{equation}

We then define $S$ as:

\begin{equation}\label{eq:ols-wls-s-matrix}
    S = \text{Diag}\left(\frac{1}{\sqrt{w_1}}, \dots, \frac{1}{\sqrt{w_n}}\right)
\end{equation}

The question then arises of how one sets the weight for each observation. This depends on why weighting is desirable. If for example, your response is some sort of average, then it would make sense to weight the observations by the sample size for each observation, denoted $n_i$. If observations are of varying quality, weights can be set manually to reflect this. Lastly, and perhaps most-commonly, if heteroskedasticity is present it makes sense to weight the observations by $w_i = \frac{1}{\sigma^2_i}$. This obviously requires estimation of $\sigma^2_i$, which we'll show how to do in the following section. 

\subsubsection{Example code}

In Source Code \ref{listing:wls-example} we generate simulated data with heteroskedasticity, specifically with the variance being positively correlated with $X$. After fitting an \gls{ols} model, we model the residuals as a function of the fitted values, then use the inverse of the squared fitted values from that model as weights. When comparing the \gls{ols} and \gls{wls} models via adjusted $R^2$, we see that the \gls{wls} model performs much better. In addition, when viewing the output from \texttt{summary()}, we can see that the residual \gls{se} of the \gls{wls} model is lower than that of the \gls{ols} model. This disparity is confirmed when comparing the \gls{aic} of the two models - we see that the \gls{wls} model's value is a fair bit lower. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/weighted_least_squares.R}
\caption{Fitting a weighted least-squares model with empirically estimated weights.}
\label{listing:wls-example}
\end{listing}

\subsection{Bayesian inference}\label{sec:bayes}

Bayesian inference is based on the posterior probability distribution of $\theta$ after observing data $y$. The posterior is defined using Baye's Theorem:

\begin{equation}\label{eq:ols-bayes-posterior}
    p(\theta | y) = \frac{p(y | \theta) \pi(\theta)}{p(y)}
\end{equation}

where the denominator is the so-called ``normalizing constant'':

\begin{equation}\label{eq:ols-bayes-norm-constant}
    p(y) = \int_\theta p(y | \theta) \pi(\theta) d\theta
\end{equation}

Ignoring the normalizing constant gives the following general form of the posterior:

\begin{equation}\label{eq:ols-bayes-general-concept}
    \text{Posterior} \propto \text{Likelihood} \times \text{Prior}
\end{equation}

The posterior mean is given by:

\begin{equation}\label{eq:ols-bayes-posterior-mean}
    \mathbb{E}[\theta_i] = \int_{\theta_i} \theta_i p(\theta_i | y) d\theta_i
\end{equation}

whereas the posterior median is obtained by solving:

\begin{equation}\label{eq:ols-bayes-posterior-median}
    \mathbb{P}(\theta_i < \theta_i(0.5)) = \int_{-\infty}^{\theta_i(0.5)} p(\theta_i | y) d\theta_i
\end{equation}

These two measures are used to summarize the location of the posterior marginal distribution. In addition we will often estimate a credible interval, the Bayesian equivalent of a Frequentist \gls{ci}. This is done by computing e.g., the 2.5\% and 97.5\% percentiles of the marginal posterior distribution of the parameter of interest as shown below. The interpretation of a credible interval differs from that of a \gls{ci}, and is often considered more intuitive; it simply provides the interval within which the true parameter is probable to be. 

\begin{equation}\label{eq:ols-bayes-credible-interval}
    \text{95\% credible interval} = \left(\theta_i(0.025), \theta_i(0.975)\right)
\end{equation}

We can also estimate the so-called predictive distribution of new data , denoted $z$, given our original data $y$.The predictive distribution can then be used to generate prediction intervals that have an attractive interpretation; for example, a 95\% Bayesian prediction interval has probability 0.95 of containing the new data. Assuming conditional independence and that the likelihood of the original data is appropriate for the new data, the predictive distribution is computed like so:

\begin{equation}\label{eq:ols-bayes-predictive-distribution}
    p(z | y) = \int_\theta p(z | \theta) p(\theta | y) d\theta
\end{equation}

As per \cite[Chapter~3.3]{wakefield_bayesian_2013}, the Bernstein-von Mises theorem states that ``with increasing sample size, the posterior distribution tends to a normal distribution whose mean is the \gls{mle} and whose variance-covariance matrix is the inverse of Fisher's information.'' If we denote the posterior mean as $\Tilde{\theta}_n$, the \gls{mle} as $\hat{\theta}_n$, and Fisher's information as $\mathcal{I}(\theta)$ we have the following:

\begin{equation}\label{eq:ols-bayes-posterior-mean-asymptotics}
    \begin{aligned}
        \sqrt{n}\left(\Tilde{\theta}_n - \theta\right) &= \sqrt{n}\left(\Tilde{\theta}_n - \hat{\theta}_n\right) + \sqrt{n}\left(\hat{\theta}_n - \theta\right) \\
        \sqrt{n}\left(\Tilde{\theta}_n - \hat{\theta}_n\right) &\overset{p}{\to} 0 \\
        \sqrt{n}\left(\Tilde{\theta}_n - \theta\right) &\overset{d}{\to} \mathcal{N}\left(0, (\mathcal{I}(\theta)^{-1}\right) \\
    \end{aligned}
\end{equation}

\subsubsection{Choosing a prior}

Intuitively, as $n \to \infty$ the effect of the prior distribution diminishes, and the posterior is dominated by the likelihood of the observed data $y$. This leads us to one of the central problems inherent to Bayesian inference: the choice of a prior distribution. Priors generally fall into two categories: \textit{baseline priors} and \textit{substantive priors}. The former takes the stance that the prior should be chosen such that the posterior is dominated by the likelihood, whereas the latter incorporates some sort of contextual information. Alternate names for these two modes of prior choice are \textit{objective Bayes} and \textit{subjective Bayes}. A so-called \textit{diffuse} prior is one that is spread over a large area of the parameter space e.g., a Gaussian distribution with a large value of $\sigma^2$. Lastly, what's known as an \textit{improper prior} is one such that the prior does not integrate to a positive constant less than $\infty$. This may not be an issue in practice, but in some cases it will lead to an improper posterior distribution which is not desirable. A proper baseline prior that is often used is Jeffrey's prior, shown below:

\begin{equation}\label{eq:ols-bayes-jeffreys-prior}
    \pi(\theta) \propto |\mathcal{I}(\theta)|^{\frac{1}{2}}
\end{equation}

Jeffrey's prior has the desirable property of being invariant to reparameterization. However, its utility does not extend well to multivariate $\theta$, and thus other priors must be employed; see \cite[Chapter~3.4.1]{wakefield_bayesian_2013} for details. In the case that, given the likelihood function, the posterior and prior distributions are of the same probability distribution family the prior and posterior are denoted \textit{conjugate distributions} with respect to the likelihood function. 

As a motivating example, consider a linear regression model of the following form:

\begin{equation}\label{eq:ols-bayes-model-format}
    \begin{aligned}
        y | \beta, \sigma^2 &\sim \mathcal{N}(\beta X, \sigma^2I) \\
        \beta | \lambda, \tau^2 &\sim \mathcal{N}(\lambda, \tau^2I) \\
    \end{aligned}
\end{equation}

The posterior distribution of $\beta$ is given by:

\begin{equation}\label{eq:ols-bayes-beta-posterior-dist}
    \beta \sim \mathcal{N}\left(\left(\sigma^{-2}X^\intercal X + \tau^{-2} I\right)^{-1}\left(X^\intercal \sigma^{-2} y + \tau^{-2} \lambda\right), \left(\sigma^{-2}X^\intercal X + \tau^{-2} I\right)^{-1}\right)
\end{equation}

The posterior mean is thus a weighted average of the \gls{ols} estimate $\hat{\beta}$ and the prior mean:

\begin{equation}\label{eq:ols-bayes-beta-posterior-mean}
    \Tilde{\beta} = \left(\sigma^{-2} X^\intercal X + \tau^{-2} I\right)^{-1}\left(\sigma^{-2} X^\intercal X \hat{\beta} + \tau^{-2} \lambda\right)
\end{equation}

In many cases the prior distribution depends on some parameters $\lambda$, which makes the marginal distribution of $y$ the following:

\begin{equation}\label{eq:ols-bayes-y-marginal-on-lambda}
    p(y | \lambda) = \int_\theta p(y | \theta) \pi(\theta | \lambda) d\theta
\end{equation}

A natural question is then: how should one choose proper values for $\lambda$? The empirical Bayes method chooses the value of $\lambda$ that maximizes $p(y | \lambda)$ i.e., it uses the marginal maximum likelihood estimators of $\lambda$. The resulting posterior, denoted $\pi\left(\theta | \hat{\lambda}\right)$, is then used for inference. However, this approach has an obvious disadvantage - it does not account for the uncertainty in estimating $\hat{\lambda}$. This motivates the hierarchical Bayes approach, which places a prior on on the hyperparameters. The prior is thus as follows, where $h$ denotes the prior distribution of the hyperparameters:

\begin{equation}\label{eq:ols-bayes-hierarchical-prior}
    \pi(\theta) = \int_\lambda \pi(\theta | \lambda) h(\lambda) d\lambda
\end{equation}

\subsubsection{Bayesian model averaging}

When considering multiple models, \gls{bma} is often employed in order to estimate model uncertainty. Suppose we consider $j = 1, \dots, J$ discrete models each denoted $M$, and our parameter of interest is denoted $\phi$. The posterior for $\phi$ is given by:

\begin{equation}\label{eq:ols-bayes-bma-phi-posterior}
    p(\phi | y) = \sum_{j=1}^J p(\phi | M_j, y) \mathbb{P}(M_j | y)
\end{equation}

The posterior for $\phi$ for an individual model $M_j$ is estimated by:

\begin{equation}\label{eq:ols-bayes-bma-phi-posterior-individual}
    p(\phi | M_j, y) = \int_{\mathbb{R}} p(\phi | M_j, \theta_j, y) p(\theta_j | M_j, y) d\theta_j
\end{equation}

and the probability of model $M_j$ given our observed data is:

\begin{equation}\label{eq:ols-bayes-bma-model-prob}
    \mathbb{P}(M_j | y) = \frac{\int_{\mathbb{R}} p(y | M_j, \theta_j)p(\theta_j | M_j) d\theta_j \mathbb{P}(M_j)}{p(y)}
\end{equation}

Lastly, the marginal probability of observing our data given prior belief in model $M_j$ is given by:

\begin{equation}\label{eq:ols-bayes-bma-data-prob}
    p(y | M_j) = \int_{\mathbb{R}} p(y | M_j, \theta_j) p(\theta_j | M_j) d\theta_j
\end{equation}

Thus:

\begin{equation}\label{eq:ols-bayes-bma-y-prob}
    p(y) = \sum_{j=1}^J p(y | M_j) \mathbb{P}(M_j)
\end{equation}

The posterior mean of $\phi$ is estimated via:

\begin{equation}\label{eq:ols-bayes-bma-phi-posterior-mean}
    \mathbb{E}[\phi | y] = \sum_{j=1}^J \mathbb{E}[\phi | M_j, y] \mathbb{P}(M_j | y)
\end{equation}

\subsubsection{Bayesian linear models}

In order to compare two models - which may or may not be nested - we can compute the so-called Bayes factor, which is the ratio of the two models' marginal densities (here we denote the null hypothesis by $H_0$ and the alternative by $H_1$):

\begin{equation}\label{eq:ols-bayes-bf-1}
    B = \frac{\mathbb{P}(y | H_1)}{\mathbb{P}(y | H_0)}
\end{equation}

The probability of observing $y$ given hypothesis $H_k$ is given by:

\begin{equation}\label{eq:ols-bayes-bf-2}
    \mathbb{P}(y | H_k) = \int_{\theta_k} \mathbb{P}(y | \theta_k, H_k) \pi(\theta_k | H_k) d\theta_k
\end{equation}

\cite{kass_bayes_1995} provides us with some general rules of thumb for interpreting the Bayes factor, which we lay out in Table \ref{table:bayes-factor}. 

\begin{table}[h!]
\centering
\begin{tabular}{||l l l||} 
 \hline
 $B$ & $2\log_{10}(B)$ & \textbf{Strength of Evidence} \\ [0.5ex] 
  \hline\hline
  1-3 & 0-2 & Not worth mentioning \\
  \hline
  3-20 & 2-6 & Positive \\
  \hline 
  20-150 & 6-10 & Strong \\
  \hline
  >150 & >10 & Very strong \\
  \hline
\end{tabular}
\caption{General rules for the interpretation of Bayes factors.}
\label{table:bayes-factor}
\end{table}

Per \cite[Chapter~10.1]{wakefield_bayesian_2013}, with large enough $n$ we have the following, where the \gls{bic} is defined as in Equation \ref{eq:glm-fit-measures}:

\begin{equation}\label{eq:ols-bayes-bf-3}
    2\log(B) \approx \left|\text{BIC}_{H_1} - \text{BIC}_{H_0}\right|
\end{equation}

There are several software implementations for Bayesian linear models in R. Some of these are simply interfaces to dedicated Bayesian analysis softwares such as \href{https://mc-stan.org}{Stan} and \href{https://mcmc-jags.sourceforge.io}{JAGS}. Several popular options are shown below:

\begin{table}[h!]
\centering
\begin{tabular}{||l l||}
  \hline
  \textbf{Package} & \textbf{Fitting Strategy} \\ [0.5ex]
  \hline\hline
  \href{https://cran.r-project.org/web/packages/rstanarm/index.html}{\texttt{rstanarm}} & No-U-Turn sampling using Stan \\
  \hline 
  \href{https://cran.r-project.org/web/packages/brms/index.html}{\texttt{brms}} & No-U-Turn sampling using Stan \\
  \hline 
  \href{https://cran.r-project.org/web/packages/MCMCpack/index.html}{\texttt{MCMCpack}} & MCMC via Gibbs sampling \\
  \hline 
  \href{https://cran.r-project.org/web/packages/rjags/index.html}{\texttt{rjags}} & MCMC via Gibbs sampling using JAGS \\
  \hline 
\end{tabular}
\caption{Software for the fitting of Bayesian linear models.}
\label{table:bayes-software}
\end{table}

\subsubsection{Example code}

To demonstrate how to fit a Bayesian linear model we'll use the \texttt{brms} package. Note that this requires the installation of the Stan software prior to usage as detailed in Table \ref{table:bayes-software}. We begin by simulating a small dataset consisting of 3 covariates - 2 numeric and 1 categorical - with a true interaction term between $X_2$ and $X_3$. Bayesian methods are much slower than their Frequentist counterparts, so we use 4 cores in order to speed up the process. In addition, we fit a model with an interaction term between $X_2$ and $X_3$. To compare the two models we estimate the Bayes factor in favor of the interaction model; going by the guidelines in Table \ref{table:bayes-factor} it is highly preferable to the model with no interaction. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/brms_linear_model.R}
\caption{Fitting a Bayesian linear model using \texttt{brms}.}
\label{listing:brms-example}
\end{listing}

\subsection{Linear mixed models}

A key assumption of ordinary linear models is that of independence of individual observations. This assumption is often clearly violated in the case of longitudinal data, where multiple observations are collected over time per-subject. If ordinary linear models are fit to this sort of data the estimated standard errors of the coefficients will be artificially low, as the correlations between observations within each subject are ignored. This brings us to the domain of \glspl{lmm} - also referred to as hierarchical or multilevel models - which explicitly model inter-subject variance using what are usually referred to as random effects. Models take the following general form, where the random effects are assumed to follow a Gaussian distribution with zero mean and variance-covariance matrix $\Sigma$, and the random effects design matrix is denoted $Z$:

\begin{equation}\label{eq:ols-mixed-model-form}
    \mu_i = \beta X_i + \gamma Z_i + \epsilon_i
\end{equation}

Conditional on the random effects, the distribution of $y$ is then:

\begin{equation}\label{eq:ols-mixed-model-y-dist-conditional}
    y|\gamma \sim \mathcal{N}\left(\beta X + \gamma Z, \sigma^2 I\right)
\end{equation}

The variance of $y$ is then (assuming that the error and random effects are orthogonal):

\begin{equation}\label{eq:ols-mixed-model-y-variance}
    \text{Var}(y) = \sigma^2 Z\Sigma Z^\intercal + \sigma^2 I
\end{equation}

Thus the unconditional distribution of $y$ is:

\begin{equation}\label{eq:ols-mixed-model-y-dist-unconditional}
    y \sim \mathcal{N}\left(\beta X, \sigma^2\left(I + Z \Sigma Z^\intercal\right)\right)
\end{equation}

Random effects take two basic forms: random intercepts and random slopes. Each type of random effect allows a different model parameter to vary across subjects. The estimated coefficients for the fixed effects then represent the overall coefficient across subjects, whereas the coefficients for random effects represent the subject-level deviations from the overall coefficients. Random intercepts allow each subject to have their own intercept, and random slopes allow the slope of a covariate to differ between subjects. When more than one random effect is present they are referred to as either \textit{nested} or \textit{crossed}, depending on the experimental design. Nested effects occur when one categorical factor has levels that occur exclusively within the levels of another categorical factor e.g., classrooms within schools. Crossed effects are those that are not nested. See \cite[Chapter~10.9]{faraway_extending_2016} for an example analysis of a latin square experiment with crossed effects. 

\subsubsection{Restricted maximum likelihood estimation}

A key thing to understand about \glspl{lmm} is that model fitting is usually performed using a method called \gls{reml} instead of the standard maximum likelihood approach. Briefly, \gls{reml} estimation first estimates the random effects while ignoring the fixed effects, after which the fixed effects are estimated. This approach generally leads to less biased estimates, especially of the variance components. Under balanced experimental designs the \gls{reml} and maximum likelihood estimates will generally be quite similar. However, \gls{reml} estimation does have some drawbacks: if \gls{reml} is used instead of standard maximum likelihood, then likelihood-based tests such as an \gls{lrt} cannot be performed. With that being said, \gls{reml} is often preferred as a fitting method because of the aforementioned biasedness of maximum likelihood estimation, a trait that is exacerbated for unbalanced designs. That bias occurs because the variance components can have a value of zero i.e., a value that is on the boundary of the parameter space. In addition, maximum likelihood estimation does not account for the degrees of freedom lost when estimating the fixed effects. Due to these issues, some software implementations such as the popular \texttt{lme4} package decline to report \textit{p}-values based on the often-inaccurate asymptotic approximations. 

\subsubsection{Estimating degrees of freedom}

The definition of degrees of freedom in the mixed model setting is rather murky. The number of parameters taken up by the fixed effects is simple and matches that of ordinary linear models; as for the random effects, it's rather up in the air as to whether only the variance components ``count'' as parameters, or whether each subject-level coefficient should be considered a parameter too. In general, it's assumed that a model term with $q$ components should be considered as having $\frac{q(q+1)}{2}$ parameters. For example, a random slope has 2 components (the covariate and the group), and thus 3 parameters: the slope variance, the intercept variance, and the correlation between the two. This structure is rather simple though, and the definition of degrees of freedom becomes less clear as model complexity increases. Having an appropriate measure for degrees of freedom is highly necessary when e.g., comparing nested models or testing the significance of random effects. These issues are exacerbated in the presence of unbalanced experimental designs and complex random effects structures, such as crossed effects. For a detailed discussion of this problem with examples see \cite{baayen_mixed-effects_2008}. The degrees of freedom problem can be circumvented via the usage of \gls{mcmc} sampling, though this approach has its own downsides. For example, as per \cite{luke_evaluating_2017} it cannot be used when the fitted model includes random slopes which are quite often desirable. In addition, \gls{mcmc} sampling can be (and often is) quite computationally intensive. As such, methods for approximating the degrees of freedom in an \gls{lmm} have been developed; two such methods are the Kenward-Roger and Satterthwaite approximations. The details of these methods are honestly rather confusing notationally and are beyond our scope here; as such, we refer to \cite{kenward_small_1997} and \cite{satterthwaite_approximate_1946}, respectively, for the original derivations. For a comparison of the various available software implementations as regards mixed models see \cite{luke_evaluating_2017}; in general the \texttt{pbkrmodcomp} R package should be used. 

\subsubsection{Inferential approaches}

In the case that maximum likelihood was used to fit the model, you can use an \gls{lrt} to test your null hypothesis, denoted $H_0$, versus your alternate hypothesis, denoted $H_1$:

\begin{equation}\label{eq:glm-glmm-lrt-statistic}
    \chi^2 = 2 \left(\mathcal{l}\left(\hat{\beta}_1, \hat{\sigma}^2_1, \hat{\Sigma}_1; y_1, \dots, y_n\right) - \mathcal{l}\left(\hat{\beta}_0, \hat{\sigma}^2_0, \hat{\Sigma}_0; y_1, \dots, y_n\right)\right)
\end{equation}

This test statistic is approximately $\chi$-squared distributed with degrees of freedom equal to the difference in the number of parameters between the two models. However, a note of caution: due to the issues that occur with maximum likelihood estimation under the \gls{lmm} paradigm, the asymptotic $\chi$-squared approximation can at times be quite inaccurate. For testing the null hypothesis that a variance component is equal to zero the test tends to be overly conservative, whereas for testing fixed effects the test is often overly liberal, generating \textit{p}-values that are too small. As such, when testing fixed effects the parametric bootstrap is often used. An implementation of this type of test is found in the \texttt{pbkrtest} R package. To test the significance of random effects terms, use the \texttt{RLRsim} R package, which implements the parametric bootstrap to simulate the null distribution of either an \gls{lrt} or a \gls{rlrt}. The parametric bootstrap can also be used to estimate empirical confidence intervals for the variance components, and to estimate prediction intervals. For an example of such approaches applied to a simple dataset see \cite[Chapter~10.2]{faraway_extending_2016}. 

\subsubsection{Model diagnostics}

When generating predictions from an \gls{lmm} there are two possible approaches. The first is to use only the fixed effects to provide estimates; this is usually done when estimating outcomes for individuals that were not included in the training dataset. For individuals that were included in the training dataset, and thus for whom random effects have been estimated, the so-called \gls{blup}, a linear combination of the fixed and random effects, is used to estimate the outcome. The \glspl{blup} should be used when examining residuals, as those residuals can be viewed as an estimate of $\epsilon$ as per \cite[Chapter~10.5]{faraway_extending_2016}. As with ordinary linear models, it is usually recommended to plot the \gls{blup} residuals versus the \gls{blup} fitted values to check for heteroskedasticity, nonconstant variance, etc. In addition, a QQ-plot of the \gls{blup} residuals can be used to identify outliers; \glspl{lmm} are especially vulnerable to outliers because they can skew the estimates of the variance components severely. Lastly, QQ-plots should be examined in order to determine whether the estimated random effects follow a (relatively) Gaussian distribution. 

\subsubsection{Software implementations}

There are two main R packages for the fitting of \gls{lmm}: \href{https://cran.r-project.org/web/packages/nlme/index.html}{\texttt{nlme}} and \href{https://cran.r-project.org/web/packages/lme4/index.html}{\texttt{lme4}}. Both packages support estimation via maximum likelihood or \gls{reml}. We note that software for the fitting of generalized linear mixed models can generally be used as well - for details see Table \ref{table:glmm-software}. While both packages have the same overall aim, there are a few key differences between them. First, \texttt{lme4} is usually faster and can handle larger datasets; this is due to its efficient utilization of the \texttt{Eigen} C++ framework for sparse matrix operations. Second, with \texttt{nlme} it is possible to specify a wide range of covariance structures for the random effects such as AR-1, exponential, Gaussian, and spatial, whereas when using \texttt{lme4} you are limited to diagonal or unstructured structures. In addition, \texttt{nlme} allows you to specify covariance structures for the residuals as well; if left unspecified the within-group errors are assumed to be homoskedastic. Third, \texttt{lme4} can handle crossed random effects while \texttt{nlme} cannot. This is often an important modeling feature when experimental designs are complex. Lastly, both packages support the fitting of nonlinear mixed effects models using a modification of the \gls{nls} approach. See \cite{lindstrom_nonlinear_1990} for the original implementation. Overall, these differences can be broadly summarized by claiming that \texttt{lme4} is more efficient whereas \texttt{nlme} is more flexible. In most situations \texttt{lme4} will do just fine, but if you need to fit a highly complex model - excluding the case of crossed random effects - \texttt{nlme} is the way to go. 

\subsubsection{Example code}

In Source Code \ref{listing:lme4-example} we use the \texttt{lme4} package to fit two different \glspl{lmm}, one with just random intercepts and one with random slopes as well, to a dataset composed of subject reaction times after repeated days of sleep deprivation. Since we fit the models using maximum likelihood we can use the \gls{aic} as defined in Equation \ref{eq:glm-fit-measures} to compare the models; we prefer the model with the random slopes included. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/lme4_random_slopes.R}
\caption{Fitting simple LMMs to the sleepstudy dataset using \texttt{lme4}.}
\label{listing:lme4-example}
\end{listing}

Next, in Source Code \ref{listing:pbkrtest-example} we use the \texttt{pbkrtest::KRmodcomp()} function to perform an F test of a fixed effect for time using the Kenward-Roger DF adjustment. Note: the function requires that both models be fit with REML instead of maximum likelihood; if it detects that the models were not fit with REML it will re-fit them internally using REML before performing the test. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/lme4_kr_comparison.R}
\caption{Testing significance of fixed effects in LMMs using \texttt{pbkrtest}.}
\label{listing:pbkrtest-example}
\end{listing}

In order to test the significance of a random effect we can use the \texttt{RLRsim} package, which implements the parametric bootstrap in order to approximate the null distribution of an LRT or RLRT. A downside of this implementation is that it can only test a single random effect at a time, which we display in Source Code \ref{listing:rlrsim-example}. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/lme4_rlrt.R}
\caption{Testing significance of random effects in LMMs using \texttt{RLRsim}.}
\label{listing:rlrsim-example}
\end{listing}

Lastly, in Source Code \ref{listing:nlme-example} we demonstrate how fit an LMM with a specified correlation structure using \texttt{nlme}. The income dataset consists of individuals' income measured over time (in years) along with several relevant covariates such as education and gender identity. Since time is measured in years and thus is evenly-spaced, we choose an \gls{ar}-1 structure. We also fit another model without age as a covariate, then use the \texttt{anova()} function to perform an F test of age's significance - we can do this because we fit the models via maximum likelihood instead of \gls{reml}. The \textit{p}-value is well above $\alpha = 0.05$, thus we fail to reject the null hypothesis that age has no effect. We also compare the models via \gls{aic}, and see that the model without age is slightly preferable. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/nlme_correlation_structure.R}
\caption{Fitting LMMs with a correlation structure using \texttt{nlme}.}
\label{listing:nlme-example}
\end{listing}

\section{Generalized linear models}\label{sec:tools-glm}

\subsection{Exponential family distributions} \label{sec:exponential-family-glms}

Notation on the definition of an exponential family distribution differs between texts. The notation used in \cite{faraway_extending_2016} is as follows:

\begin{equation}\label{eq:glm-exp-fam-dist-1}
    f(y_i|\theta, \phi) = \text{exp}\left(\frac{y_i\theta - b(\theta)}{a(\phi)} + c(y_i, \phi)\right)
\end{equation}

This notation is also used verbatim in \cite{agresti_foundations_2015} and \cite{hardin_generalized_2018}. Conversely, the notation used in \cite{dobson_introduction_2018} is:

\begin{equation}\label{eq:glm-exp-fam-dist-2}
    f(y_i|\theta) = \text{exp}\left(a(y_i)b(\theta) + c(\theta) + d(y_i)\right)
\end{equation}

Preferring the former notation, we have:

\begin{equation}\label{eq:glm-exp-fam-dist-properties}
    \begin{aligned}
        \mathbb{E}[y_i] &= b^\prime(\theta_i) \\
        \text{Var}(y_i) &= b^{\prime\prime}(\theta_i)a(\phi) \\
    \end{aligned}
\end{equation}

\subsection{Model fitting}

Table \ref{table:links} is adapted from \cite[Chapter~8.1-2]{faraway_extending_2016}, and shows the link and variance functions for several commonly-used distributions.  

\begin{table}[h!]
\centering
\begin{tabular}{||l l l l||} 
 \hline
 \textbf{Distribution} & \textbf{Canonical Link} & \textbf{Variance Function} & \textbf{Use Case} \\ [0.5ex] 
  \hline\hline
  Gaussian & $\eta = \mu$ & 1 & Unbounded continuous response \\ 
  \hline
  Poisson & $\eta = \log(\lambda)$ & $\lambda$ & Count response \\
  \hline
  Binomial & $\eta = \log\left(\frac{p}{1-p}\right)$ & $p(1-p)$ & Binary response \\
  \hline 
  Gamma & $\eta = \mu^{-1}$ & $\mu^2$ & Continuous nonnegative response \\
  \hline
  Inverse Gaussian & $\eta = \mu^{-2}$ & $\mu^3$ & Continuous nonnegative response over time \\ 
  \hline
\end{tabular}
\caption{Descriptions of the canonical links and use cases for common distributions.}
\label{table:links}
\end{table}

Fitting a \gls{glm} can be performed via maximum likelihood estimation using an algorithm called \gls{irwls}. The log-likelihood for a single observation is as follows, where $a_i(\phi) = \frac{\phi}{w_i}$:

\begin{equation}\label{eq:glm-loglik-individual}
    \mathcal{l}(\beta; y_i) = w_i \left(\frac{y_i\theta_i - b(\theta_i)}{\phi} + c(y_i, \phi)\right)
\end{equation}

The full log-likelihood is thus (assuming independent observations):

\begin{equation}\label{eq:glm-loglik-full}
    \mathcal{l}(\beta; y) = \sum_{i=1}^n \mathcal{l}(\beta; y_i)
\end{equation}

The goal is to maximize that quantity over $\beta$, the first step of which is to take the partial derivative with respect to $\beta$:

\begin{equation}\label{eq:glm-score-equation-1}
    \frac{\partial\mathcal{l}}{\partial\beta_j} = \phi^{-1} \sum_{i=1}^n w_i \left(y_i\frac{\partial\theta_i}{\partial\beta_j} - b^\prime(\theta_i)\frac{\partial\theta_i}{\partial\beta_j}\right)
\end{equation}

Leveraging the chain rule:

\begin{equation}\label{eq:glm-chain-rule}
    \frac{\partial\theta_i}{\partial\beta_j} = \frac{\partial\theta_i}{\partial\mu_i}\frac{\partial\mu_i}{\partial\beta_j}
\end{equation}

We know that $\frac{\partial\mu_i}{\partial\theta_i} = b^{\prime\prime}(\theta_i)$, thus we have:

\begin{equation}\label{eq:glm-score-equation-2}
    \frac{\partial\mathcal{l}}{\partial\beta_j} = \phi^{-1} \sum_{i=1}^n \frac{y_i - b^\prime(\theta_i)}{w_i^{-1}b^{\prime\prime}(\theta_i)} \frac{\partial\mu_i}{\partial\beta_j}
\end{equation}

Setting the partial derivatives equal to zero allows us to obtain the \glspl{mle} for all $j = 1, \dots, p$ coefficients, where $V$ is the variance function:

\begin{equation}\label{eq:glm-score-equation-3}
    \sum_{i=1}^n \frac{y_i - \mu_i}{V(\mu_i)} \frac{\partial\mu_i}{\partial\beta_j} = 0 \quad \forall j
\end{equation}

The \gls{irwls} algorithm is (basically) as follows, where $\eta = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p$ is the so-called linear predictor such that $\eta = g(\mu)$ for a given link function $g$:

\begin{algorithm}[h!]
    \caption{The iteratively-reweighted least-squares algorithm.}\label{alg:IRWLS}
    \begin{algorithmic}[1]
        \STATE Provide an initial estimate denoted $\hat{\mu}^{(0)}$
        \FOR{iterations $i = 1$ to $m$}
            \STATE Generate the adjusted dependent variable $z^{(i)} = \hat{\eta}^{(i)} + (y - \hat{\mu}^{(i)})\frac{d\eta}{d\mu}|_{\hat{\eta}^{(i)}}$
            \STATE Generate the weights $\frac{1}{w^{(0)}} = \left(\frac{d\eta}{d\mu}\right)^2|_{\hat{\eta}^{(0)}}V(\hat{\mu}^{(0)})$
            \STATE Re-estimate to obtain $\hat{\beta}^{(i+1)}$ and thus $\hat{\eta}^{(i+1)}$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

The estimated variance of the coefficients is shown below, where $W$ is a diagonal matrix of the individual weights $w_{ii} = \frac{1}{\text{Var}(y_i)}\left(\frac{\partial\eta_i}{\partial\mu_i}\right)^2$:

\begin{equation}\label{eq:glm-beta-variance-est}
    \widehat{\text{Var}}\left(\hat{\beta}\right) = \left(X^\intercal WX\right)^{-1}\hat{\phi}
\end{equation}

The estimator $\hat{\beta}$ is consistent for $\beta$ i.e., $\hat{\beta} \overset{p}{\to} \beta$ as $n \to \infty$. As long as the link function and linear predictor are correctly specified, $\hat{\beta}$ remains consistent even if the probability distribution for $y$ is misspecified. However, if the variance function is misspecified (almost a certainty if the probability distribution of the response is incorrect), the estimated variances of the coefficients will be incorrect. 

\subsection{Hypothesis tests}

Henceforth we will refer to the intercept-only model as the ``null'' model, and the model of interest as the ``alternate'' model; likewise, the term ``saturated'' model refers to a theoretical best model that fully explains the variation in the response. To compare the full model to the model of interest - in order to obtain a measure of how well the model fits - an \gls{lrt} statistic can be generated like so:

\begin{equation}\label{eq:glm-lrt-statistic-1}
    2(\mathcal{l}(y, \phi;y) - \mathcal{l}(\hat{\mu}, \phi;y))
\end{equation}

This can be alternatively formulated as follows, where $\Tilde{\theta}$ denotes the estimates from the saturated model and $\hat{\theta}$ the estimates from the model of interest:

\begin{equation}\label{eq:glm-lrt-statistic-2}
\phi^{-1}\sum_{i=1}^n 2w_i\left(y_i\left(\Tilde{\theta}_i - \hat{\theta}_i\right) - b\left(\Tilde{\theta}_i\right) + b\left(\hat{\theta}_i\right)\right)
\end{equation}

This quantity is known as the scaled deviance, denoted $\frac{D(y, \hat{\mu})}{\phi}$. Table \ref{table:deviance} is adapted from \cite[Chapter~8.3]{faraway_extending_2016}, and shows the deviances for several common \glspl{glm}. We note that it is assumed for the Binomial that $y_i \sim \text{Binomial}(m, p_i)$ and $\mu_i = mp_i$. 

% make table rows larger 
\renewcommand{\arraystretch}{2}

\begin{table}[h!]
\centering
\begin{tabular}{||l l||} 
 \hline
 \textbf{Distribution} & \textbf{Deviance} \\ [0.5ex] 
  \hline\hline
  Gaussian & $\sum_{i=1}^n \left(y_i - \hat{\mu}_i\right)^2$ \\
  \hline
  Poisson & $2\sum_{i=1}^n y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i)$ \\
  \hline
  Binomial & $2\sum_{i=1}^n y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) + (m-y_i)\log\left(\frac{m-y_i}{m-\hat{\mu}_i}\right)$ \\
  \hline 
  Gamma & $2\sum_{i=1}^n -\log\left(\frac{y_i}{\hat{\mu}_i}\right) + \frac{y_i - \hat{\mu}_i}{\hat{\mu}_i}$ \\
  \hline
  Inverse Gaussian & $\sum_{i=1}^n \frac{\left(y_i - \hat{\mu}_i\right)^2}{\hat{\mu}_i^2y_i}$ \\ 
  \hline
\end{tabular}
\caption{Formulae for the deviances of common distributions.}
\label{table:deviance}
\end{table}

% reset table row height
\renewcommand{\arraystretch}{1.5}

Instead of the deviance, Pearson's $\chi$-squared statistic is sometimes used; this test statistic is asymptotically $\chi$-squared distributed with degrees of freedom equal to $n-p$. 

\begin{equation}\label{eq:glm-pearson-chi-squared-statistic}
    \chi^2 = \sum_{i=1}^n \frac{\left(y_i - \hat{\mu}_i\right)^2}{V\left(\hat{\mu}_i\right)}
\end{equation}

Note that this test is only practicably applicable when the dispersion parameter is known e.g., for the Poisson it is $\phi = 1$. For distributions where the dispersion is not known e.g., the Gaussian where $\phi = \sigma^2$, it cannot be used. In this case, in order to compare two nested models (the larger model is denoted by $\Omega$ and the smaller model by $\omega$), the following F-statistic can be computed using the difference in the scaled deviances between models; it is asymptotically F distributed with DF equal to $n - p$:

\begin{equation}\label{eq:glm-f-statistic}
    F = \frac{\frac{D_\omega - D_\Omega}{\text{df}_\omega - \text{df}_\Omega}}{\hat{\phi}}
\end{equation}

To compare the alternate model to the null model, the following \gls{lrt} statistic is used, where $\mathcal{L}_0$ denotes the likelihood of the null model and $\mathcal{L}_1$ that of the alternate model:

\begin{equation}\label{eq:glm-lrt-statistic-nested}
    -2\log\left(\frac{\mathcal{L}_0}{\mathcal{L}_1}\right)
\end{equation}

This quantity is asymptotically $\chi$-squared distributed under the null hypothesis with degrees of freedom equal to the difference in number of estimated parameters between models. 

Next, a Wald test can be used to test individual coefficients. To test the null hypothesis $H_0: \beta = \beta_0$, the statistic is computed using the estimated standard error of $\hat{\beta}$:

\begin{equation}\label{eq:glm-z-statistic}
    Z = \frac{\hat{\beta} - \beta_0}{\widehat{\text{se}}\left(\hat{\beta}\right)}
\end{equation}

This statistic has an asymptotic standard normal distribution under the null hypothesis. In addition, $Z^2$ has an asymptotic $\chi$-squared distribution with \gls{df} equal to 1. 

Lastly, a score test (sometimes referred to as a Lagrange multiplier test) uses the expected curvature of the log-likelihood function to test a null hypothesis $H_0: \beta = \beta_0$; the statistic is asymptotically $\chi$-squared distributed with \gls{df} equal to $p$:

\begin{equation}\label{eq:glm-score-statistic}
    \chi = \frac{\left(\frac{\partial\mathcal{l}(\beta)}{\partial\beta_0}\right)^2}{-\mathbb{E}\left[\frac{\partial^2\mathcal{l}(\beta)}{\partial\beta_0^2}\right]}
\end{equation}

\subsection{Model diagnostics}

Unlike with linear models, where the residuals are defined as $\hat{\epsilon} = y - \hat{\mu}$ (these are denoted ``response residuals'' for \glspl{glm}), modified definitions of residuals are required due to the non-constant variance of the response. The Pearson residual is defined as follows:

\begin{equation}\label{eq:glm-resid-pearson-1}
    r_{P_i} = \frac{y_i - \hat{\mu}_i}{\sqrt{V\left(\hat{\mu}_i\right)}}
\end{equation}

We note that this definition conveniently has the property that the sum of the squared Pearson residuals is equal to Pearson's $\chi$-squared statistic:

\begin{equation}\label{eq:glm-resid-pearson-2}
    \chi^2 = \sum_{i=1}^n r_{P_i}^2
\end{equation}

Next, deviance residuals are defined similarly, such that their sum of their squares is equal to the total deviance of the model:

\begin{equation}\label{eq:glm-resid-deviance-1}
    r_{D_i} = \text{sign}(y_i - \hat{\mu}_i)\sqrt{d_i}
\end{equation}

where:

\begin{equation}\label{eq:glm-resid-deviance-2}
    d_i = 2w_i\left(y_i\left(\Tilde{\theta}_i - \hat{\theta}_i\right) - b\left(\Tilde{\theta}_i\right) + b\left(\hat{\theta}_i\right)\right)
\end{equation}

and:

\begin{equation}\label{eq:glm-resid-deviance-3}
    D(y, \hat{\mu}) = \sum_{i=1}^n r_{D_i}^2
\end{equation}

Similarly to linear models, we can compute the leverage of each point, denoted $h_i$, by extracting the diagonal element of the projection, or ``hat'', matrix. However, the composition of the hat matrix differs from that of linear models (compare to Equation \ref{eq:ols-hat-matrix}):

\begin{equation}\label{eq:glm-hat-matrix}
    H = W^{\frac{1}{2}}X\left(X^\intercal WX\right)^{-1}X^\intercal W^{\frac{1}{2}}
\end{equation}

We can then compute the studentized residuals as follows:

\begin{equation}\label{eq:glm-resid-studentized}
    r_{SD_i} = \frac{r_{D_i}}{\sqrt{\hat{\phi}(1 - h_i)}}
\end{equation}

Lastly, approximate jackknife residuals can be computed as follows, and may be used to detect outliers:

\begin{equation}\label{eq:glm-resid-jackknife}
    r_{JK_i} = \text{sign}(y_i - \hat{\mu}_i)\sqrt{(1-h_i)r_{SD_i}^2 + h_i\left(\frac{r_{P_i}}{\sqrt{1-h_i}}\right)^2}
\end{equation}

The Cook's distance for a given observation measures the change in the estimated coefficients caused by omitting each observation:

\begin{equation}\label{eq:glm-cooks-distance}
    D_i = \frac{\left(\hat{\beta}_{(i)} - \hat{\beta}\right)^\intercal\left(X^\intercal WX\right)\left(\hat{\beta}_{(i)} - \hat{\beta}\right)}{p\hat{\phi}}
\end{equation}

While leverage only measures the potential of each observation to affect the model fit, influence measures like Cook's distance directly assess the effect. 

\subsection{Logistic regression}

Models that assume a binomial distribution with a logit link are referred to a logistic regression. The natural parameter for such models is the log odds. The response variables $y_i$ thus have the following distribution:

\begin{equation}\label{eq:glm-logistic-resp-dist}
    y_i \sim \text{Binomial}(m_i, p_i)
\end{equation}

The logistic regression model thus has the following form:

\begin{equation}\label{eq:glm-logistic-model-1}
    p_i = \frac{e^{X_i\beta}}{1 + e^{X_i\beta}}
\end{equation}

Alternatively, using the logit:

\begin{equation}\label{eq:glm-logistic-model-2}
    \log\left(\frac{p_i}{1 - p_i}\right) = X_i\beta
\end{equation}

Binary data can either be \textit{grouped} or \textit{ungrouped}. Ungrouped data occur when each observation is individually recorded and thus has a zero or one value representing success or failure. This is equivalent to a single Bernoulli trial. Grouped data occur when sets of observations with the same covariate values are summarized, and thus have a count of total successes given $m$ Bernoulli trials. Statistical software such as R's \texttt{glm()} function can handle either form of data. Obviously, ungrouped data can be converted to grouped data without loss of information. When fitting models, the maximum likelihood estimates for $\hat{\beta}$ and the accompanying standard errors will not change between formats, but measures of fit such as the deviance will differ. 

\subsubsection{Coefficient interpretation}

The interpretation for a given coefficient in a logistic regression model differs based on whether the covariate of interest is continuous or discrete. For a continuous covariate, the derivative of $p_i$ with respect to $X_{ij}$ is given by Equation \ref{eq:glm-logistic-interp-continuous}. Thus, $e^{\beta_j}$ is the multiplicative increase in the odds of a success occurring given a unit increase in $X_j$, adjusting for other covariates in the model. Alternatively, on the log scale a unit increase in $X_j$ results in a $\beta_j$ increase in the log odds of success, again while holding other covariates constant. For a discrete covariate, $e^{\beta_j}$ is the \gls{or} versus whatever the reference level of the discrete variable is. 

\begin{equation}\label{eq:glm-logistic-interp-continuous}
    \begin{aligned}
        \frac{\partial p_i}{\partial X_{ij}} 
          &= \beta_j \frac{e^{X_i\beta}}{\left(1 + e^{X_i\beta}\right)^2} \\
          &= \beta_j p_i (1 - p_i) \\
    \end{aligned}
\end{equation}

\subsubsection{Maximum likelihood estimation}

The likelihood for a logistic regression model is given by:

\begin{equation}\label{eq:glm-logistic-likelihood}
    \begin{aligned}
        \mathcal{L}(\beta; y_1, \dots, y_n, X_1, \dots, X_n) 
          &= \prod_{i=1}^n \binom{m}{y_i} p_i^{y_i}(1-p_i)^{m-y_i} \\
          &= \prod_{i=1}^n \binom{m}{y_i} \left(\frac{e^{X_i\beta}}{1 + e^{X_i\beta}}\right)^{y_i}\left(1-\frac{e^{X_i\beta}}{1 + e^{X_i\beta}}\right)^{m-y_i} \\
          &= \prod_{i=1}^n \binom{m}{y_i}\left(\frac{e^{X_i\beta}}{1 + e^{X_i\beta}}\right)^{y_i}\left(\frac{1}{1+e^{X_i\beta}}\right)^{m-y_i} \\ 
    \end{aligned}
\end{equation}

Taking the log provides us with the log-likelihood:

\begin{equation}\label{eq:glm-logistic-loglik}
    \begin{aligned}
        \mathcal{l}(\beta; y_1, \dots, y_n, X_1, \dots, X_n)
          &= \log\left(\prod_{i=1}^n \binom{m}{y_i}\left(\frac{e^{X_i\beta}}{1 + e^{X_i\beta}}\right)^{y_i}\left(\frac{1}{1+e^{X_i\beta}}\right)^{m-y_i}\right) \\
          &= \sum_{i=1}^n \log\left(\binom{m}{y_i}\right) + y_i\log\left(\frac{e^{X_i\beta}}{1+e^{X_i\beta}}\right) + (m-y_i)\log\left(\frac{1}{1+e^{X_i\beta}}\right) \\
          &= \sum_{i=1}^n y_i \log\left(e^{X_i\beta}\right) -m\log\left(1 + e^{X_i\beta}\right) + \log\left(\binom{m}{y_i}\right) \\
          &= \sum_{i=1}^n y_iX_i\beta - m\log\left(1 + e^{X_i\beta}\right) + \log\left(\binom{m}{y_i}\right) \\
    \end{aligned}
\end{equation}

We then derive with respect to $\beta$ in order to obtain the score equation:

\begin{equation}\label{eq:glm-logistic-score-equation}
    \begin{aligned}
        \mathcal{S}(\beta; y_1, \dots, y_n, X_1, \dots, X_n)
          &= \frac{\partial}{\partial\beta} \left(\sum_{i=1}^n y_iX_i\beta - m\log\left(1 + e^{X_i\beta}\right) + \log\left(\binom{m}{y_i}\right)\right) \\
          &= \sum_{i=1}^n y_iX_i -m\left(\frac{e^{X_i\beta}}{1+e^{X_i\beta}}X_i\right) \\
          &= \sum_{i=1}^n X_i\left(y_i - \frac{me^{X_i\beta}}{1+e^{X_i\beta}}\right) \\
    \end{aligned}
\end{equation}

The score equation has no analytical solution, and thus numerical methods must be implemented in order to obtain $\hat{\beta}$; see Algorithm \ref{alg:IRWLS} for a description of such a procedure. 

\subsubsection{Model selection and diagnostics}

In order to assess goodness-of-fit for a logistic regression model, the Hosmer-Lemeshow test statistic may be used. This statistic groups observations into $J$ bins and compares the predicted probabilities within each bin to the true observed proportion:

\begin{equation}\label{eq:glm-logistic-hosmer-lemeshow-statistic}
    \chi^2_{\text{HL}} = \sum_{j=1}^J \frac{\left(y_j - m_j\hat{p}_j\right)^2}{m_j\hat{p}_j\left(1-\hat{p}_j\right)}
\end{equation}

The statistic has an asymptotic $\chi$-squared distribution with $J - 1$ degrees of freedom, allowing for the calculation of \textit{p}-values under the null hypothesis that the observed and expected proportions are the same. Rejection of the null hypothesis thus indicates some lack of fit. Note that a sufficient (defined loosely) number of observations are required per-bin in order for the large sample asymptotics to be accurate. 

Another possible measure of goodness-of-fit is Nagelkerke's pseudo-$R^2$, defined below, where $D$ denotes the deviance of the fitted model:

\begin{equation}\label{eq:glm-logistic-r-squared-nagelkerke}
    R^2 = \frac{1 - \text{exp}\left(\frac{D - D_{\text{null}}}{n}\right)}{1 - \text{exp}\left(-\frac{D_{\text{null}}}{n}\right)}
\end{equation}

Occasionally when fitting logistic regression models one encounters ``overdispersion'' i.e., the variance being greater than expected given the assumed model structure. The dispersion parameter may be estimated as shown below, where the numerator is Pearson's $\chi^2$ statistic:

\begin{equation}\label{eq:glm-logistic-dispersion-estimator}
    \hat{\sigma}^2 = \frac{\sum_{i=1}^n \frac{\left(y_i - n_i\hat{p}_i\right)^2}{n_i\hat{p}_i\left(1-\hat{p}_i\right)}}{n-p}
\end{equation}

With this quantity in hand the estimated variance of $\hat{\beta}$ becomes:

\begin{equation}\label{eq:glm-logistic-dispersion-corrected-beta-variance}
    \widehat{\text{Var}}\left(\hat{\beta}\right) = \hat{\sigma}^2\left(X^\intercal \hat{W} X\right)^{-1}
\end{equation}

To model the dispersion parameter jointly during \gls{glm} fitting the quasi-Binomial family may be used, see \cite[Chapter~3.5]{faraway_extending_2016} for a detailed treatment. 

\subsubsection{Example code}

In Source Code \ref{listing:logistic-example} we fit two logistic regression models to a dataset composed of data concerning the number of times an O-ring was damaged given 6 trials across varying temperatures. We start by fitting a Binomial model, after which we compute a \textit{p}-value using Pearson's $\chi$-squared statistic as defined in Equation \ref{eq:glm-resid-pearson-2}. The \textit{p}-value is approximately 0.14, thus we cannot reject the null hypothesis that the model fits. Regardless, we estimate the dispersion parameter as detailed in Equation \ref{eq:glm-poisson-overdispersion}; it's only slightly larger than one, indicating that the data are likely not overdispersed. Just in case, we also fit a quasi-Binomial model that allows the dispersion to vary. In this case we would prefer the ordinary Binomial model for simplicity, as the overdispersion does not appear to be severe. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/quasi_binomial.R}
\caption{Fitting a series of logistic regression models to the O-ring dataset.}
\label{listing:logistic-example}
\end{listing}

\subsection{Multinomial regression}

The defining characteristic of Binomial data and logistic regression is that the response can only take two values. However, we often encounter data with a categorical response variable that can take multiple values e.g., survey data. This motivates the technique of \textit{multinomial regression}, which generalizes logistic regression to $c > 2$ possible responses. Multinomial regression can take two forms: 1) \textit{nominal} data, where the order of the response categories does not matter and 2) \textit{ordinal} data, where the response categories do have an inherent order. In either paradigm, we impose the following constraint on the estimated probabilities of each category for observation $i$:

\begin{equation}\label{eq:glm-multinom-prob-constraint}
    \sum_{r=1}^c \pi_{ir} = 1
\end{equation}

Suppose we observe $n$ \gls{iid} observations, with $y_r, r = 1, \dots, c$ observations falling into each of the $c$ categories. Thus we have:

\begin{equation}\label{eq:glm-multinom-y-equals-n}
    \sum_{r=1}^c y_r = n
\end{equation}

The \gls{pmf} of the Multinomial distribution is then:

\begin{equation}\label{eq:glm-multinom-pmf}
    f(y | n) = \frac{n!}{\prod_{r=1}^c y_r!} \prod_{r=1}^c \pi_r^{y_r}
\end{equation}

As per \cite[Chapter~8.2]{dobson_introduction_2018}, the Multinomial distribution is not strictly of the exponential family of distributions. However, the Multinomial can be regarded as the joint distribution of $c$ Poisson \glspl{rv} conditional upon their sum $n$, and thus the usage of the \gls{glm} framework is justified. 

\subsubsection{Nominal logistic regression}

As mentioned earlier, in nominal logistic regression the response categories have no inherent order. As such, it does not matter which category we assign as the baseline category; going forward we assume that the first category has been assigned as the baseline. With our baseline category defined, we can define the logits for the other categories with respect to the baseline:

\begin{equation}\label{eq:glm-nominal-logits}
    \log\left(\frac{\pi_r}{1 - \pi_r}\right) = \log\left(\frac{\pi_r}{\pi_1}\right) = \beta_r X_r \quad \forall \: r = 2, \dots, c
\end{equation}

The system of $c - 1$ equations as defined above are used to estimate $\hat{\beta}_j$, after which we can define the predicted probabilities:

\begin{equation}\label{eq:glm-nominal-pred-prob}
    \begin{aligned}
        \hat{\pi}_1 &= \frac{1}{1 + \sum_{r=2}^c \text{exp}\left(\hat{\beta}_r X_r\right)} \\
        \hat{\pi}_r &= \frac{\text{exp}\left(\hat{\beta}_r X_r\right)}{1 + \sum_{r=2}^c \text{exp}\left(\hat{\beta}_r X_r\right)} \quad \forall \: r = 2, \dots, c \\
    \end{aligned}
\end{equation}

Model residuals and diagnostics are generally the same as those performed for logistic regression e.g., Pearson's $\chi$-squared statistic and the deviance both are asymptotically $\chi^2$-distributed with \gls{df} equal to $n - p$. The \gls{lrt} and the \gls{aic} can be used to perform model selection. For a good look at how nominal models are fit in practice, refer to \cite[Chapter~6.1.3]{agresti_foundations_2015}. 

\subsubsection{Ordinal logistic regression}

Ordinal logistic regression is slightly more complicated than nominal logistic regression, since the order of the categories matters. Several different frameworks may be used, depending on the desired interpretation of the coefficients. For a detailed overview see \cite[Chapter~8.4]{dobson_introduction_2018}. To start, the \textit{cumulative logit model} is defined as:

\begin{equation}\label{eq:glm-ordinal-cumulative-logit}
    \log\left(\frac{\pi_1 + \dots + \pi_r}{\pi_{r+1} + \dots + \pi_c}\right) = \beta_r X_r
\end{equation}

The \textit{proportional odds model} assumes that the effects of the covariates are the same on the log scale for all categories, while the intercept differs between categories:

\begin{equation}\label{eq:glm-ordinal-prop-odds}
    \log\left(\frac{\pi_1 + \dots + \pi_r}{\pi_{r+1} + \dots + \pi_c}\right) = \beta_{0r} + \beta_1 X_1 + \dots + \beta_p X_p
\end{equation}

The \textit{adjacent categories logit model} considers the ratios of probabilities of successive categories:

\begin{equation}\label{eq:glm-ordinal-adjacent-categories}
    \log\left(\frac{\pi_r}{\pi_{r+1}}\right) = \beta_r X_r
\end{equation}

Lastly, the \textit{continuation ratio logit model} models the ratio of probabilities:

\begin{equation}\label{eq:glm-ordinal-continuation-ratio}
    \log\left(\frac{\pi_r}{\pi_{r+1} + \dots + \pi_c}\right) = \beta_r X_r
\end{equation}

For a description of how cumulative link models such as those described above are fit, see \cite[Chapter~6.2.4]{agresti_foundations_2015}. 

\subsubsection{Software implementations}

There aren't a lot of available software packages for the fitting of multinomial models. In general I would recommened using the \texttt{VGAM} R package, as it supports both nominal and ordinal models and implements a large number of different link functions. There is also rather good documentation online, which can be accessed \href{https://www.stat.auckland.ac.nz/~yee/VGAM/}{here}. For simple nominal problems though the \texttt{nnet} R package, which ships with base R, might be preferred. 

\begin{table}[h!]
\centering
\begin{tabular}{||l l l||} 
  \hline
  \textbf{R Package} & \textbf{Function} & \textbf{Notes} \\ [0.5ex]
  \hline\hline
  \href{https://cran.r-project.org/web/packages/VGAM/index.html}{\texttt{VGAM}} & \texttt{vglm()} & Supports widest variety of nominal and ordinal model classes \& link functions \\
  \hline
  \href{https://cran.r-project.org/web/packages/nnet/index.html}{\texttt{nnet}} & \texttt{multinom()} & Only supoorts nominal logistic regression \\
  \hline 
  \href{https://cran.r-project.org/web/packages/MASS/index.html}{\texttt{MASS}} & \texttt{polr()} & Supports 5 different link functions for proportional odds logistic regression \\
  \hline 
\end{tabular}
\caption{Software packages for the fitting of multinomial logistic regression models.}
\label{table:multinomial-software}
\end{table}

Outside of R, SAS has \href{https://support.sas.com/documentation/cdl/en/statug/63347/HTML/default/viewer.htm#statug_logistic_sect004.htm}{\texttt{PROC LOGISTIC}} which supports multinomial regression by specifying \texttt{link=glogit}, Python has \href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{\texttt{sklearn.linear\_model.LogisticRegression}} and \href{https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.html}{statsmodels.discrete.discrete\_model.MNLogit}, and the Julia package \href{https://juliahub.com/ui/Packages/General/Econometrics}{\texttt{Econometrics.jl}} implements functionality to fit nominal and ordinal logistic regression models. 

\subsubsection{Example code}

In Source Code \ref{listing:vgam-nominal-example} we demonstrate the usage of the \texttt{VGAM} package to fit a nominal logistic regression model to a dataset describing the types of food Florida alligators prefer based on what lake they live in and whether they are adults or young. For a detailed description of the dataset see \cite[Chapter~6.3.2]{agresti_foundations_2015}. After we fit the model, we perform a goodness-of-fit test using the asymptotic $\chi^2$ approximation for the deviance; we obtain a \textit{p}-value of approximately 0.15, thus we fail to reject the null hypothesis of no lack of fit. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/nominal_logistic_regression.R}
\caption{Fitting a nominal logistic regression model to the alligator dataset using \texttt{VGAM}.}
\label{listing:vgam-nominal-example}
\end{listing}

Next, in Source Code \ref{listing:mass-nominal-example} we investigate a dataset consisting of mental health impairment as a function of socioeconomic status and a factor called the life events index that measures how many severe life events each subject experienced during the preceding 3 years. For a detailed description of the dataset see \cite[Chapter~6.3.3]{agresti_foundations_2015}. We utilize the \texttt{polr()} function from the \texttt{MASS} package to fit a proportional odds logistic regression model. After fitting the model, we use the deviance to test goodness-of-fit. Unfortunately, our \textit{p}-value is well below our threshold of $\alpha = 0.05$, thus we reject the null hypothesis of no lack of fit. This could indicate that the proportional odds structure is inappropriate for the data, so we next utilize the \texttt{VGAM} package to fit the same model but with the proportional odds assumption relaxed. We compute a \textit{p}-value based on the deviance and residual \gls{df} and see that it is approximately 0.83, thus we fail to reject the null hypothesis of no lack of fit for the new model. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/ordinal_logistic_regression.R}
\caption{Fitting an ordinal logistic regression model to the mental health dataset using \texttt{MASS}.}
\label{listing:mass-nominal-example}
\end{listing}

\subsection{Count regression}

The distribution most commonly used for count data is the Poisson i.e., we assume $y_i \sim \text{Poisson}(\lambda)$. The canonical link is the natural log, ergo:

\begin{equation}\label{eq:glm-poisson-canonical-link}
    \log(\lambda) = \beta X_i
\end{equation}

The \gls{pmf} for a Poisson regression is thus:

\begin{equation}\label{eq:glm-poisson-resp-dist}
    f(y_i|\lambda) = \frac{e^{\beta X_iy_i}e^{-e^{\beta X_i}}}{y_i!}
\end{equation}

The likelihood function is the product of the \gls{iid} \glspl{pmf}:

\begin{equation}\label{eq:glm-poisson-likelihood}
    \mathcal{L}(\beta; y_1, \dots, y_n, X_1, \dots, X_n) = \prod_{i=1}^n \frac{e^{\beta X_iy_i}e^{-e^{\beta X_i}}}{y_i!}
\end{equation}

Taking the log provides us with the log-likelihood:

\begin{equation}\label{eq:glm-poisson-loglik}
    \begin{aligned}
        \mathcal{l}(\beta; y_1, \dots, y_n, X_1, \dots, X_n)
          &= \log\left(\prod_{i=1}^n \frac{e^{\beta X_iy_i}e^{-e^{\beta X_i}}}{y_i!}\right) \\
          &= \sum_{i=1}^n \log\left(e^{\beta X_iy_i}\right) + \log\left(e^{-e^{\beta X_i}}\right) - \log(y_i!) \\
          &= \sum_{i=1}^n \beta X_iy_i - e^{\beta X_i} - \log(y_i!) \\
    \end{aligned}
\end{equation}

Deriving with respect to $\beta$ generates the score equation, which is then set equal to zero and solved numerically via \gls{irwls}; see Algorithm \ref{alg:IRWLS} for details. 

\begin{equation}\label{eq:glm-poisson-score-equation}
    \begin{aligned}
        \mathcal{S}(\beta; y_1, \dots, y_n, X_1, \dots, X_n)
          &= \frac{\partial}{\partial\beta} \left(\sum_{i=1}^n \beta X_iy_i - e^{\beta X_i} - \log(y_i!)\right) \\
          &= \sum_{i=1}^n X_iy_i -X_ie^{\beta X_i} \\
          &= \sum_{i=1}^n X_i \left(y_i - e^{\beta X_i}\right) \\
    \end{aligned}
\end{equation}

Sometimes the variance in a count regression model is greater than that of the mean i.e., overdispersion occurs. The dispersion parameter can be estimated as shown below, where the numerator is Pearson's $\chi$-squared statistic:

\begin{equation}\label{eq:glm-poisson-overdispersion}
    \hat{\phi} = \frac{\sum_{i=1}^n\frac{\left(y_i - \hat{\mu}_i\right)^2}{\hat{\mu}_i}}{n-p}
\end{equation}

The variance then becomes:

\begin{equation}\label{eq:glm-poisson-overdispersed-variance}
    \text{Var}(y) = \phi\mu
\end{equation}

An implementation of a test for overdispersion can be found in \texttt{odTest()} function of the \texttt{pscl} package. It uses a modified \gls{lrt} to compare a Poisson and Negative-binomial GLM and provides both a $\chi$-squared test statistic and accompanying \textit{p}-value. 

When comparing nested models with overdispersion, such as Negative-binomial or quasi-Poisson models, an F test should be preferred over a $\chi$-squared test. An example of how to do this can be found in Source Code \ref{listing:count-regression-example}. 

\subsubsection{Example code}

The Galapagos dataset consists of several geographical covariates that are hypothesized to be linked to the number of unique plant species on each island. Since our response is a count, we start by fitting a Poisson \gls{glm} to the data, using all covariates as predictors. Using Pearson's $\chi$-squared test for goodness-of-fit, we reject the null hypothesis of no lack of fit at the $\alpha = 0.05$ level. In addition, when estimating the dispersion parameter we see that it is well over 1, indicating that the Poisson distribution is not a good fit for this dataset. This motivates us to fit a Negative-binomial model, which we do using \texttt{MASS::glm.nb()}. After fitting, we again compute a \textit{p}-value for the null hypothesis that there is no lack of fit using the deviance (as mentioned earlier we cannot use Pearson's $\chi$-squared test here because the data are overdispersed); the \textit{p}-value is above our cutoff of $\alpha = 0.05$ and thus we fail to reject the null. Lastly, using the \texttt{pscl} package we perform a significance test for the overdispersion parameter $\theta$ and see that it is highly significant at the $\alpha = 0.05$ level. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/count_regression.R}
\caption{Fitting Poisson and Negative-binomial regression models to the Galapagos dataset.}
\label{listing:count-regression-example}
\end{listing}

\subsection{Model selection}

The following is adapted from \cite[Chapter~4.6]{agresti_foundations_2015}. The goal of model selection is to identify the most parsimonious model i.e., the model that best explains variation in the response while remaining interpretable. This is often done by comparing measures of model fit that are penalized by the number of parameters in the model such as adjusted $R^2$, the \gls{aic}, or the \gls{bic}), the formulae for which are shown below. Note: while adjusted $R^2$ works for ordinary linear models, extensions such as pseudo-$R^2$ are necessary for \glspl{glm}. Several versions of pseudo-$R^2$ exist (Cox \& Snell, Nagelkerke, McFadden, etc.), but they share the characteristic that they cannot be interpreted as the proportion of variance explained in the response as is done in linear regression. 

\begin{equation}\label{eq:glm-fit-measures}
    \begin{aligned}
        \text{Adjusted } R^2 &= 1 - \frac{(1 - R^2)(n-1)}{n - p - 1} \\
        \text{AIC} &= 2\left(p - \mathcal{L}\left(\hat{\beta}\right)\right) \\
        \text{BIC} &= \log(n)p - 2\mathcal{L}\left(\hat{\beta}\right) \\
    \end{aligned}
\end{equation}

\Gls{aic} seeks to minimize the expected Kullback-Leibler divergence between the true and fitted probability distributions, with $\hat{\beta}$ treated as a random variable. \Gls{bic} is based on the idea of identifying which candidate model has the highest posterior probability; in general, \gls{bic} is more conservative i.e., tends to select simpler models than \gls{aic} due to its usage of $\log(n)$ as the penalty on the number of parameters instead of the value of 2 used when calculating \gls{aic}. 

A simple method of model selection is subset selection, of which there are various forms. Given $p$ potential covariates, there are $2^p$ possible candidate models. Best subset selection compares all possible models, which is often impractical in practice for obvious reasons. Forward selection starts with the null model and sequentially adds terms, while backwards elimination begins with the saturated model and sequentially removes terms. In general, stepwise model selection techniques should be used with caution. 

Like with linear models, regularization techniques such as ridge regression and the \gls{lasso} can be used to bias the estimated coefficients towards zero. These techniques are implemented for several common distributions in the \texttt{glmnet} R package. 

\subsection{Generalized estimating equations}

The technique of \glspl{gee} allows the modeling of longitudinal or clustered data i.e., data with multiple observations per-subject, via the specification of a so-called ``working correlation matrix'' that is estimated from the data. This estimate determines the \gls{gee} estimates of the coefficients and their standard errors. The working correlation matrix is denoted $R_i(\alpha)$, and leads to the working covariance matrix, where $\mu_i = g^{-1}(\beta X_i)$:

\begin{equation}\label{eq:glm-gee-working-covariance-matrix}
    V(\mu_i) = \text{Diag}\left(V(\mu_{it})\right)^\frac{1}{2} R_i(\alpha) \text{Diag}\left(V(\mu_{it})\right)^\frac{1}{2}
\end{equation}

The score function used to obtain $\hat{\beta}$ is given in its expanded form by:

\begin{equation}\label{eq:glm-gee-score-equation-beta-1}
    \mathcal{S}(\beta) = \sum_{i=1}^n \sum_{t=1}^{n_i} \frac{y_{it} - \mu_{it}}{a(\phi)V(\mu_{it})} \left(\frac{\partial\mu}{\partial\eta}\right)_{it} X_{it}
\end{equation}

In matrix form, where $i$ indicates the complete panel of observations from subject $i$, we have:

\begin{equation}\label{eq:glm-gee-score-equation-beta-2}
    \mathcal{S}(\beta) = \sum_{i=1}^n X_i^\intercal \text{Diag}\left(\frac{\partial\mu}{\partial\eta}\right)_i V^{-1}(\mu_i) \left(\frac{y_i - \mu_i}{a(\phi)}\right)
\end{equation}

Likewise, the score equation for is defined as follows, where $W_i$ is the vector of length $n_i$ of Pearson residuals for individual $i$, $H_i = \text{Diag}\left(V(W_{ij})\right)$, and $\xi_i = \mathbb{E}[W_i]$:

\begin{equation}\label{eq:glm-gee-score-equation-alpha}
    \mathcal{S}(\alpha) = \sum_{i=1}^n \left(\frac{\partial\xi_i}{\partial\alpha}\right)^\intercal H_i^{-1} (W_i - \xi_i)
\end{equation}

The \gls{gee} fitting algorithm estimates $\hat{\beta}$ by switching between estimating the coefficients given moment estimates of $\alpha$ and $\phi$, and estimating those quantities given an estimate for $\beta$. This is performed again using the \gls{irwls} algorithm (see Algorithm \ref{alg:IRWLS}), which, as per \cite[Chapter~4]{hardin_generalized_2012} ``is a modification of the Newton-Raphson algorithm in which the expected Hessian matrix is substituted for the observed Hessian''; this is also referred to as Fisher scoring. For the original details on this algorithm see the seminal paper that introduced \glspl{gee}, \cite{liang_longitudinal_1986}. 

\subsubsection{Correlation structures}

A variety of correlation structures are supported (depending on the software used); the most common structures are shown in Table \ref{table:correlation-structures}. Other correlation structures that are less commonly used or supported by software include the stationary and non-stationary M-dependent, exponential, and Gaussian structures. Note that, depending on the reference, the exchangeable correlation structure is sometimes also referred to as compound symmetry. In addition, a common software requirement for fitting \glspl{gee} is that the data be sorted by subject and by observation time. 

\begin{table}[h!]
\centering
\begin{tabular}{||l l||} 
 \hline
 \textbf{Correlation Structure} & \textbf{Assumptions} \\ [0.5ex] 
  \hline\hline
  Independence & Observations within subjects are uncorrelated \\ 
  \hline
  Exchangeable & Pairs of observations within subjects have the same correlation \\
  \hline
  AR1 & Correlation within subjects decreases over (evenly-spaced) time \\
  \hline 
  Unstructured & Correlations between pairs of observations are unique and estimated from the data \\ 
  \hline
\end{tabular}
\caption{Descriptions of the use cases for each GEE correlation structure.}
\label{table:correlation-structures}
\end{table}

In practice, the exchangeable correlation parameter is estimated via:

\begin{equation}\label{eq:glm-gee-alpha-exchangeable}
    \hat{\alpha} = \hat{\phi}^{-1}\sum_{i=1}^n \frac{\sum_{u=1}^{n_i}\sum_{v=1}^{n_i} \hat{r}_{iu}\hat{r}_{iv} - \sum_{u=1}^{n_i} \hat{r}^2_{iu}}{n_i(n_i-1)}
\end{equation}

where the Pearson residual for individual $i$ at time $t$ is estimated by:

\begin{equation}\label{eq:glm-gee-pearson-residual}
    \hat{r}_{it} = \frac{y_{it} - \hat{\mu}_{it}}{\sqrt{V(\mu_{it})}}
\end{equation}

and the dispersion parameter $\phi$ is estimated by:

\begin{equation}\label{eq:glm-gee-phi-hat}
    \hat{\phi} = \left(-p + \sum_{i=1}^n n_i\right)^{-1} \sum_{i=1}^n \sum_{t=1}^{n_i} \hat{r}^2_{it}
\end{equation}

Finally, the working correlation matrix can be simply defined as:

\begin{equation}\label{eq:glm-gee-R-alpha-exchangeable}
    R_{uv} = \begin{cases}
        1 & \text{if } u = v \\
        \alpha & \text{otherwise} \\
    \end{cases}
\end{equation}

Conversely, the estimate of $\alpha$ for an \gls{ar} process with $k$ lags is given by the following vector:

\begin{equation}\label{eq:glm-gee-alpha-ar-1}
    \hat{\alpha} = \hat{\phi}^{-1} \sum_{i=1}^n \frac{\sum_{t=1}^{n_i-0}\hat{r}_{i,t}\hat{r}_{i,t+0}}{n_i}, \dots, \frac{\sum_{t=1}^{n_i-k}\hat{r}_{i,t}\hat{r}_{i,t+k}}{n_i}
\end{equation}

The correlation between observations $y_{it}$ and $y_{is}$ is then:

\begin{equation}\label{eq:glm-gee-alpha-ar-2}
    \text{Corr}(y_{it}, y_{is}) = \alpha^{|t-s|}
\end{equation}

Finally, the unstructured correlation structure is estimated fully empirically from the data, with every entry in $R(\alpha)$ being nonzero. See \cite[Chapter~3.2.1]{hardin_generalized_2012} for a fully detailed derivation, with a simplified version presented below. When utilizing an unstructured correlation structure it is important to be cognizant of the computational complexity of the model being fitted, as well as of possibility that $R(\alpha)$ might not be invertible. These problems may be exacerbated when unbalanced data are present i.e., when the number of observations per-individual varies considerably. 

\begin{equation}\label{eq:glm-gee-alpha-unstructured}
    R(\alpha) = \hat{\alpha} = \frac{\sum_{i=1}^n n_i}{\sum_{i=1}^n\sum_{t=1}^{n_i} n_i^{-1}\hat{r}^2_{i,t}} \begin{bmatrix}
        \hat{r}_{i,1}\hat{r}_{i,1} & \dots & \hat{r}_{i,1}\hat{r}_{i,n_i} \\
        \vdots & \ddots & \vdots \\
        \hat{r}_{i, n_i}\hat{r}_{i,1} & \dots & \hat{r}_{i,n_i}\hat{r}_{i,n_i} \\ 
    \end{bmatrix}
\end{equation}

\subsubsection{Variance estimation}

A unique property of \glspl{gee} is that their estimated coefficients are robust to misspecification of the working correlation structure; this is due to their leveraging of a modified sandwich variance estimator. A benefit of this approach is that the sandwich standard errors for the coefficients can be used to estimate Wald statistics for covariate significance. In addition, the coefficients estimated by \gls{gee} solvers carry the same interpretation as that of a \gls{glm} of the same family. As per \cite[Chapter~13.5]{faraway_extending_2016} we have that, regardless of how the variance is specified:

\begin{equation}\label{eq:glm-gee-beta-asymptotics-1}
    \hat{\beta} \overset{p}{\to} \beta
\end{equation}

and as per \cite[Chapter~3]{hardin_generalized_2012}, the modified sandwich variance estimator is given by:

\begin{equation}\label{eq:glm-gee-beta-asymptotics-2}
    \begin{aligned}
        V &= B^{-1} M \left(B^{-1}\right)^T \\
        B &= \begin{bmatrix}
            -\frac{\partial\mathcal{S}(\beta}{\partial\beta} & -\frac{\partial\mathcal{S}(\beta)}{\partial\alpha} \\
            -\frac{\partial\mathcal{S}(\alpha)}{\partial\beta} & -\frac{\partial\mathcal{S}(\alpha)}{\partial\alpha} \\
        \end{bmatrix} \\
        M &= \sum_{i=1}^n \left(\sum_{t=1}^{n_i} \begin{pmatrix}
            \mathcal{S}(\beta)_{it} \\
            \mathcal{S}(\alpha)_{it} \\
        \end{pmatrix}\right)\left(\sum_{t=1}^{n_i} \begin{pmatrix}
            \mathcal{S}(\beta)_{it} \\
            \mathcal{S}(\alpha)_{it} \\
        \end{pmatrix}\right)^\intercal \\ 
    \end{aligned}
\end{equation}

Ergo, we have the asymptotic distribution of the estimated coefficients:

\begin{equation}\label{eq:glm-gee-beta-asymptotics-3}
    \sqrt{n}\left(\hat{\beta} - \beta\right) \overset{d}{\to} \mathcal{MVN}\left(0, V\left(\hat{\beta}\right)\right)
\end{equation}

\subsubsection{Model selection and diagnostics}

A major downside of \glspl{gee} is that, due to their lack of a true likelihood, likelihood-based measures for detecting model fit, comparing models, and performing inference cannot be applied. In particular this complicates model selection (both with respect to correlation structure and covariates), as \gls{aic} and \gls{bic} cannot be computed. Fortunately, there exists a criterion designated the \gls{qic} that can help; see \cite{pan_akaikes_2001} for the original implementation. The statistic is defined below, where the model is fitted and $\hat{\beta}$ estimated under the desired correlation structure $R(\alpha)$. However, regardless of the correlation structure used to fit the model, the quasi-likelihood is calculated under the assumption of an independent correlation structure. As with \glspl{ic}, models with a smaller value for \gls{qic} are preferable. In general, one should favor the choice of a correlation structure based on the experimental design as detailed in Table \ref{table:correlation-structures}; this is because the QIC tends to favor the unstructured correlation structure, or, more generally, correlation structures with higher numbers of parameters. See either \cite{wang_perils_2015} or \cite[Chapter~4.1]{hardin_generalized_2012} for alternate, more detailed explanations of the pros and cons of utilizing QIC for model selection.

\begin{equation}\label{eq:glm-gee-qic-1}
    \text{QIC} = -2Q\left(\hat{\beta}, \hat{\phi}; I, \frac{\partial\mu}{\partial\beta}\right) + 2 \text{trace}\left(\hat{\Omega}\hat{V}_G\right)
\end{equation}

where $\hat{\Omega}$ is estimated under the independent correlation structure regardless of the correlation structure specified in the fitted model:

\begin{equation}\label{eq:glm-gee-qic-2}
    \hat{\Omega} = -\frac{\partial^2Q\left(\beta; I, \frac{\partial\mu}{\partial\beta}\right)}{\partial\beta\partial\beta^\intercal} \bigg |_{\beta = \hat{\beta}}
\end{equation}

and the quasi-likelihood is defined by the following integral:

\begin{equation}\label{eq:glm-gee-qic-3}
    Q(\mu, \phi; y) = \int_y^\mu \frac{y - t}{\phi V(t)} dt
\end{equation}

The quasi-likelihoods for several exponential family distributions are shown in Table \ref{table:quasi-likelihoods}, which is adapted from \cite[Chapter~4]{hardin_generalized_2012}. We note that differing but similar notation is used in \cite[Chapter~2.5.1]{wakefield_bayesian_2013}. 

% make table rows larger
\renewcommand{\arraystretch}{2}

\begin{table}[h!]
\centering
\begin{tabular}{||l l||} 
  \hline
  \textbf{Distribution} & \textbf{Quasi-likelihood} \\ [0.5ex]
   \hline\hline
   Gaussian & $-\frac{1}{2} \sum_{i=1}^n (y_i - \mu)^2$ \\
   \hline
   Binomial & $\sum_{i=1}^n y_i \log\left(\frac{\mu}{1-\mu}\right) + \log(1-\mu)$ \\
   \hline 
   Negative-binomial & $\sum_{i=1}^n y_i\log(\alpha\mu) - \left(\frac{y+1}{\alpha}\right)\log(1 + \alpha\mu)$ \\
   \hline
   Poisson & $\sum_{i=1}^n y_i\log(\mu) - \mu$ \\
   \hline
   Gamma & $-\sum_{i=1}^n \mu^{-1}y_i + \log(\mu)$ \\
   \hline
   Inverse Gaussian & $\sum_{i=1}^n -\frac{y_i}{2\mu^2} + \mu^{-1}$ \\ 
   \hline
\end{tabular}
\caption{Quasi-likelihoods for several common distributions.}
\label{table:quasi-likelihoods}
\end{table}

% reset table row height
\renewcommand{\arraystretch}{1.5}

An alternative to the original \gls{qic} measure designed specifically for the selection of the best subset of covariates (and \textit{not} for the selection of a correlation structure) is the so-called $\text{QIC}_u$. The criterion is defined in Equation \ref{eq:glm-gee-qic-4}. Like the original \gls{qic} it assumes an independent correlation structure when computing the quasi-likelihood, but does so with the $\hat{\beta}$ values estimated using the specified correlation structure. Unlike the original \gls{qic}, the $\text{QIC}_u$ explicitly penalizes for the number of covariates $p$. As with all \glspl{ic}, smaller values indicate more parsimonious models. 

\begin{equation}\label{eq:glm-gee-qic-4}
    \text{QIC}_u = -2 \left(Q\left(\hat{\beta}, \hat{\phi}; I, \frac{\partial\mu}{\partial\beta}\right) - p\right)
\end{equation}

An alternative measure of fit is the concordance correlation, which compares the observed and fitted values from a model. It is defined as:

\begin{equation}\label{eq:glm-gee-concordance-correlation}
    r_c = \frac{2 \sum_{i=1}^n \sum_{t=1}^{n_i} \left(y_{it} - \bar{y}\right)\left(\hat{y}_{it} - \bar{\hat{y}}\right)}{\sum_{i=1}^n \sum_{t=1}^{n_i} \left(y_{it} - \bar{y}\right)^2 + \sum_{i=1}^n \sum_{t=1}^{n_i} \left(\hat{y}_{it} - \bar{\hat{y}}\right)^2}
\end{equation}

Like with standard \glspl{glm}, Cook's distance can be used to estimate the effect of outliers on the response variable. After calculating the values, graphical inspection is generally preferred as a means of identifying potentially problematic observations. An added wrinkle is that this metric may be calculated in two different ways due to the nature of repeated measures data i.e., it can be calculated based on the deletion of all $n_i$ observations for a single subject, or based on the deletion of just a single observation from individual $i$ and time $t$. The former is defined as:

\begin{equation}\label{eq:glm-gee-cooks-distance-panel}
    \begin{aligned}
        D_i &= \left(p\hat{\phi}\right)^{-1} S_i^\intercal \left(W_i^{-1} -  Q_i^{-1}\right)^{-1} Q_i \left(W_i^{-1} - Q_i^{-1}\right)^{-1} S_i \\
        S_i &= y_i - g^{-1}\left(\hat{\eta}_i\right) \\
        Q &= X\left(X^\intercal W X\right)^{-1} X^\intercal \\
        W_i &= \left[\hat{r}_{i,1}\hat{r}_{i,2}, \dots, \hat{r}_{1, n_i-1}\hat{r}_{1,n_i}\right]^\intercal
    \end{aligned}
\end{equation}

and the latter as:

\begin{equation}\label{eq:glm-gee-cooks-distance-observation}
    D_i = \frac{S_{it}^2Q_{it}}{p\hat{\phi}\left(W_{it}^{-1} - Q_{it}\right)^2}
\end{equation}

As mentioned previously, the modified sandwich variance-covariance matrix can be used to compute Wald tests for the null hypothesis that $\beta = \beta_0$ (usually with $\beta_0 = 0$). The generalized Wald test statistic is defined below, and has an asymptotic $\Chi^2$ distribution with degrees of freedom equal to the length of the coefficient vector being tested:

\begin{equation}\label{eq:glm-gee-generalized-wald-stat}
    Z = n \left(\hat{\beta} - \beta_0\right)^\intercal V^{-1} \left(\hat{\beta} - \beta_0\right)
\end{equation}

In addition, the modified sandwich variance-covariance matrix can be used to construct a generalized score test, where $\mathcal{S}(\beta_0)$ indicates the estimating equation under the null hypothesis $H_0: \beta = \beta_0$:

\begin{equation}\label{eq:glm-gee-generalized-score-stat}
    T = n^{-1} \mathcal{S}(\beta_0)^\intercal V \mathcal{S}(\beta_0)
\end{equation}

\subsubsection{Software implementations}

Lastly, Table \ref{table:gee-software} lists the various \gls{gee} solvers available in R along with their supported exponential family distributions. In practice, the \texttt{geeM} package is a good option as it is relatively fast and supports the widest array of distributions, though the \texttt{geepack} package is probably the most widely-used option. In addition, the \texttt{geepack} and \texttt{gee} packages have their link and variance functions hardcoded in the underlying C code, making it impossible to supply custom options, whereas the \texttt{geeM} package allows the user to supply their own link and variance functions. For details on this and a comparison between the architectures see \cite{mcdaniel_fast_2013}. For a comparison of different \gls{gee} implementations' performance on ordinal data see \cite{nooraee_gee_2014}. 

\begin{table}[h!]
\centering
\begin{tabular}{||l l||} 
  \hline
  \textbf{R Package} & \textbf{Supported Distributions} \\ [0.5ex]
   \hline\hline
   \texttt{geepack} & Gaussian, Binomial, Gamma, Inverse Gaussian, Poisson, Quasi, Ordinal \\
   \hline
   \texttt{gee} & Gaussian, Binomial, Poisson, Gamma, Quasi \\
   \hline 
   \texttt{multgee} & Nominal, Ordinal \\
   \hline
   \texttt{repolr} & Ordinal \\
   \hline 
   \texttt{geeM} & Gaussian, Binomial, Poisson, Gamma, Negative-binomial, Inverse Gaussian, Quasi \\ 
   \hline
\end{tabular}
\caption{Software packages for the fitting of GEEs.}
\label{table:gee-software}
\end{table}

Outside of R, SAS provides \href{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#genmod_toc.htm}{\texttt{PROC GENMOD}}, Python's \href{https://www.statsmodels.org/stable/gee.html}{\texttt{statmodels.genmod.generalized\_estimating\_equations}} implements \glspl{gee} for several exponential family distributions, and Julia has the \href{https://github.com/kshedden/EstimatingEquationsRegression.jl}{\texttt{EstimatingEquationsRegression.jl}} package. 

\subsubsection{Example code}

In Source Code \ref{listing:geem-example} we utilize the \texttt{geeM} package to fit a Poisson \gls{gee} and then a Negative-binomial \gls{gee}. This requires first pivoting the data from wide to long format; long format is generally what is expected from most longitudinal data modeling packages. We also add a variable specifying how long each observation period was, with the baseline period being 8 weeks and all other being 2 weeks. We provide this variable as an offset on the link scale. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/geem_count_regression.R}
\caption{Fitting Poisson and Negative-binomial GEEs to the seizures dataset using \texttt{geeM}.}
\label{listing:geem-example}
\end{listing}

A downside of using the \texttt{geeM} package is that it does not have built in functions to calculate \gls{qic} or its derivatives. Fortunately, the \texttt{geepack} package has the \texttt{QIC()} function, which swiftly computes quasi-likelihood, \gls{qic}, $\text{QIC}_u$, and other metrics for a user-provided list of models. We demonstrate that functionality in Source Code \ref{listing:geepack-qic-example} using a longitudinal dataset composed of records specifying whether or not children have problems wheezing as a function of age and smoking status. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/geepack_qic.R}
\caption{Choosing between correlation structures using QIC as implemented in \texttt{geepack}.}
\label{listing:geepack-qic-example}
\end{listing}

\subsection{Generalized linear mixed models}

An alternative to the \gls{gee} paradigm, which is only capable of providing marginal, population-averaged effect estimates, is the \gls{glmm}. This type of model is capable of estimating both population- and subject-level effect estimates, providing ultimate modeling flexibility. As with \glspl{lmm}, random effects are typically assumed to follow a normal distribution with mean zero i.e., $\gamma \sim \mathcal{N}(0, \Sigma)$, though in special cases other distributions such as the Gamma may be used. Conditional on the random effects, the model structure is then as follows, where $\eta_i = g(\mu_i)$, $\epsilon_i \sim \mathcal{N}(0, \sigma^2I)$, and $Z_i$ is the matrix of random effects:

\begin{equation}\label{eq:glm-glmm-model-form}
    \eta_i = \beta X_i + \gamma Z_i + \epsilon_i
\end{equation}

The likelihood is then the following, where the \gls{pdf} of the random effects is denoted by $h$:

\begin{equation}\label{eq:glm-glmm-model-likelihood}
    \mathcal{L}\left(\beta, \phi, \Sigma; y_1 \dots, y_n\right) = \prod_{i=1}^n \int_{\mathbb{R}} f(y_i|\beta, \phi, \gamma) h(\gamma|\Sigma) d\gamma
\end{equation}

\subsubsection{Software implementations}

The integrals in Equation \ref{eq:glm-glmm-model-likelihood} are generally not solvable analytically, and thus must be approximated by numerical methods such as \gls{agq}, Laplace approximation, or \gls{mcmc} sampling. \Gls{pql} is also as option; several software packages for fitting \glspl{glmm} and their methodologies are listed below in Table \ref{table:glmm-software}. Briefly, \gls{agq} approximates the integrals by finite weighted sums that evaluate the function at a selected set of points. When using methods based on \gls{agq}, it is possible to increase the likelihood of model convergence by iteratively increasing the number of quadrature points used. This does however come at the cost of increased computational time. Conversely, the Laplace approximation and \gls{pql} maximize an analytical approximation of the likelihood function. As such, these two methods do not provide exact \glspl{mle}. In general, the \texttt{glmmTMB} package is a flexible and fast option that should be preferred in most situations. With that being said, Bayesian methods such as those implemented in the \texttt{brms} and \texttt{INLA} R packages are capable of handling much more complex designs and correlation structures; the downside of course is that Bayesian methods are generally slower, and can be more difficult to diagnose and interpret. In addition, specifying Bayesian models is often more complex and verbose than \gls{reml} or maximum likelihood approaches, and it is often necessary to first determine how to specify the prior distribution. For a broad comparison of these numerical fitting methods see \cite[Chapter~9.5.2-3]{agresti_foundations_2015} and \cite[Chapter~13.5]{faraway_extending_2016}; for a very detailed description of the integrated nested Laplace approximation method see \cite{rue_approximate_2009}. See \cite{morrison_gentle_2017} for a gentler introduction to the math behind the \texttt{INLA} package. 

\begin{table}[h!]
\centering
\begin{tabular}{||l l l||} 
 \hline
 \textbf{R Package} & \textbf{Function} & \textbf{Fitting Method} \\ [0.5ex] 
  \hline\hline
  \href{https://cran.r-project.org/web/packages/MASS/index.html}{\texttt{MASS}} & \texttt{glmmPQL()} & Penalized quasi-likelihood \\ 
  \hline
  \href{https://cran.r-project.org/web/packages/lme4/index.html}{\texttt{lme4}} & \texttt{glmer()} & Laplace approximation \\
  \hline
  \href{http://glmmadmb.r-forge.r-project.org}{\texttt{glmmADMB}} & \texttt{glmmadmb()} & Laplace approximation \\
  \hline
  \href{https://www.r-inla.org}{\texttt{INLA}} & \texttt{inla()} & Integrated nested Laplace approximation \\ 
  \hline
  \href{https://cran.r-project.org/web/packages/glmmTMB/index.html}{\texttt{glmmTMB}} & \texttt{glmmTMB()} & Adaptive Gauss-Hermite quadrature \\
  \hline
  \href{https://cran.r-project.org/web/packages/GLMMadaptive/index.html}{\texttt{GLMMadaptive}} & \texttt{mixed\_model()} & Adaptive Gauss-Hermite quadrature \\
  \hline
  \href{https://cran.r-project.org/web/packages/repeated/index.html}{\texttt{repeated}} & \texttt{glmm()} & Gauss-Hermite quadrature \\
  \hline 
  \href{https://cran.r-project.org/web/packages/brms/index.html}{\texttt{brms}} & \texttt{brm()} & MCMC sampling \\
  \hline
  \href{https://cran.r-project.org/web/packages/MCMCglmm/index.html}{\texttt{MCMCglmm}} & \texttt{MCMCglmm()} & MCMC sampling \\
  \hline
\end{tabular}
\caption{Software packages for the fitting of GLMMs.}
\label{table:glmm-software}
\end{table}

There also exist implementations outside of R, though R is generally considered to be the best option for fitting \glspl{glmm}. SAS includes the well-documented \href{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#glimmix_toc.htm}{\texttt{PROC GLMMIX}}, Python's \href{https://www.statsmodels.org/stable/mixed_glm.html}{\texttt{statsmodels.genmod.bayes\_mixed\_glm}} supports Binomial and Poisson \glspl{glmm}, and Julia has \href{https://juliastats.org/MixedModels.jl/stable/}{\texttt{MixedModels.jl}}, which is similar in functionality to \texttt{lme4}.  

\subsubsection{Model fitting}

The approximation for the covariance of $y$ is given by the following, where $A_i = \text{Diag}\left(V(\mu_i)\right)$ and $L = \text{Diag}\left(\frac{\partial g^{-1}(\mu)}{\partial\mu}\right)$:

\begin{equation}\label{eq:glm-glmm-y-covariance-approx}
    \Tilde{V}_i = \text{Cov}(y_i) \approx L_i Z_i \Sigma Z_i^\intercal L_i + \phi A_i
\end{equation}

As per \cite{zeger_models_1988}, the asymptotic distribution of the estimated coefficients is multivariate Gaussian:

\begin{equation}\label{eq:glm-glmm-beta-asymptotics}
    \sqrt{n}\left(\hat{\beta} - \beta\right) \overset{d}{\to} \mathcal{N}\left(0, V_\beta\right)
\end{equation}

The above variance-covariance matrix can be consistently estimated via:

\begin{equation}\label{eq:glm-glmm-beta-vcov-estimator}
    \hat{V}_{\hat{\beta}} = \left(\sum_{i=1}^n\left(\frac{\partial\hat{\mu}_i}{\partial\beta}\right)^\intercal \hat{\Tilde{V}}^{-1}_i \frac{\partial\hat{\mu}_i}{\partial\beta}\right)^{-1} \left(\left(\frac{\partial\hat{\mu}_i}{\partial\beta}\right)^\intercal \hat{\Tilde{V}}^{-1}_i \left(y_i - \hat{\mu}_i\right)\left(y_i - \hat{\mu}_i\right)^\intercal \frac{\partial\hat{\mu}_i}{\partial\beta}\right) \left(\sum_{i=1}^n\left(\frac{\partial\hat{\mu}_i}{\partial\beta}\right)^\intercal \hat{\Tilde{V}}^{-1}_i \frac{\partial\hat{\mu}_i}{\partial\beta}\right)^{-1}
\end{equation}

A moment estimator for $\Sigma$ can be derived from \ref{eq:glm-glmm-y-covariance-approx}:

\begin{equation}\label{eq:glm-glmm-Sigma-estimator}
    \hat{\Sigma} = \sum_{i=1}^n \left(Z_i^\intercal Z_i\right)^{-1} Z_i^\intercal \hat{L}^{-1}_i \left(\left(y_i - \hat{\mu}_i\right)\left(y_i - \hat{\mu}_i\right)^\intercal - \hat{\phi}\hat{A}_i\right) \hat{L}^{-1}_i Z_i^\intercal \left(Z_i^\intercal Z_i\right)^{-1}
\end{equation}

Similarly, a moment estimator for the dispersion parameter is given by:

\begin{equation}\label{eq:glm-glmm-phi-estimator}
    \hat{\phi} = \sum_{i=1}^n \sum_{t=1}^{n_i} \frac{\left(y_{it} - \hat{\mu}_{it}\right)^2 - \left(\hat{L}_{it}\right)^2Z_{it}^\intercal \hat{\Sigma} Z_{it}}{\hat{\eta}_{it}}
\end{equation}

In order to fit the model, we iterate between computing $\hat{\phi}$ and $\hat{\Sigma}$ given a current value for $\hat{\beta}$, and solving for $\hat{\beta}$ using the following estimating equation, where the working correlation matrix $R(\alpha)$ is defined as described previously for \glspl{gee}:

\begin{equation}\label{eq:glm-glmm-beta-est-equation}
    \mathcal{S}(\beta) = \sum_{i=1}^n \left(\frac{\partial\mu_i}{\partial\beta}\right)^\intercal \left(A_i^{\frac{1}{2}} R_i(\alpha) A_i^{\frac{1}{2}}\right)^{-1} (y_i - \mu_i) = 0
\end{equation}

\subsubsection{Inferential approaches}

Inference for \glspl{glmm} is rather complicated, and suffers from many of the same concerns as do \glspl{lmm}. For the estimated coefficients to be consistent, it is necessary for both the link function and the assumed distribution of the random effects (usually Gaussian) to be correct. This is in contrast to how consistency works for \glspl{gee}, where only the link function must be correctly specified. Precise inference as regards \glspl{glmm} can take several forms depending on what parameters are of interest e.g., one might treat the random effects as simply nuisance parameters and perform standard \gls{glm}-like inference on the fixed effects, versus inspecting the inspecting the variances of the random effects and testing whether or not their variances differ from zero. Inference also depends on how the model itself was fit. Unlike \glspl{glmm}, where \gls{reml} is the default estimation method, most GLMM software implementations only support standard maximum likelihood estimation. A notable exception is the \texttt{glmmTMB} R package, which does support \gls{reml} estimation. For a recent look at the differences between \gls{reml} and maximum likelihood for \glspl{glmm}, see \cite{maestrini_restricted_2024}. As with \glspl{lmm}, likelihood-based inference techniques such as model selection with \gls{aic} or \gls{bic}, or model comparison using \glspl{lrt} do not apply if \gls{reml} was used to fit the model. For a detailed overview of the methods and pitfalls of \gls{glmm} inference, see \cite{bolker_glmm_2023}. 

\subsubsection{Example code}

As shown in Source Code \ref{listing:glmmtmb-example}, we can leverage the \texttt{glmmTMB} package to fit Poisson and Negative-binomial models (using maximum likelihood) to a longitudinal dataset consisting of counts of epileptic patient seizures over time as a response to the treatment progabide. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/glmmtmb_count_regression.R}
\caption{Fitting Poisson and Negative-binomial GLMMs to the epilepsy dataset using \texttt{glmmTMB}.}
\label{listing:glmmtmb-example}
\end{listing}

For a Bayesian approach we can use the \texttt{INLA} package, which I generally prefer over \texttt{brms} due to its speed (\texttt{brms} is really so slow). As shown in Source Code \ref{listing:inla-example} we fit a \gls{glmm} to a dataset consisting of records of death rates of pediatric cardiac surgery patients in 12 different hospitals. Unlike most modeling packages in R, \texttt{INLA} doesn't have generic methods such as \texttt{summary()}, \texttt{predict()}, or \texttt{fitted.values()}. All of this information is stored in the model object itself and can be extracted. For example, in the final line we retrieve the model and per-observation \gls{dic} values. The \gls{dic} is a generalization of other \glspl{ic} like \gls{aic} and \gls{bic} to the domain of mixed models and can be interpreted similarly. The model's deviance residuals can be extracted using the \texttt{residuals()} function as is done for \glspl{glm}. We can also extract the posterior mean and a 95\% credible interval from the \texttt{summary.fixed} part of the model. \texttt{INLA} model objects are large and contain a lot of information; with that being said \href{https://www.r-inla.org/documentation}{the documentation} is pretty complete and is updated regularly, and most questions can be answered by combing through the docs. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/inla_binomial_regression.R}
\caption{Fitting a Bayesian logistic regression GLMM to the cardiac surgery dataset using \texttt{INLA}.}
\label{listing:inla-example}
\end{listing}

\subsection{Generalized additive models}

While linear models and their extensions are extraordinarily powerful, their assumption of linear covariate effects is sometimes violated by nonlinear data generating processes. This brings us to the \gls{gam}, which relaxes this key assumption while maintaining many of the attractive properties of \glspl{glm}. Two quote \cite[Chapter~4.1]{wood_generalized_2017}, a \gls{gam} is ``a generalized linear model with a linear predictor involving a sum of \textit{smooth} functions of covariates'' (emphasis mine). The \gls{gam} still makes use of a link function, denoted $g$, to define the linear predictor, denoted $\eta = g(\mu)$:

\begin{equation}\label{eq:glm-gam-model-form}
    \eta_i = \beta_0 + \sum_{j=1}^p f_j(X_{ij}) + \gamma A_i + \epsilon_i
\end{equation}

In the above, the design matrix $A_i$ contains all covariates to be modeled in a parametric fashion e.g., categorical covariates; $\gamma$ is the corresponding vector of coefficients. The \gls{gam} is thus a semiparametric model. In the case that $f_j(X_j) = \beta_j X_j$, the \gls{gam} clearly reduces to the standard \gls{glm}. The power of the \gls{gam} comes from its key assumption about how the effects of the nonlinear functions of the covariates behave; as \cite[Lecture~14]{molstad_sta_2022} puts it, \glspl{gam} ``effectively impose a restriction on the space of nonlinear models considered: they must be additive.'' This formulation motivates two key questions one must answer when fitting a \gls{gam}: first, what functions should be chosen, and second, how smooth how should they be?

\subsubsection{Model fitting}

To answer the first question, we must accept that the true functions $f_j$ are unknown, and thus must be somehow approximated. We do this using \textit{basis functions}, which are elements of the basis of a function space such that every function in that space can be represented as a weighted sum of basis functions. More plainly:

\begin{equation}\label{eq:glm-gam-basis-functions}
    f(X) = \sum_{m=1}^M \beta_m b_m(X)
\end{equation}

Once we have a set of basis functions, which can also be viewed as transformations of $X$, techniques such as linear or ridge regression can be used to estimate their coefficients. Basis functions can take many forms: constant, linear, polynomial, etc. A linear combination of basis functions is referred to as a \textit{spline}, and splines form the backbone of \glspl{gam}. A key component of a spline is what's called a \textit{knot} i.e., a point at which one of the basis functions changes. Often basis functions take the following form (sometimes referred to as a hinge function), where $\kappa$ denotes the location of the knot:

\begin{equation}\label{eq:glm-gam-hinge-function}
    b(X) = \text{max}(X - \kappa, 0) = (X - \kappa)_+
\end{equation}

A coefficient is then estimated for the basis function. One can also take the power of a basis function e.g., $b(X) = (X - \kappa)_+^2$. For a great visual explanation of how basis functions induce nonlinearity and enforce continuity to approximate complex functions see \cite[Lecture~6]{molstad_sta_2022}. To generalize, we can define an order-$M$ spline with evenly-spaced knots $\kappa_1 < \kappa_2 < \dots < \kappa_k$ as a piecewise polynomial of degree $M-1$ having continuous derivatives of degree $M-2$:

\begin{equation}\label{eq:glm-gam-order-m-spline}
    \begin{aligned}
        s(x) &= \begin{cases}
            s_1(x) & -\infty < x < \kappa_1 \\
            s_2(x) & \kappa_1 \le x < \kappa_2 \\
            \vdots & \\
            s_{k+1}(x) & \kappa_k \le x < \infty \\
        \end{cases} \\
        \frac{d^{(m)}}{dx^{(m)}} s_{i-1}(\kappa_{i-1}) &= \frac{d^{(m)}}{dx^{(m)}} s_i(\kappa_{i-1}) \quad i = 2, \dots, k+1; \: m = 0, 1, \dots, M-2 \\
    \end{aligned}
\end{equation}

In the above, each $s_i$ is a polynomial of degree $M-1$. For example, a cubic spline is a spline with order $M=4$. However, one should clearly see a potential issue with such ``wiggly'' functions as the cubic spline: behavior beyond the boundaries of the data. Specifically, as detailed by \cite[Chapter~2.9]{hastie_generalized_1990} the fitted values generated by typical regression smoothers have higher variance as they approach the boundary. This concern motivates the definition of the \textit{natural cubic spline}, which imposes the requirement of linearity beyond the two boundary knots and thus is less ``wiggly'' near the boundaries. More formally, as per \cite[Lecture~7]{molstad_sta_2022}, we have the following constraint:

\begin{equation}\label{eq:glm-gam-linearity-constraint}
    s_1^{\prime\prime}(x) = s_{k+1}^{\prime\prime}(y) = 0 \quad \forall \: x \le \kappa_1, y \ge \kappa_k
\end{equation}

Thus, the definition of a natural cubic spline with $k$ knots represented by $k$ basis functions is:

\begin{equation}\label{eq:glm-gam-natural-cubic-spline-1}
    b_1(X) = 1, \quad b_2(X) = X, \quad \dots, \quad b_{l+2}(X) = d_l(X) - d_{k-1}(X)
\end{equation}

where:

\begin{equation}\label{eq:glm-gam-natural-cubic-spline-2}
    d_l(X) = \frac{(X - \kappa_l)_+^3 - (X - \kappa_k)_+^3}{\kappa_k - \kappa_l}
\end{equation}

The natural cubic spline has some attractive theoretical properties e.g., being a smoothest interpolator - for details on these properties and proofs thereof see \cite[Chapter~5.1.1]{wood_generalized_2017} or \cite[Lecture~7]{molstad_sta_2022}. For our purposes, suffice it to say that natural cubic splines are very good approximators of the unknown true function $f$. They can also be motivated from an optimization perspective - for example, suppose we want to identify a smoothing spline estimator of $f$, which we'll denote $\hat{f}$ and define as the solution to the following Lagrangian optimization problem:

\begin{equation}\label{eq:glm-gam-smoothing-spline-optimizer}
    \hat{f} = \underset{f \in \mathcal{F}}{\text{arg min}} \left\{\sum_{i=1}^n \left(y_i - f(X_i)\right)^2 - \lambda \int_{\mathbb{R}} \left(\frac{d^2}{dt^2} f(t)\right)^2 dt \right\}
\end{equation}

Above, $\mathcal{F}$ defines the infinite-dimensional Sobolev space of functions with two continuous derivatives. In addition, we've introduced a penalty parameter $\lambda$ that controls the complexity of $\hat{f}$, much like the penalty parameter used in the ridge regression and \gls{lasso} optimization problems. Clearly, larger values of $\lambda$ lead to approximators of $f$ with higher degrees of curvature. The special cases of $\lambda = 0$ and $\lambda = \infty$ lead to an $\hat{f}$ that perfectly interpolates the data and an OLS estimator, respectively. At first glance this seems like an awfully hard problem to solve, however as per \cite[Lecture~7]{molstad_sta_2022}, $\hat{f}$ is actually a natural cubic spline with $k$ knots placed at each unique value of $X$. Generally, the optimal value for $\lambda$ is selected via \gls{gcv}. With this knowledge in hand, it clearly makes sense to utilize natural cubic splines as basis functions in a \gls{gam}, though we note that many other types of splines exist, each with their own use cases - see \cite[Chapter~5]{wood_generalized_2017} for descriptions of several different classes of splines. All of this leads us to the following \gls{gam} optimization problem, which is minimized by having each $f_j$ as a natural cubic spline:

\begin{equation}\label{eq:glm-gam-optimization-problem}
    \underset{\beta_0, \{f_j\}_{j=1}^p}{\text{arg min}} \left\{\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p f_j(X_{ij})\right)^2 + \sum_{j=1}^p \lambda_j \int_{\mathbb{R}} \left(\frac{d^2}{dt_j^2} f_j(t_j)\right)^2 dt_j\right\}
\end{equation}

In order to make this problem identifiable, we impose the following constraint:

\begin{equation}\label{eq:glm-gam-optimization-problem-constraint}
    \sum_{i=1}^n f_j(X_{ij}) = 0 \quad \forall \: j = 1, \dots, p
\end{equation}

The last step in fitting a \gls{gam} is the estimation of the coefficients for each smooth function. This must be done algorithmically, through a process known as \textit{backfitting}. Broadly, we iteratively update each $\hat{f}_j$ one at a time while holding the others constant. Suppose for simplicity that our assumed error distribution is Gaussian, that our smoothers are natural cubic splines, and that we have no terms to be parametrically estimated i.e., that our model has the following form:

\begin{equation}\label{eq:glm-gam-model-form-gaussian}
    \mu_i = \beta_0 + \sum_{j=1}^p f_j(X_{ij}) + \epsilon_i
\end{equation}

The backfitting algorithm is thus as follows, for an \textit{a priori} specified overall tolerance $\Delta$ in the change of any of the $f_j$:

\begin{algorithm}[h!]
    \caption{The backfitting algorithm for a Gaussian response.}\label{alg:backfitting-gaussian}
    \begin{algorithmic}[1]
        \STATE Set $\hat{\beta}_0 = n^{-1} \sum_{i=1}^n y_i$
        \STATE Initialize all $\hat{f}_j = 0$
        \WHILE{any $\Delta_j > \Delta$}
            \FOR{$j = 1, \dots, p$}
                \STATE Define $r_{ij} = y_i - \hat{\beta}_0 - \sum_{k \neq j} \hat{f}_k(X_{ik})$
                \STATE Update $\hat{f}_j = \underset{f_j}{\text{arg min}} \left\{\sum_{i=1}^n \left(r_{ij} - f_j(X_{ij})\right)^2 + \lambda_j \int_{\mathbb{R}} \left(\frac{d^2}{dt_j^2} f_j(t_j)\right)^2 dt_j\right\}$
                \STATE Define $\hat{f}_j^{\text{new}} = \hat{f}_j - n^{-1} \sum_{i=1}^n \hat{f}_j(X_{ij})$
                \STATE Define $\Delta_j = \left|\hat{f}_j^{\text{new}} - \hat{f}_j\right|$
            \ENDFOR
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

This algorithm can be somewhat easily extended to any arbitrary smoothing operator $\mathcal{S}_j$ such as kernel smoothers, local polynomials, etc. In addition, Algorithm \ref{alg:backfitting-gaussian} can be extended to non-Gaussian distributions by embedding the backfitting procedure within a Newton-Raphson loop; see \cite[Lecture~14]{molstad_sta_2022} for details on both modified algorithms. In addition, \cite[Chapter~9.1.2]{hastie_elements_2009} provides an example algorithm specific to logistic regression.

Finally, we must consider how to choose appropriate values of the penalty terms. One approach is to construct a smoothing matrix for each function, which we denote $S_j(\lambda_j)$, and constrain its estimation such that $\text{trace}(S_j(\lambda_j)) - 1$ is equal to a constant. This constant is called the \gls{edf}. Unfortunately the model itself also has an \gls{edf} value, which is of course calculated differently. If we define $S_\lambda = \sum_{j=1}^p \lambda_jS_j(\lambda_j)$, the model \gls{edf} is given by:

\begin{equation}\label{eq:glm-gam-model-edf}
    \tau = \text{trace}\left(\left(X^\intercal W X + S_\lambda\right)^{-1} X^\intercal W X\right)
\end{equation}

where $W$ is the diagonal matrix of weights as mentioned in Equation \ref{eq:glm-beta-variance-est}. In general, increasing the model EDF leads to higher variance, lower bias fits and vice versa. Simply put, the higher the model \gls{edf}, the ``wigglier'' the final fit. For a more detailed explanation of smoother and model \gls{edf} values refer to \cite[Chapter~6.1.2]{wood_generalized_2017}. 

\subsubsection{Model selection and diagnostics}

Due to the complexity of the \gls{gam} framework, model selection and diagnostics are tricky and subjective, more akin to an art than a science. Much of the process of selecting and evaluating a candidate model is visual, though there exist some quantitative measures of fit that we'll discuss here. For technical reasons the traditional AIC as defined in Equation \ref{eq:glm-fit-measures} cannot be used for GAMs - in short this is because uncertainty in the smoothing parameters is ignored when calculating the model \gls{edf} (read \cite[Chapter~6.11]{wood_generalized_2017} for a full explanation). However, we can define a corrected \gls{aic} that accounts for that uncertainty. First we define $\rho_j = \log(\lambda_j)$ and $S_\lambda = \sum_{j=1}^p \lambda_jS_j(\lambda_j)$ (as before), then we have the following Bayesian large sample approximation:

\begin{equation}\label{eq:glm-gam-beta-asymptotics-bayesian}
    \beta | y, \rho \sim \mathcal{N}\left(\hat{\beta}, \left(\hat{\mathcal{I}} + S_\lambda\right)^{-1}\right)
\end{equation}

where $\hat{\mathcal{I}}$ is the negative Hessian of the log-likelihood. We also have:

\begin{equation}\label{eq:glm-gam-rho-asymptotics}
    \rho | y \sim \mathcal{N}\left(\hat{\rho}, V_\rho\right)
\end{equation}

where $V_\rho$ is the inverse of the Hessian of the negative marginal log-likelihood with respect to $\rho$. We also define $R_\rho^\intercal R_\rho = V_\beta = \left(\hat{\mathcal{I}} + S_\lambda\right)^{-1}$. We then have (apologies for the notation):

\begin{equation}\label{eq:glm-gam-vcov-matrices}
    \begin{aligned}
        J &= \frac{\partial\hat{\beta}}{\partial\rho} \bigg |_{\hat{\rho}} \\
        V^\prime_\beta &= V_\beta + V^\prime + V^{\prime\prime} \\
        V^\prime &= JV_\rho J^\intercal \\
        V^{\prime\prime}_{jm} &= \sum_{i=1}^p\sum_{l=1}^M\sum_{k=1}^M \frac{\partial R_{ij}}{\partial\rho_k} V_{\rho, kl} \frac{\partial R_{im}}{\partial\rho_l} \\
    \end{aligned}
\end{equation}

This allows us to define the corrected \gls{edf}:

\begin{equation}\label{eq:glm-gam-corrected-edf}
    \tau_c = \text{trace}\left(V_\beta^\prime \hat{\mathcal{I}}\right)
\end{equation}

which we then use to define the corrected \gls{aic}:

\begin{equation}\label{eq:glm-gam-corrected-aic}
    \text{AIC}_c = -2\mathcal{l}\left(\hat{\beta}\right) + 2\tau_c
\end{equation}

In addition to the corrected \gls{aic}, the \gls{gcv} score for the model can be used to compare models. Like with \glspl{ic}, lower values are preferable. Beyond the usage of such metrics for model selection, it is often of use to visually inspect the components of a fitted model. One way to do so is to plot each of the covariates on the x-axis versus their smoothers on the y-axis. A rough rule of thumb given by \cite[Chapter~15.2]{faraway_extending_2016} is that if a straight line could be drawn within the confidence band for the smoother, then a linear term is probably sufficient. In addition, one can examine the \gls{edf} for each smoother; values close to one indicate an approximately linear relationship. 

For models with an assumed Gaussian error distribution it is also important to check the assumptions of constant variance and normality of residuals. These can be assessed visually by producing a scatterplot of the residuals versus the fitted values and a QQ-plot of the residuals, respectively. Some of these diagnostic methods have been wrapped into a the \texttt{mgcv::gam.check()} function, which produces some basic residual plots, provides information about model convergence, and performs a basic test of whether the specified basis dimension is sufficient. For an example interpretation of this functions output see \cite[Chapter~7.2.1]{wood_generalized_2017}. 

\subsubsection{Software implementations}

There are several available R packages for the fitting of \glspl{gam}. The \texttt{mgcv} package is definitely the most widely-used, with over 79k monthly downloads at the time of this writing. For a detailed overview of the package refer to \cite[Chapter~7]{wood_generalized_2017}; for an interactive modeling guide see \cite{ross_generalized_2019}. \texttt{mgcv} is well-documented and there exists a lot of content online related to its use, making it rather easy to troubleshoot when things go wrong. With that being said, I personally prefer the \texttt{gamlss} package; it supports an incredibly wide variety of distributions including truncated, zero-inflated, and non-exponential family distributions. For a list of every supported distribution see the \href{https://cran.r-project.org/web/packages/gamlss.dist/gamlss.dist.pdf}{\texttt{gamlss.dist}} package. In addition, I have personally found it to be faster than \texttt{mgcv}. For fitting models with random effects the \texttt{gamm4} package should be preferred over the equivalent functionality implemented in \texttt{mgcv}, as the former makes use of the more reliable \texttt{lme4} package under the hood instead of \texttt{nlme} as is used by the latter. Lastly, for \gls{lasso}-like variable selection we have the \texttt{gamlr} package, which also implements cross-validation for selection of the penalty parameter. Note: \texttt{gamlr} currently only supports the Gaussian, Binomial, and Poisson distributions. 

\begin{table}[h!]
\centering
\begin{tabular}{||l l l||} 
 \hline
 \textbf{R Package} & \textbf{Function} & \textbf{Notes} \\ [0.5ex] 
  \hline\hline
  \href{https://cran.r-project.org/web/packages/gam/index.html}{\texttt{gam}} & \texttt{gam()} & Only supports local regression and smoothing splines \\ 
  \hline
  \href{https://cran.r-project.org/web/packages/gamlss/index.html}{\texttt{gamlss}} & \texttt{gamlss()} & Supports the widest variety of distributions \\
  \hline
  \href{https://cran.r-project.org/web/packages/gamlss/index.html}{\texttt{gss}} & \texttt{gssanova()} & Offers spline-based survival modeling \\
  \hline 
  \href{https://cran.r-project.org/web/packages/mgcv/index.html}{\texttt{mgcv}} & \texttt{gam()} & Automates choice of the degree of smoothing \\
  \hline 
  \href{https://cran.r-project.org/web/packages/gamm4/index.html}{\texttt{gamm4}} & \texttt{gamm4()} & Estimates generalized additive mixed models using \texttt{lme4} \\
  \hline 
  \href{https://cran.r-project.org/web/packages/gamlr/index.html}{\texttt{gamlr}} & \texttt{gamlr()} & Provides $L_0$ to $L_1$ norm penalties for GAMs (similar to LASSO) \\
  \hline
\end{tabular}
\caption{Software packages for the fitting of GAMs.}
\label{table:gam-software}
\end{table}

Beyond R, SAS implements \href{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_gam_sect004.htm}{\texttt{PROC GAM}}, Python has \href{https://www.statsmodels.org/stable/gam.html}{statsmodels.gam.generalized\_additive\_model} and \href{https://pygam.readthedocs.io/en/latest/}{\texttt{pyGAM}}, and Julia has the (rather incomplete) \href{https://github.com/hendersontrent/GAM.jl?tab=readme-ov-file}{\texttt{GAM.jl}}.

\subsubsection{Example code}

Shown in Source Code \ref{listing:mgcv-example} is a short example of how to fit two families of model using the \texttt{mgcv} library. The dataset used is a study on the relationship between various meteorological variables and atmospheric ozone concentration, with the data being gathered in Los Angeles in 1976. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/mgcv_count_regression.R}
\caption{Fitting Poisson and Negative-binomial GAMs to the ozone dataset using \texttt{mgcv}.}
\label{listing:mgcv-example}
\end{listing}

In Source Code \ref{listing:gamlss-example} we utilize the \texttt{gamlss} package to fit a \gls{gam} with random intercepts to a dataset describing how children with leukemia's height changes as a function of age and treatment. We opt to use the Gamma distribution as height is continuous and strictly positive. After fitting one model with a smoother on age and another with age as a strictly parametric effect, we utilize the generalized \gls{aic} to compare the two models. In doing so we see that the model with a smoother on age is preferred as its value is much lower than that of the model with no smoother. We also use the \texttt{Rsq()} function to estimate the Cox-Snell pseudo-$R^2$ value for each model; the model with the smoother is slightly better by this metric, confirming our suspicion that it should be preferred over the model without a smoother. 

\begin{listing}[h!]
\inputminted{r}{Example-Code/gamlss_random_effects.R}
\caption{Fitting random intercept GAMs to the leukemia dataset using \texttt{gamlss}.}
\label{listing:gamlss-example}
\end{listing}

\section{Large sample theory}\label{sec:tools-lst}

\subsection{Limit inferior and limit superior}

Suppose we observe a sequence of sets $A_1, \dots, A_n$. The limit inferior and limit superior are then defined as follows:

\begin{equation}\label{eq:lst-lim-inf-and-sup}
    \begin{aligned}
        \liminf_{n\to\infty} A_n &= \bigcup_{n=1}^\infty \bigcap_{k=n}^\infty A_k \\
        \limsup_{n \to\infty} A_n &= \bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_k \\
    \end{aligned}
\end{equation}

\subsection{The Borel-Cantelli lemma}

Suppose $A_1, \dots, A_n$ are events on some probability space. Then we have:

\begin{equation}\label{eq:lst-borel-cantelli}
    \sum_{i=1}^\infty \mathbb{P}(A_n) < \infty \implies \mathbb{P}\left(\limsup_{n\to\infty} A_n\right) = 0
\end{equation}

\subsection{Uniform integrability}

A class of \glspl{rv}, denoted $\mathcal{C}$, is called \gls{ui} if the following holds, where $I$ is the indicator function:

\begin{equation}\label{eq:lst-uniformly-integrable-1}
    \forall \epsilon > 0 \: \exists \: K \in [0, \infty) \colon \mathbb{E}\left[|X| I_{|X|\geq K}\right] \leq \epsilon \: \forall X \in \mathcal{C}
\end{equation}

An alternative definition is adapted from \cite{hu_note_2017}: a sequence of \glspl{rv} $X_i, i = 1, \dots, n$ is said to be \gls{ui} if the following holds:

\begin{equation}\label{eq:lst-uniformly-integrable-2}
    \lim_{K\to\infty}\left(\sup_{n\geq 1}\mathbb{E}\left[|X_n|I_{|X_n| \geq K}\right]\right) = 0
\end{equation}

A consequence of a sequence of \glspl{rv} being \gls{ui} is that they are \textit{uniformly tight}. Uniform tightness is defined as follows:

\begin{equation}\label{eq:lst-tightness}
    \forall \: \delta > 0 \: \exists \: K > 0 \colon \mathbb{P}(|X| > K) \leq \delta \: \forall \: X \in \mathcal{C}
\end{equation}

This leads us to Prohorov's theorem; for a proof see \cite[Chapter~2.1]{van_der_vaart_asymptotic_1998}. The theorem states that the following are true for any random vectors $X_n \in \mathbb{R}^k$:

\begin{enumerate}
    \item $X_n \overset{d}{\to} X \implies \{X_n \colon n \in \mathbb{N}\}$ is uniformly tight
    \item If $X_n$ is uniformly tight, then there exists a subsequence with $X_{n_j} \overset{d}{\to} X$ as $j \to \infty$
\end{enumerate}

\subsection{Boole's inequality}

Suppose we observe a set of countable events $A_1, \dots, A_n$. Boole's inequality then states that the following is true:

\begin{equation}\label{eq:lst-booles-inequality}
    \mathbb{P}\left(\bigcup_{i=1}^n A_i\right) \leq \sum_{i=1}^n \mathbb{P}(A_i)
\end{equation}

\subsection{Modes of convergence and the relationships between them}

Almost sure convergence is defined as:

\begin{equation}\label{eq:lst-convergence-as}
    \mathbb{P}(\omega: X_n(\omega) \to X(\omega)) = 1
\end{equation}

Convergence in the $r^\text{th}$ mean (sometimes denoted $L^r$ convergence) is defined as:

\begin{equation}\label{eq:lst-convergence-rth-mean}
    \lim_{n \to \infty} \mathbb{E}\left[(X_n - X)^r\right] = 0
\end{equation}

Convergence in probability is defined as:

\begin{equation}\label{eq:lst-convergence-probability}
    \lim_{n\to \infty} \mathbb{P}(|X_n - X| > \epsilon) = 0,\quad \forall \epsilon > 0
\end{equation}

Convergence in distribution is defined as:

\begin{equation}\label{eq:lst-convergence-distribution}
    \lim_{n\to \infty} \mathbb{P}(X_n \le X) = \mathbb{P}(X \le x)
\end{equation}

The relationships between the modes of convergence are as follows:

\begin{equation}\label{eq:lst-convergence-relationships}
    \begin{aligned}
      X_n \overset{a.s.}{\to} X &\implies X_n \overset{p}{\to} X \\
      X_n \overset{L^r}{\to} X &\implies X_n \overset{p}{\to} X \\
      X_n \overset{L^r}{\to} X &\implies X_n \overset{L^p}{\to} X, \quad \forall r > p \\
      X_n \overset{p}{\to} X &\implies X_n \overset{d}{\to} X \\
    \end{aligned}
\end{equation}

\subsection{The weak law of large numbers}

Assume we observe a sequence of \gls{iid} \glspl{rv} $X_1, \dots, X_n$ with $\mathbb{E}[X_i] = \mu \in \mathbb{R}$ and $\text{Var}(X_i) = \sigma^2 < \infty$. If we define the sample mean as $\bar{X}_n = n^{-1}\sum_{i=1}^n X_i$, then $\bar{X}_n \overset{p}{\to} \mu$ i.e.:

\begin{equation}\label{eq:lst-weak-lln}
    \lim_{n\to\infty} \mathbb{P}\left(\left|\bar{X}_n - \mu\right| < \epsilon\right) = 1 \quad \forall \epsilon > 0
\end{equation}

\subsection{The strong law of large numbers}

Under the same set of assumptions necessary for Equation \ref{eq:lst-weak-lln}, $\bar{X}_n \overset{a.s.}{\to} \mu$ i.e.:

\begin{equation}\label{eq:lst-strong-lln}
    \mathbb{P}\left(\lim_{n\to\infty} \left|\bar{X}_n - \mu\right| < \epsilon\right) = 1 \quad \forall \epsilon > 0
\end{equation}

\subsection{The continuous mapping theorem}

The continuous mapping theorem states that continuous functions preserve the limits of sequences of \glspl{rv}. 

\begin{equation}\label{eq:lst-cmt}
    \begin{aligned}
      X_n \overset{d}{\to} X &\implies g(X_n) \overset{d}{\to} g(X) \\
      X_n \overset{p}{\to} X &\implies g(X_n) \overset{p}{\to} g(X) \\
      X_n \overset{a.s.}{\to} X &\implies g(X_n) \overset{a.s.}{\to} g(X) \\
    \end{aligned}
\end{equation}

\subsection{The law of the iterated logarithm}

The law of the iterated logarithm is described in several ways depending on the reference text used. In \cite[Chapter~2.7]{van_der_vaart_asymptotic_1998} it's defined as follows. Suppose we have a sequence of \gls{iid} \glspl{rv} $Y_1, \dots, Y_n$ with $\mu = 0$ and $\sigma^2 = 1$, and define $S_n = \sum_{i=1}^n Y_i$. The following then holds almost surely:

\begin{equation}\label{eq:lst-iterated-logarithm-1}
    \limsup_{n\to\infty} \frac{S_n}{\sqrt{n\log(\log(n))}} = \sqrt{2}
\end{equation}

A more general formalization is found in \cite[Chapter~1.3]{dasgupta_asymptotic_2008}. Suppose again a sequence of \gls{iid} \glspl{rv}, this time with mean $\mu$ and variance $\sigma^2$. Define $S_n = \sum_{i=1}^n Y_i$. The following then hold almost surely:

\begin{equation}\label{eq:lst-iterated-logarithm-2}
    \begin{aligned}
        \limsup_{n\to\infty} \frac{S_n - n\mu}{\sqrt{2n\log(\log(n))}} &= \sigma \\
        \liminf_{n\to\infty} \frac{S_n - n\mu}{\sqrt{2n\log(\log(n))}} &= -\sigma \\
    \end{aligned}
\end{equation}

If $\text{Var}(Y_1) = \infty$, the following is true almost surely:

\begin{equation}\label{eq:lst-iterated-logarithm-3}
    \limsup_{n\to\infty} \frac{S_n - n\mu}{\sqrt{2n\log(\log(n))}} = \infty
\end{equation}

Lastly, suppose we have finite constants $\gamma$ and $\tau$ satisfying $\limsup_{n\to\infty} \frac{S_n - n\gamma}{\sqrt{2n\log(\log(n))}} = \tau$ almost surely. Then the following holds almost surely:

\begin{equation}\label{eq:lst-iterated-logarithm-4}
    \begin{aligned}
        \mathbb{E}[Y_1] &= \gamma \\
        \text{Var}(Y_1) &= \tau \\
    \end{aligned}
\end{equation}

\subsection{Slutsky's theorem}

\begin{equation}\label{eq:lst-slutsky}
    \begin{aligned}
      X_n \overset{d}{\to} X \cap Y_n \overset{p}{\to} c &\implies X_nY_n \overset{d}{\to} cX \\
      X_n \overset{d}{\to} X \cap Y_n \overset{p}{\to} c \neq 0 &\implies \frac{X_n}{Y_n} \overset{d}{\to} \frac{X_n}{c} \\
      X_n \overset{d}{\to} X \cap Y_n \overset{p}{\to} c &\implies X_n + Y_n \overset{d}{\to} X + c \\
    \end{aligned}
\end{equation}

\subsection{The portmanteau lemma}

The following is adapted from \cite[Chapter~2.1]{van_der_vaart_asymptotic_1998}. For any random vectors $X_n$ and $X$ each of the following statements are equivalent:

\begin{equation}\label{eq:lst-portmanteau-lemma}
    \begin{aligned}
        \mathbb{P}(X_n \leq x) &\to \mathbb{P}(X \leq x) \text{ for all continuity points of } x \mapsto P(X \leq x) \\
        \mathbb{E}[f(X_n)] &\to \mathbb{E}[f(X)] \text{ for all bounded, continuous functions } f \\
        \mathbb{E}[f(X_n)] &\to \mathbb{E}[f(X)] \text{ for all bounded, Lipschitz functions } f \\
        \liminf \mathbb{E}[f(X_n)] &\geq \mathbb{E}[f(X)] \text{ for all nonnegative, continuous functions } f \\
        \liminf \mathbb{P}(X_n \in G) &\geq \mathbb{P}(X \in G) \text{ for every open set } G \\
        \limsup \mathbb{P}(X_n \in F) &\leq \mathbb{P}(X \in F) \text{ for very closed set } F \\
        \mathbb{P}(X_n \in B) &\to \mathbb{P}(X \in B) \text{ for all Borel sets } B \text{ with } \mathbb{P}(X \in \delta B) = 0 \\
    \end{aligned}
\end{equation}

In the final statement, $\delta B$ denotes the boundary of $B$. In addition, a Lipschitz function is one such that for every pair of points on the graph of the function there exists a real number that bounds the absolute value of the slope between the pair. 

\subsection{Markov's inequality}

\begin{equation}\label{eq:lst-markovs-inequality}
    \mathbb{P}(|X_n| > a) \le \frac{\mathbb{E}\left[|X_n|^p\right]}{a^p}
\end{equation}

\subsection{Triangle inequalities}

\begin{equation}\label{eq:lst-triangle-inequalities}
    \begin{aligned}
      | x + y| &\le |x| + |y| \\
      ||x| - |y|| &\le |x - y|
    \end{aligned}
\end{equation}

\subsection{Taylor expansion}

The Taylor expansion of a given function $f(\cdot)$ around $x = a$ is given by:

\begin{equation}\label{eq:lst-taylor-expansion}
    f(x) = f(a) + f^\prime(a)(x - a) + f^{\prime\prime}(a) \frac{(x - a)^2}{2!} + \dots + f^{(n)}(a) \frac{(x - a)^n}{n!}
\end{equation}

\subsection{The delta method}

The univariate case:

\begin{equation}\label{eq:lst-delta-method-univariate}
    \sqrt{n}\left(g\left(\hat{\theta}_n\right) - g(\theta)\right) \overset{d}{\to} \mathcal{N}\left(0, (g^\prime(\theta))^2\sigma^2\right)
\end{equation}

The multivariate case, where $\mathfrak{J}$ is the Jacobian with respect to $\theta$:

\begin{equation}\label{eq:lst-delta-method-multivariate}
    \sqrt{n}\left(g\left(\hat{\theta}_n\right) - g(\theta)\right) \overset{d}{\to} \mathcal{N}\left(0, \mathfrak{J}_g(\theta)\symbf{\Sigma}\mathfrak{J}_g(\theta)^\intercal\right)
\end{equation}

\subsection{Influence functions}

\Glspl{if} are a tricky concept but, abstractly, the \gls{if} provides us with an estimate of the effect that each particular observation has upon an estimator. In order to define the \gls{if}, we first need to define the so-called contaminated distribution function:

\begin{equation}\label{eq:lst-if-contaminated-dist-function}
    F_\epsilon(x) = (1 - \epsilon)F + \epsilon\delta_x
\end{equation}

Above, $\delta_x$ is the probability measure that assigns probability 1 to $\{x\}$ and probability 0 to all other elements of the probability space upon which $F$ is a distribution. We then define the \gls{if} of $\hat{\theta}_n$ at $F$ as:

\begin{equation}\label{eq:lst-if}
    \text{IF}_{\hat{\theta}_n, F}(x) = \lim_{\epsilon \to 0} \frac{\hat{\theta}_n(F_\epsilon(x)) - \hat{\theta}_n(F)}{\epsilon}
\end{equation}

More technically, the \gls{if} is the Gateaux derivative of $\hat{\theta}_n$ at $F$ in the direction of $\delta_x$. As per \cite[Chapter~2.1]{hampel_robust_2011}, the \gls{if} ``describes the effect of an infinitesimal contamination at the point $x$ on the estimate, standardized by the size of the contamination.'' This is all very abstract, but the usefulness of an \gls{if} comes in computing the asymptotic variance of a statistic, as shown below:

\begin{equation}\label{eq:lst-if-variance}
    \text{Var}\left(\hat{\theta}_n, f\right) = \int_{\mathbb{R}} \left(\text{IF}_{\hat{\theta}_n, F}(x)\right)^2 dF(x)
\end{equation}

Concretely, \glspl{if} are most useful (for our purposes) when it comes to computing the asymptotic variance of a U- or V-statistic. 

\subsection{U- and V-statistics}

A U-statistic of order $r$ with kernel $h$ is defined as follows, where $\beta$ indicates all unordered subsets of $r$ different integers:

\begin{equation}\label{eq:lst-u-statistic}
    U_n = \frac{1}{\binom{n}{r}} \sum_\beta h(X_{\beta_1}, \dots, X_{\beta_r})
\end{equation}

Similar to U-statistics, a V-statistic of degree $r$ with kernel $h$ is defined as:

\begin{equation}\label{eq:lst-v-statistic}
    V_n = \frac{1}{n^r} \sum_{i_1=1}^n \dots \sum_{i_r=1}^n h(X_{i_1}, \dots X_{i_r})  
\end{equation}

\subsection{M-estimators}

An M-estimator maximizes a criterion function denoted $\Psi_n(\theta)$ over the parameter space $\Theta$. Usually this is done by deriving, setting equal to zero, and solving i.e.:

\begin{equation}\label{eq:lst-m-estimator}
    \Psi_n(\theta) = n^{-1} \sum_{i=1}^n \psi_\theta (X_i) = 0
\end{equation}

Maximum likelihood estimators are M-estimators with $\psi_\theta = \log(p_\theta)$. So-called location estimators follow the below general form:

\begin{equation}\label{eq:lst-location-m-estimator}
    \sum_{i=1}^n \psi(X_i - \theta) = 0
\end{equation}

Obviously, the appropriate choice of $\psi$ will depend on the problem at hand and will lead to differing results given different situations. For example, the Huber M-estimator for location is the solution to $\sum_{i=1}^n \psi_0(x_i - t) = 0$, where:

\begin{equation}\label{eq:huber-m-estimator}
    \psi_0(z) = \begin{cases}
        z &\text{ if } |z| \le k  \\
        k &\text{ if } z > k \\
        -k &\text{ if } z < -k \\
    \end{cases}
\end{equation}

It's asymptotic distribution is as follows, given the assumption that $f(x) > 0 \forall x$:

\begin{equation}\label{eq:huber-estimator-asymptotics}
    \sqrt{n}\left(\hat{\theta}_n - \theta\right) \overset{d}{\to} \mathcal{N}\left(0, \frac{\int_{-k}^k x^2 f(x)dx + k^2 F(-k) + k^2 (1 - F(k))}{(F(k) - F(-k))^2}\right)
\end{equation}

\subsection{The jackknife}

Jackknife resampling performs, in its simplest form, bias and variance estimation by iteratively recomputing a statistic while deleting one observation at a time (other variants such as the delete-$d$ jackknife exist as well). For example, the jackknife estimate of the sample mean is estimated like so:

\begin{equation}\label{eq:lst-jackknife-mean}
    \begin{aligned}
      \hat{\theta}_{(i)} &= (n-1)^{-1} \sum_{j \neq i}^n x_j \\
      \hat{\theta}_{\text{jack}} &= n^{-1} \sum_{i=1}^n \hat{\theta}_{(i)} \\
    \end{aligned}
\end{equation}

The jackknife estimate of bias is given by:

\begin{equation}\label{eq:lst-jackknife-bias}
    \widehat{\text{bias}}\left(\hat{\theta}_{\text{jack}}\right) = (n-1)\left(\hat{\theta}_{\text{jack}} - \hat{\theta}\right)
\end{equation}

This gives rise to the bias-corrected jackknife estimate:

\begin{equation}\label{eq:lst-jackknife-est-bias-corrected}
    \begin{aligned}
      \hat{\theta}_{\text{jack}}^* 
        &= \hat{\theta} - \widehat{\text{bias}}\left(\hat{\theta}_{\text{jack}}\right) \\
        &\equiv n\hat{\theta} - (n-1)\hat{\theta}_{\text{jack}} \\
    \end{aligned}
\end{equation}

The $i^\text{th}$ jackknife pseudovalue is defined as:

\begin{equation}\label{eq:lst-jackknife-pseudovalue}
    \Tilde{\theta}_{(i)} = n\hat{\theta} - (n-1)\hat{\theta}_{(i)}
\end{equation}

This leads to the jackknife estimated variance, which is given by:

\begin{equation}\label{eq:lst-jackknife-variance-estimator}
    \widehat{\text{Var}}\left(\hat{\theta}_\text{jack}\right) = (n(n-1))^{-1} \sum_{i=1}^n \left(\Tilde{\theta}_{(i)} - \bar{\Tilde{\theta}_{(i)}}\right)^2
\end{equation}

The jackknife standard error is then the square root of the jackknife estimated variance. Note that the jackknife is known to not perform well when estimating confidence intervals for sample quantiles, as quantiles are not well-approximated by averages of \glspl{rv}. 

\chapter{Worked Examples} \label{chap:worked-examples}

\section{Linear models}\label{sec:examples-ols}

\subsection{Maximum likelihood estimation}

Suppose that we have a regression model of the following form, with an assumed Gaussian error distribution:

\begin{equation}\label{eq:ex-ols-model-form}
    y_i = \beta X_i + \epsilon_i
\end{equation}

The \gls{pdf} is:

\begin{equation}\label{eq:ex-ols-model-pdf}
    f(y_i | \beta x_i) = (2\pi\sigma^2)^{-\frac{1}{2}} e^{-\frac{(y_i - \beta x_i)^2}{2\sigma^2}}
\end{equation}

The likelihood function is then:

\begin{equation}\label{eq:ex-ols-model-likelihood}
    \begin{aligned}
        \mathcal{L}(\beta, \sigma^2;y_1, \dots, y_n, x_1, \dots, x_n)
          &= \prod_{i=1}^n f(y_i | \beta x_i) \\
          &= \prod_{i=1}^n (2\pi\sigma^2)^{-\frac{1}{2}} e^{-\frac{(y_i - \beta x_i)^2}{2\sigma^2}} \\
          &= (2\pi\sigma^2)^{-\frac{n}{2}} \prod_{i=1}^n e^{-\frac{(y_i - \beta x_i)^2}{2\sigma^2}} \\ 
          &= (2\pi\sigma^2)^{-\frac{n}{2}} e^{-(2\sigma^2)^{-1}\sum_{i=1}^n (y_i - \beta x_i)^2} \\
    \end{aligned}
\end{equation}

The log-likelihood function is:

\begin{equation}\label{eq:ex-ols-model-loglik}
    \begin{aligned}
        \mathcal{l}(\beta, \sigma^2;y_1, \dots, y_n, x_1, \dots, x_n)
          &= \log\left(\mathcal{L}(\beta, \sigma^2;y_1, \dots, y_n, x_1, \dots, x_n)\right) \\
          &= \log\left((2\pi\sigma^2)^{-\frac{n}{2}} e^{-(2\sigma^2)^{-1}\sum_{i=1}^n (y_i - \beta x_i)^2}\right) \\
          &= -\frac{n}{2}\log\left(2\pi\sigma^2\right) - (2\sigma^2)^{-1} \sum_{i=1}^n (y_i - \beta x_i)^2 \\
    \end{aligned}
\end{equation}

The score equation for $\beta$ is given by the partial derivative of the log-likelihood function with respect to $\beta$:

\begin{equation}\label{eq:ex-ols-model-score-equation-beta}
    \begin{aligned}
        \mathcal{S}(\beta;y_1, \dots, y_n, x_1, \dots, x_n)
          &= \frac{\partial}{\partial\beta} \mathcal{l}(\beta, \sigma^2;y_1, \dots, y_n, x_1, \dots, x_n) \\
          &= \sigma^{-2} \sum_{i=1}^n x_i^\intercal (y_i - \beta x_i) \\
          &= \sigma^{-2} \left(\sum_{i=1}^n x_i^\intercal y_i - \sum_{i=1}^n x_i^\intercal x_i \beta \right) \\
    \end{aligned}
\end{equation}

Setting the score equation for $\beta$ equal to zero and solving provides the \gls{mle} for $\beta$ (we switch to matrix notation for the final step for the sake of convenience as well as matching traditional \gls{ols} notation):

\begin{equation}\label{eq:ex-ols-beta-mle}
    \begin{aligned}
        \sigma^{-2} \left(\sum_{i=1}^n x_i^\intercal y_i - \sum_{i=1}^n x_i^\intercal \beta x_i\right) &= 0 \\
        \sum_{i=1}^n x_i^\intercal y_i &= \beta \sum_{i=1}^n x_i^\intercal x_i \\
        \hat{\beta} &= \left(\sum_{i=1}^n x_i^\intercal x_i\right)^{-1} \sum_{i=1}^n x_i^\intercal y_i \\
        \implies \hat{\beta} &= \left(X^\intercal X\right)^{-1} X^\intercal y \\
    \end{aligned}
\end{equation}

Likewise, the score equation for $\sigma^2$ is given by:

\begin{equation}\label{eq:ex-ols-model-score-equation-sigma-squared}
    \begin{aligned}
        \mathcal{S}(\sigma^2;y_1, \dots, y_n, x_1, \dots, x_n)
          &= \frac{\partial}{\partial\sigma^2} \mathcal{l}(\beta, \sigma^2;y_1, \dots, y_n, x_1, \dots, x_n) \\
          &= \frac{\partial}{\partial\sigma^2} \left(-\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - (2\sigma^2)^{-1} \sum_{i=1}^n (y_i - \beta x_i)^2\right) \\
          &= -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (y_i - \beta x_i)^2 \\
          &= \frac{1}{2\sigma^2} \left(-n + \sigma^{-2}\sum_{i=1}^n (y_i - \beta x_i)^2\right) \\
    \end{aligned}
\end{equation}

Setting equal to zero and solving provides the \gls{mle} for $\sigma^2$:

\begin{equation}\label{eq:ex-ols-sigma-squared-mle}
    \begin{aligned}
        -n + \sigma^{-2}\sum_{i=1}^n (y_i - \beta x_i)^2 &= 0 \\
        \sigma^{-2}\sum_{i=1}^n (y_i - \beta x_i)^2 &= n \\
        \sum_{i=1}^n (y_i - \beta x_i)^2 &= n\sigma^2 \\
        \implies \hat{\sigma}^2 &= n^{-1}\sum_{i=1}^n (y_i - \hat{\beta} x_i)^2 \\
    \end{aligned}
\end{equation}

The \gls{ols} estimator for $\beta$ can also be derived (much more simply) using matrix calculus. See \cite[Chapter~3.2]{hastie_elements_2009} for a geometric treatment. Define the \gls{rss} as follows:

\begin{equation}\label{eq:ex-ols-model-rss-matrix}
    \text{RSS}(\beta) = (y - X\beta)^\intercal (y - X\beta)
\end{equation}

Derive with respect to $\beta$ and set equal to zero, then solve:

\begin{equation}\label{eq:ex-ols-matrix-solution-beta}
    \begin{aligned}
        \frac{\partial\text{RSS}}{\partial\beta} \left((y - X\beta)^\intercal (y - X\beta)\right) &= 0 \\
        -2X^\intercal(y - X\beta) &= 0 \\
        -X^\intercal y + X^\intercal X \beta &= 0 \\
        X^\intercal X \beta &= X^\intercal y \\
        \implies \hat{\beta} &= \left(X^\intercal X\right)^{-1} X^\intercal y
    \end{aligned}
\end{equation}

This is a global minimum if the second derivative is positive, which is true if and only if $X$ is of full rank (and thus $X^\intercal X$ is positive definite):

\begin{equation}\label{eq:ex-ols-model-matrix-pd}
    \frac{\partial^2\text{RSS}}{\partial\beta\partial\beta^\intercal} \left((y - X\beta)^\intercal (y - X\beta)\right) = 2X^\intercal X
\end{equation}

The asymptotic distribution for $\hat{\beta}$ is multivariate normal:

\begin{equation}\label{eq:ex-ols-beta-hat-asymptotics}
    \hat{\beta} \sim \mathcal{N}\left(\beta, \left(X^\intercal X\right)^{-1}\sigma^2\right)
\end{equation}

Lastly, there is also an unbiased estimator of $\sigma^2$, defined as follows:

\begin{equation}\label{eq:ex-ols-unbiased-sigma-squared-hat}
    \hat{\sigma}^2 = (n - p - 1)^{-1} \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2
\end{equation}

This quantity has an asymptotic $\chi$-squared distribution with $n - p - 1$ degrees of freedom i.e.:

\begin{equation}\label{eq:ex-ols-sigma-squared-unbiased-asymptotics}
    (n - p - 1)\hat{\sigma}^2 \sim \sigma^2\chi^2_{n-p-1}
\end{equation}

The following is taken verbatim from Question 1 of the 2023 Qualifying Exam. Suppose we observe a sequence of \glspl{rv} $X_1, \dots, X_n \sim \text{Pareto}(\theta)$, such that $X_i$ has the following \gls{pdf} where $\phi > 0$ is an \textit{a priori} known scale parameter:

\begin{equation}\label{eq:ex-ols-pareto-pdf}
    p(x_i|\theta) = \frac{\theta\phi^\theta}{x_i^{\theta+1}} \quad x_i \geq \phi
\end{equation}

Our first task is to identify the likelihood function for $\theta$, which we show below:

\begin{equation}\label{eq:ex-ols-pareto-likelihood}
    \begin{aligned}
        \mathcal{L}(\theta; x_1, \dots, x_n)
          &= \prod_{i=1}^n f(x_i|\theta) \\
          &= \prod_{i=1}^n \frac{\theta\phi^\theta}{x_i^{\theta+1}} \\
          &= \theta^n\phi^{n\theta} \prod_{i=1}^n x_i^{-(\theta + 1)} \\
    \end{aligned}
\end{equation}

It follows that the log-likelihood is:

\begin{equation}\label{eq:ex-ols-pareto-log-likelihood}
    \begin{aligned}
        \mathcal{l}(\theta; x_1, \dots, x_n)
          &= \log\left(\theta^n\phi^{n\theta} \prod_{i=1}^n x_i^{-(\theta + 1)}\right) \\
          &= n\log(\theta) + n\theta\log(\phi) + \sum_{i=1}^n -(\theta + 1)\log(x_i) \\
          &= n\log(\theta) + n\theta\log(\phi) -(\theta + 1) \sum_{i=1}^n \log(x_i) \\
    \end{aligned}
\end{equation}

Our next goal is to find the score function for $\theta$, which we do by differentiating the log-likelihood with respect to $\theta$:

\begin{equation}\label{eq:ex-ols-pareto-score-function}
    \begin{aligned}
        \mathcal{S}(\theta; x_1, \dots, x_n)
          &= \frac{\partial}{\partial\theta}\left(n\log(\theta) + n\theta\log(\phi) -(\theta + 1) \sum_{i=1}^n \log(x_i)\right) \\
          &= \frac{n}{\theta} + n\log(\phi) - \sum_{i=1}^n \log(x_i) \\
          &= n \left(\theta^{-1} + \log(\phi)\right) - \sum_{i=1}^n \log(x_i) \\
    \end{aligned}
\end{equation}

Continuing on, we need to find the expected Fisher's Information for $\theta$:

\begin{equation}\label{eq:ex-ols-pareto-fishers-information}
    \begin{aligned}
        \mathcal{I}_n(\theta)
          &= -\mathbb{E}\left[\frac{\partial}{\partial\theta} \left(n \left(\theta^{-1} + \log(\phi)\right) - \sum_{i=1}^n \log(x_i)\right) \bigg | \theta\right] \\
          &= -\mathbb{E}\left[-n\theta^{-2} | \theta\right] \\
          &= n\theta^{-2} \\
        \implies I_1(\theta) &= \theta^{-2} \\
    \end{aligned}
\end{equation}

The next part tells us to find the \gls{mle} of $\theta$, which we do by setting the score function equal to 0 and solving (for a confirmation of this result see \cite{rytgaard_estimation_1990}):

\begin{equation}\label{eq:ex-ols-pareto-theta-mle}
    \begin{aligned}
        n \left(\theta^{-1} + \log(\phi)\right) - \sum_{i=1}^n \log(x_i) &= 0 \\
        n\theta^{-1} + n\log(\phi) &= \sum_{i=1}^n \log(x_i) \\
        n\theta^{-1} &= \sum_{i=1}^n \log(x_i) - \log(\phi) \\
        \implies \frac{n}{\sum_{i=1}^n \log\left(\frac{x_i}{\phi}\right)} &= \hat{\theta}_n \\
    \end{aligned}
\end{equation}

Next, we're told to find a large sample, model-based variance estimator for $\hat{\theta}_n$ as a function of $\bar{Z}_n$, where $\bar{Z}_n$ is defined like so:

\begin{equation}\label{eq:ex-ols-pareto-zbar-definition}
    \begin{aligned}
        \mathcal{S}(\theta; x_1, \dots, x_n)
          &= n \left(\theta^{-1} + \log(\phi)\right) - \sum_{i=1}^n \log(x_i) \\
          &= n\theta^{-1} - \sum_{i=1}^n \log(x_i) - \log(\phi) \\
          &= n\theta^{-1} - \sum_{i=1}^n Z_i \\
          &= n\left(\theta^{-1} - \bar{Z}_n\right) \\
    \end{aligned}
\end{equation}

From \cite[Chapter~2.4.1]{wakefield_bayesian_2013}, the asymptotic distribution of the \gls{mle} is given by:

\begin{equation}\label{eq:ex-ols-pareto-mle-asymptotic-dist}
    \begin{aligned}
        n^{\frac{1}{2}} \left(\hat{\theta}_n - \theta\right) &\overset{d}{\to} \mathcal{N}\left(0, I_1(\theta)^{-1}\right) \\
        n^{\frac{1}{2}} \left(\hat{\theta}_n - \theta\right) &\overset{d}{\to} \mathcal{N}\left(0, \theta^2\right) \\
    \end{aligned}
\end{equation}

Lastly, we are instructed to utilize sandwich variance estimation to show that, when $n$ is large, a variance estimator for $\hat{\theta}_n$ is $n^{-1}S^2_n\left(\bar{Z}_n\right)^{-4}$, where $S^2_n = \text{Var}(Z_i)$. We recall that the sandwich estimator of variance is defined like so, where $G_n(\theta) = n^{-1} \sum_{i=1}^n G(\theta, y_i)$ is an estimating equation for $\theta$ (refer to Section \ref{sec:sandwich-variance-estimation}):

\begin{equation}\label{eq:ex-ols-pareto-sandwich-variance-estimator}
    \begin{aligned}
        n^{\frac{1}{2}}\left(\hat{\theta_n} - \theta\right) &\overset{d}{\to} \mathcal{N}\left(0, B^{-1} M \left(B^\intercal\right)^{-1}\right) \\
        B &= \mathbb{E}\left[\frac{\partial}{\partial\theta} G(\theta, y)\right] \\
        M &= \text{Var}(G(\theta, y)) \\
    \end{aligned}
\end{equation}

In our case, we define the estimating equation as:

\begin{equation}\label{eq:ex-ols-pareto-estimating-equation}
    \begin{aligned}
        G_n(\theta, y) 
          &= n^{-1} \mathcal{S}(\theta; y_1, \dots, y_n) \\
          &= n^{-1} \sum_{i=1}^n \frac{\partial}{\partial\theta} \left(\log(f(x_i | \theta))\right) \\
          &= n{-1} \sum_{i=1}^n \frac{\partial}{\partial\theta} \left(\log(\theta) + \theta\log(\phi) - (\theta + 1)\log(x_i)\right) \\
          &= n^{-1} \sum_{i=1}^n \theta^{-1} + \log\left(\frac{\phi}{x_i}\right) \\
    \end{aligned}
\end{equation}

Now we can define the ``pieces'' of our sandwich:

\begin{equation}\label{eq:ex-ols-pareto-sandwich-pieces}
    \begin{aligned}
        B 
          &= \mathbb{E}\left[\frac{\partial}{\partial\theta} \left(\theta^{-1} + \log\left(\frac{\phi}{x_i}\right)\right)\right] \\
          &= \mathbb{E}\left[-\theta^{-2}\right] \\
          &= -\theta^{-2} \\
        M 
          &= \text{Var}\left(\theta^{-1} + \log\left(\frac{\phi}{x_i}\right)\right) \\
          &= \mathbb{E}\left[\left(\theta^{-1} + \log\left(\frac{\phi}{x_i}\right)\right)^2\right] \\
          &= \mathbb{E}\left[\theta^{-2} + 2\theta^{-1}\log\left(\frac{\phi}{x_i}\right) + \log^2\left(\frac{\phi}{x_i}\right)\right] \\
    \end{aligned}
\end{equation}

TODO -- finished this question after asking someone for help.

\subsection{Regularized regression}

This problem is taken verbatim from a 2022 STA 7233 homework assignment. Consider a true regression model of the following form, where $X_i \in \mathbb{R}^p$ and $i = 1, \dots, n$:

\begin{equation}\label{eq:ex-ridge-model-form}
    y_i = \beta_0 + \beta X_i + \epsilon_i
\end{equation}

Suppose that the dependent variable $y$ and the covariates $X$ have been mean-centered, and that the following are true concerning the error:

\begin{equation}\label{eq:ex-ridge-model-assumptions}
    \begin{aligned}
        \mathbb{E}[\epsilon_i] &= 0 \\
        \text{Var}(\epsilon_i) &< \infty \\
        e_i &\perp e_j\; \forall i \neq j \\
    \end{aligned}
\end{equation}

Suppose as well that the design matrix is orthonormal i.e., $X^\intercal X = I$. We would like to identify the value of $\lambda$ that yields the minimum \gls{mse} of the ridge regression estimator of the coefficients i.e.:

\begin{equation}\label{eq:ex-ridge-minimization-problem}
    \underset{\lambda>0}{\text{arg min }}\text{MSE}\left(\hat{\beta}^{\text{Ridge}}\right)
\end{equation}

The \gls{ols} and ridge regression estimators of $\beta$ are given by:

\begin{equation}\label{eq:ex-ridge-beta-ests}
    \begin{aligned}
        \hat{\beta}^{\text{OLS}} &= \left(X^\intercal X\right)^{-1} X^\intercal y \\
        \hat{\beta}^{\text{Ridge}} &= \left(X^\intercal X + \lambda I\right)^{-1} X^\intercal y \\
    \end{aligned}
\end{equation}

We also define the following:

\begin{equation}\label{eq:ex-ridge-beta-ridge-properties}
    \begin{aligned}
        W &= \left(X^\intercal X + \lambda I\right)^{-1} X^\intercal X \\
        \implies \hat{\beta}^{\text{Ridge}} &= W \hat{\beta}^{\text{OLS}} \\
        \text{Var}\left(\hat{\beta}^{\text{Ridge}}\right) &= \sigma^2 W\left(X^\intercal X\right)^{-1} W^\intercal \\
        \text{Bias}^2\left(\hat{\beta}^{\text{Ridge}}, \beta\right) &= \beta^\intercal (W - I)^\intercal (W - I) \beta \\
    \end{aligned}
\end{equation}

We'll begin by decomposing the \gls{mse} into a scalar form using the eigendecomposition of $X^\intercal X$:

\begin{equation}\label{eq:ex-ridge-mse-decomp}
    \begin{aligned}
        \text{MSE}\left(\hat{\beta}^{\text{Ridge}}\right)
          &= \mathbb{E}\left[(W\hat{\beta}^{\text{OLS}} - \beta)^\intercal(W\hat{\beta}^{\text{OLS}} - \beta)\right] \\
          &= \text{Var}\left(\hat{\beta}^{\text{Ridge}}\right) + \text{Bias}^2\left(\hat{\beta}^{\text{Ridge}}, \beta\right) \\
          &= \sigma^2 \text{trace}\left(W\left(X^\intercal X\right)^{-1}W^\intercal\right) + \beta^\intercal (W - I)^\intercal (W - I) \beta \\
          &= \sigma^2 \text{trace}\left(((\lambda+1)I)^{-1}(((\lambda+1)I)^{-1})^\intercal\right) + \beta^\intercal (((\lambda+1)I)^{-1} - I)^\intercal (((\lambda+1)I)^{-1} - I) \beta \\
          &= \sigma^2 \text{trace}((1+\lambda)I)^{-2} + ((1+\lambda)^{-1} -1)^2\beta^\intercal\beta \\
          &= \frac{\sigma^2p}{(1+\lambda)^2} + ((1+\lambda)^{-1} -1)^2\beta^\intercal\beta \\
          &= \frac{\sigma^2p}{(1+\lambda)^2} + \frac{\lambda^2}{(1+\lambda)^2} \beta^\intercal\beta \\
    \end{aligned}
\end{equation}

Next, we derive with respect to $\lambda$, set the resulting gradient equal to zero, and solve for $\lambda$ to identify the value that minimizes the \gls{mse} under orthonormal design:

\begin{equation}\label{eq:ex-ridge-lambda-optimal}
    \begin{aligned}
        \frac{\partial}{\partial\lambda} \left(\frac{\sigma^2p}{(1+\lambda)^2} + \frac{\lambda^2}{(1+\lambda)^2} \beta^\intercal\beta\right) &= 0 \\
        -\frac{2\sigma^2p}{(1+\lambda)^3} + \frac{2\lambda}{(1+\lambda)^3} \beta^\intercal\beta &= 0 \\
        2\sigma^2p &= 2\lambda\beta^\intercal\beta \\
        \lambda &= \sigma^2p\left(\beta^\intercal\beta\right)^{-1} \\
    \end{aligned}
\end{equation}

\section{Generalized linear models}\label{sec:examples-glm}

\subsection{Exponential family distributions}

Suppose an \gls{rv} $y_i$ has a Poisson distribution such that its \gls{pmf} is:

\begin{equation}\label{eq:ex-glm-poisson-dist}
    f(y_i|\lambda) = \frac{e^{-\lambda}\lambda^{y_i}}{y_i!}
\end{equation}

Rearranging, we arrive at the exponential dispersion form:

\begin{equation}\label{eq:ex-glm-poisson-dist-exp-form}
    \begin{aligned}
        f(y_i|\lambda)
          &= \text{exp}(\log(f(y_i|\lambda))) \\
          &= \text{exp}(y_i\log(\lambda) - \lambda - \log(y_i!)) \\
          &= \text{exp}(y_i\log(\lambda) - \text{exp}(\log(\lambda)) - \log(y_i!)) \\
    \end{aligned}
\end{equation}

The natural parameter of the exponential family distribution is thus $\theta = \log(\lambda)$. The rest of the parameters are:

\begin{equation}\label{eq:ex-glm-poisson-dist-params}
    \begin{aligned}
        b(\theta) &= \text{exp}(\theta) \\
        a(\phi) &= 1 \\
        c(y_i, \phi) &= -\log(y_i!) \\
    \end{aligned}
\end{equation}

The expected value and variance of the distribution are as follows, matching what we know about the Poisson distribution:

\begin{equation}\label{eq:ex-glm-poisson-dist-mean-var}
    \begin{aligned}
        \mathbb{E}[y_i] 
          &= b^\prime(\theta) \\
          &= \text{exp}(\theta) \\
          &= \text{exp}(\log(\lambda)) \\
          &= \lambda \\
        \text{Var}(y_i)
          &= b^{\prime\prime}(\theta)a(\phi) \\
          &= \text{exp}(\theta) \\
          &= \text{exp}(\log(\lambda)) \\
          &= \lambda \\
    \end{aligned}
\end{equation}

A common alternative to the Poisson in the case of overdispersion is the Negative-binomial, the \gls{pmf} of which is given by:

\begin{equation}\label{eq:ex-glm-nb-dist}
    f(y_i|r, p) = \binom{y_i+r-1}{r-1} p^r (1-p)^{y_i}
\end{equation}

This is interpreted as the probability of observing $y_i$ failures prior to $r$ successes in a series of Bernoulli trials. Rearranging gives us the exponential family form with natural parameter $\theta = \log(1-p)$:

\begin{equation}\label{eq:ex-glm-nb-dist-exp-form}
    \begin{aligned}
        f(y_i|r,p)
          &= \text{exp}\left(\log\left(\binom{y_i+r-1}{r-1} p^r (1-p)^{y_i}\right)\right) \\
          &= \text{exp}\left(\log\left(\binom{y_i+r-1}{r-1}\right) r\log(p) + y_i\log(1-p)\right) \\
    \end{aligned}
\end{equation}

The rest of the parameters are thus as follows, making use of the identity $e^\theta = 1 - p$:

\begin{equation}\label{eq:exp-glm-nb-dist-params}
    \begin{aligned}
        b(\theta)
          &= -r\log(p) \\
          &= -r\log(1 - e^\theta) \\
        a(\phi) &= 1 \\
        c(y_i, \phi) &= \log\left(\binom{y_i+r-1}{r-1}\right) \\
    \end{aligned}
\end{equation}

Deriving $b(\theta)$ provides us with the mean and variance:

\begin{equation}\label{eq:ex-glm-nb-dist-mean-var}
    \begin{aligned}
        \mathbb{E}[y_i]
          &= b^\prime(\theta) \\
          &= \left(-\frac{r}{1-e^\theta}\right)\left(-e^\theta\right) \\
          &= \frac{re^\theta}{1-e^\theta} \\
          &= \frac{r(1-p)}{p} \\
        \text{Var}(y_i)
          &= b^{\prime\prime}(\theta) \\
          &= r\left(\frac{e^\theta\left(1-e^\theta\right) - \left(-e^\theta\right)\left(e^\theta\right)}{\left(1-e^\theta\right)^2}\right) \\
          &= \frac{re^\theta}{\left(1-e^\theta\right)^2} \\
          &= \frac{r(1-p)}{p^2} \\
    \end{aligned}
\end{equation}

The variance can also be defined in terms of the mean since $\mu = \frac{r(1-p)}{p}$:

\begin{equation}\label{eq:ex-glm-nb-dist-variance-by-mean}
    \text{Var}(y_i) = \mu + \frac{\mu^2}{r}
\end{equation}

A popular alternative parameterization uses $\alpha = r^{-1}$, which makes the variance directly proportional to the mean instead of inversely. In this case, we have the following definition of the natural parameter $\theta$:

\begin{equation}\label{eq:ex-glm-nb-dist-alpha-parameterization}
    \begin{aligned}
      \mu &= \frac{\alpha^{-1}(1-p)}{p} \\
      \alpha p \mu &= 1 - p \\
      p(1+\alpha\mu) &= 1 \\
      p &= (1 + \alpha\mu)^{-1} \\
      \implies \theta 
        &= \log\left(1 - (1+\alpha\mu)^{-1}\right) \\
        &= \log\left(\frac{\alpha\mu}{1+\alpha\mu}\right) \\
    \end{aligned}
\end{equation}

We then have:

\begin{equation}\label{eq:ex-glm-nb-dist-alpha-parameterization-mean-var}
    \begin{aligned}
      b(\theta)
        &= -\alpha^{-1}\log\left((1+\alpha\mu)^{-1}\right) \\
        &= \alpha^{-1}\log(1+\alpha\mu) \\
      \mathbb{E}[y_i]
        &= b^\prime(\theta) \\
        &= \mu \\
      \text{Var}(y_i)
        &= b^{\prime\prime}(\theta) \\
        &= \mu + \alpha\mu^2 \\
    \end{aligned}
\end{equation}

If we suppose instead that $y_i$ follows a Normal distribution, the \gls{pdf} is defined as:

\begin{equation}\label{eq:ex-glm-normal-dist}
    f(y_i|\mu, \sigma^2) = (2\pi\sigma^2)^{-\frac{1}{2}} e^{-\frac{(y_i - \mu)^2}{2\sigma^2}}
\end{equation}

Rearranging, we arrive at the exponential family form:

\begin{equation}\label{eq:ex-glm-normal-dist-exp-form}
    \begin{aligned}
        f(y_i|\mu, \sigma^2) 
          &= \text{exp}(\log(f(y_i|\mu, \sigma^2))) \\
          &= \text{exp}\left(-\frac{1}{2}\log(2\pi\sigma^2) - \frac{(y_i - \mu)^2}{2\sigma^2}\right) \\
          &= \text{exp}\left(-\frac{1}{2}\log(2\pi\sigma^2) - \frac{y_i^2 - 2y_i\mu + \mu^2}{2\sigma^2}\right) \\
          &= \text{exp}\left(\frac{y_i\mu - \frac{1}{2}\mu^2}{\sigma^2} - \frac{y_i^2}{2\sigma^2} - \frac{1}{2}\log(2\pi\sigma^2)\right) \\
    \end{aligned}
\end{equation}

The natural parameter is $\theta = \mu$, and the rest of the parameters are as follows:

\begin{equation}\label{eq:ex-glm-normal-dist-params}
    \begin{aligned}
        b(\theta) &= \frac{1}{2}\mu^2 \\
        a(\phi) &= \sigma^2 \\
        c(y, \phi) &= - \frac{y_i^2}{2\sigma^2} - \frac{1}{2}\log(2\pi\sigma^2) \\
    \end{aligned}
\end{equation}

The expected value and variance are thus:

\begin{equation}\label{eq:ex-glm-normal-dist-mean-var}
    \begin{aligned}
        \mathbb{E}[y_i]
          &= b^\prime(\theta) \\
          &= \mu \\
        \text{Var}(y_i)
          &= b^{\prime\prime}(\theta)a(\phi) \\
          &= \sigma^2 \\
    \end{aligned}
\end{equation}

If we assume a Binomial distribution for $y_i$ we have the following \gls{pmf}:

\begin{equation}\label{eq:ex-glm-binomial-dist}
    f(y_i|n, p) = \binom{n}{y_i}p^{y_i} (1-p)^{n-y_i}
\end{equation}

We exponentiate the log of the \gls{pmf} and rearrange to arrive at the exponential family form:

\begin{equation}\label{eq:ex-glm-binomial-dist-exp-form}
    \begin{aligned}
        f(y_i|n, p) 
          &= \text{exp}\left(\log\left(\binom{n}{y_i}p^{y_i} (1-p)^{n-y_i}\right)\right) \\
          &= \text{exp}\left(\log\left(\binom{n}{y_i}\right) + y_i\log(p) + (n-y_i)\log(1-p)\right) \\
          &= \text{exp}\left(\log\left(\binom{n}{y_i}\right) + y_i\log(p) + n\log(1-p) - y_i\log(1-p)\right) \\
          &= \text{exp}\left(y_i\log\left(\frac{p}{1-p}\right) + n\log(1-p) + \log\left(\binom{n}{y_i}\right)\right) \\
    \end{aligned}
\end{equation}

The natural parameter is thus $\log\left(\frac{p}{1-p}\right)$, and the rest of the parameters are as follows:

\begin{equation}\label{eq:ex-glm-binomial-dist-params}
    \begin{aligned}
        b(\theta)
          &= -n \log(1 - p) \\
          &= n \log(1 + \text{exp}(\theta)) \\
        a(\phi) &= 1 \\
        c(y, \phi) &= \log\left(\binom{n}{y_i}\right) \\
    \end{aligned}
\end{equation}

The expected value and variance are thus:

\begin{equation}\label{eq:ex-glm-binomial-dist-mean-var}
    \begin{aligned}
        \mathbb{E}[y_i]
          &= b^\prime(\theta) \\
          &= n \frac{\text{exp}(\theta)}{1 + \text{exp}(\theta)} \\
          &= n \frac{\frac{p}{1-p}}{1 + \frac{p}{1-p}} \\
          &= np \\
        \text{Var}(y_i)
          &= b^{\prime\prime}(\theta)a(\phi) \\
          &= n \frac{\text{exp}(\theta)}{(1 + \text{exp}(\theta))^2} \\
          &= n \frac{\text{exp}(\theta)}{1 + 2\text{exp}(\theta) + (\text{exp}(\theta))^2} \\
          &= n \frac{\frac{p}{1-p}}{1 + \frac{2p}{1-p} + \left(\frac{p}{1-p}\right)^2} \\
          &= n \frac{\frac{p}{1-p}}{\frac{1+p}{1-p} + \frac{p^2}{(1-p)^2}} \\
          &= n \frac{\frac{p}{1-p}}{\frac{(1+p)(1-p) + p^2}{(1-p)^2}} \\
          &= np(1-p) \\
    \end{aligned}
\end{equation}

Lastly, if we assume an Exponential distribution for $y_i$, we have the following \gls{pdf}:

\begin{equation}\label{eq:ex-glm-excponential-dist}
    f(y_i|\lambda) = \lambda\text{exp}(-\lambda y_i)
\end{equation}

Rearranging, we arrive at the exponential family form:

\begin{equation}\label{eq:ex-glm-exponential-dist-exp-form}
    \begin{aligned}
        f(y_i|\lambda)
          &= \text{exp}(\log(\lambda\text{exp}(-\lambda y_i))) \\
          &= \text{exp}(\log(\lambda) - \lambda y_i) \\
    \end{aligned}
\end{equation}

The natural parameter is thus $\theta = -\lambda$, and the rest of the parameters are as follows:

\begin{equation}\label{eq:ex-glm-exponential-dist-params}
    \begin{aligned}
        b(\theta) &= \log(-\theta) \\
        a(\phi) &= 1 \\
        c(y, \phi) &= 0 \\
    \end{aligned}
\end{equation}

The expected value and variance are then:

\begin{equation}\label{eq:ex-glm-exponential-dist-mean-var}
    \begin{aligned}
        \mathbb{E}[y_i]
          &= b^\prime(\theta) \\
          &= -\theta^{-1} \\
          &= \frac{1}{\lambda} \\
        \text{Var}(y_i)
          &= b^{\prime\prime}(\theta)a(\phi) \\
          &= \theta^{-2} \\
          &= \frac{1}{(-\lambda)^2} \\
          &= \frac{1}{\lambda^2} \\
    \end{aligned}
\end{equation}

\subsection{Maximum likelihood estimation}

Suppose we have a sequence of Poisson-distributed random variables $y_i \overset{\text{IID}}{\sim} \text{Poisson}(\lambda)$ with the following \gls{pmf}:

\begin{equation}\label{eq:ex-glm-mle-poisson-dist}
    f(y_i|\lambda) = \frac{\lambda^{y_i}e^{-\lambda}}{y_i!}
\end{equation}

The likelihood function is:

\begin{equation}\label{eq:ex-glm-mle-poisson-likelihood}
    \begin{aligned}
        \mathcal{L}(\lambda;y_1, \dots, y_n) 
          &= \prod_{i=1}^n f(y_i|\lambda) \\
          &= \prod_{i=1}^n \frac{\lambda^{y_i}e^{-\lambda}}{y_i!} \\
    \end{aligned}
\end{equation}

The log-likelihood function is then:

\begin{equation}\label{eq:ex-glm-mle-poisson-loglik}
    \begin{aligned}
        \mathcal{l}(\lambda;y_1, \dots, y_n) 
          &= \log\left(\mathcal{L}(\lambda;y_1, \dots, y_n)\right) \\
          &= \sum_{i=1}^n \log\left(\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\right) \\
          &= \sum_{i=1}^n y_i\log(\lambda) - \lambda - \log(y_i!) \\
          &= \log(\lambda)\sum_{i=1}^n y_i -n\lambda - \sum_{i=1}^n \log(y_i!) \\
    \end{aligned}
\end{equation}

The score equation for $\lambda$ is obtained by deriving the log-likelihood:

\begin{equation}\label{eq:ex-glm-mle-poisson-score-equation}
    \begin{aligned}
        \mathcal{S}(\lambda; y_1, \dots, y_n)
          &= \frac{\partial}{\partial\lambda} \mathcal{l}(\lambda;y_1, \dots, y_n) \\
          &= \lambda^{-1}\sum_{i=1}^n y_i - n \\
    \end{aligned}
\end{equation}

Setting equal to zero and solving provides us with the \gls{mle} of $\lambda$:

\begin{equation}\label{eq:ex-glm-mle-poisson-lambda-mle}
    \begin{aligned}
        \lambda^{-1}\sum_{i=1}^n y_i - n &= 0 \\
        \sum_{i=1}^n y_i &= n\lambda \\
        \implies \hat{\lambda} &= n^{-1}\sum_{i=1}^n y_i \\
    \end{aligned}
\end{equation}

If we assume instead that $y_i$ is exponentially distributed i.e., $y_i \overset{\text{IID}}{\sim}\text{Exponential}(\lambda)$, we have the following \gls{pdf}:

\begin{equation}\label{eq:ex-glm-mle-exponential-dist}
    f(y_i|\lambda) = \lambda e^{-\lambda y_i}
\end{equation}

The likelihood function is thus:

\begin{equation}\label{eq:ex-glm-mle-exponential-likelihood}
    \begin{aligned}
        \mathcal{L}(\lambda;y_1, \dots, y_n) 
          &= \prod_{i=1}^n f(y_i|\lambda) \\
          &= \prod_{i=1}^n \lambda e^{-\lambda y_i} \\
    \end{aligned}
\end{equation}

The log-likelihood is then:

\begin{equation}\label{eq:ex-glm-mle-exponential-loglik}
    \begin{aligned}
        \mathcal{l}(\lambda;y_1, \dots, y_n) 
          &= \log\left(\mathcal{L}(\lambda;y_1, \dots, y_n)\right) \\
          &= \sum_{i=1}^n \log(\lambda) - \lambda y_i \\
          &= n\log(\lambda) - \lambda\sum_{i=1}^n y_i \\
    \end{aligned}
\end{equation}

The score equation for $\lambda$ is obtained by deriving with respect to $\lambda$:

\begin{equation}\label{eq:ex-glm-mle-exponential-score-equation}
    \begin{aligned}
        \mathcal{S}(\lambda; y_1, \dots, y_n)
          &= \frac{\partial}{\partial\lambda} \mathcal{l}(\lambda;y_1, \dots, y_n) \\
          &= n\lambda^{-1} - \sum_{i=1}^n y_i \\
    \end{aligned}
\end{equation}

Setting equal to zero and solving provides us with $\hat{\lambda}$:

\begin{equation}\label{eq:ex-glm-mle-exponential-lambda-mle}
    \begin{aligned}
        n\lambda^{-1} - \sum_{i=1}^n y_i &= 0 \\
        n\lambda^{-1} &= \sum_{i=1}^n y_i \\
        \implies \hat{\lambda} &= n\left(\sum_{i=1}^n y_i\right)^{-1} \\
    \end{aligned}
\end{equation}

This next question is taken directly from Question 5 of the 2023 Qualifying Exam. Suppose we have a logistic regression model of the following form:

\begin{equation}\label{eq:ex-glm-2023q5-binomial-model-form-1}
    \log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta X_i 
\end{equation}

Rearranging allows us to solve for $\pi_i$

\begin{equation}\label{eq:ex-glm-2023q5-binomial-model-form-2}
    \begin{aligned}
        \frac{\pi_i}{1-\pi_i} &= e^{\beta X_i} \\
        \pi_i &= e^{\beta X_i} - \pi_i e^{\beta X_i} \\
        \pi_i \left(1 - e^{\beta X_i}\right) &= e^{\beta X_i} \\
        \pi_i &= \frac{e^{\beta X_i}}{1-e^{\beta X_i}} \\
    \end{aligned}
\end{equation}

Our first goal is to find the log-likelihood of the response variable $y_i$, where $y_i \sim \text{Binomial}(n_i, \pi_i)$. We start by defining the \gls{pmf}:

\begin{equation}\label{eq:ex-glm-2023q5-binomial-pmf}
    f(y_i | n_i, \pi_i) = \binom{n_i}{\pi_i} \pi_i^{y_i} (1-\pi_i)^{n_i-y_i}
\end{equation}

The likelihood is then:

\begin{equation}\label{eq:ex-glm-2023q5-binomial-likelihood}
    \mathcal{L}\left(n_i, \pi_i; y_1, \dots, y_n\right) = \prod_{i=1}^n \binom{n_i}{y_i} \pi_i^{y_i} (1-\pi_i)^{n_i-y_i}
\end{equation}

Taking the log provides us with the log-likelihood:

\begin{equation}\label{eq:ex-glm-2023q5-binomial-loglikelihood}
    \begin{aligned}
        \mathcal{l}\left(n_i, \pi_i; y_1, \dots, y_n\right) 
          &= \log\left(\prod_{i=1}^n \binom{n_i}{y_i} \pi_i^{y_i} (1-\pi_i)^{n_i-y_i}\right) \\
          &= \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i \log(\pi_i) + (n_i-y_i)\log(1-\pi_i) \\
    \end{aligned}
\end{equation}

Next we need to find the deviance of the fitted model. Recall how the deviance is defined from Equations \ref{eq:glm-resid-deviance-2} and \ref{eq:glm-resid-deviance-3}. We define the overall deviance for our logistic regression as twice the difference between the log-likelihood of the saturated model and that of the fitted model :

\begin{equation}\label{eq:ex-glm-2023q5-overall-deviance-1}
    D = 2 \left(\mathcal{l}(\beta; y_1, \dots, y_n) - \mathcal{l}\left(\hat{\beta}; y_1, \dots, y_n\right)\right)
\end{equation}

The log-likelihood of the saturated model is given by the following, where we substitute in the \gls{mle} $\pi_i = \frac{y_i}{n_i}$:

\begin{equation}\label{eq:ex-glm-2023-q5-saturated-model-log-likelihood}
    \begin{aligned}
        \mathcal{l}(\beta; y_1, \dots, y_n)
          &= \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log(\pi_i) + n_i\log(1 - \pi_i) - y_i\log(1 - \pi_i) \\
          &= \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log\left(\frac{y_i}{n_i}\right) + n_i \log\left(1 - \frac{y_i}{n_i}\right) - y_i \log\left(1 - \frac{y_i}{n_i}\right) \\
          &= \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log\left(\frac{y_i}{n_i}\right) + n_i\log\left(\frac{n_i-y_i}{n_i}\right) - y_i\log\left(\frac{n_i-y_i}{n_i}\right) \\ 
    \end{aligned}
\end{equation}

The log-likelihood for the fitted model is shown below, where we substitute in the \gls{mle} $\hat{y}_i = n_i\hat{\pi}_i$:

\begin{equation}\label{eq:ex-glm-2023-q5-fitted-model-log-likelihood}
    \begin{aligned}
        \mathcal{l}\left(\hat{\beta}; y_1, \dots, y_n\right)
          &= \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log\left(\frac{\hat{y}_i}{n_i}\right) + n_i\log\left(1 - \frac{\hat{y}_i}{n_i}\right) - y_i\log\left(1 - \frac{\hat{y}_i}{n_i}\right) \\
          &= \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log\left(\frac{\hat{y}_i}{n_i}\right) + n_i\log\left(\frac{n_i - \hat{y}_i}{n_i}\right) - y_i\log\left(\frac{n_i - \hat{y}_i}{n_i}\right) \\
    \end{aligned}
\end{equation}

Now we can derive the deviance, which has a $\chi$-squared distribution with \gls{df} equal to $n-p$:

\begin{equation}\label{eq:ex-glm-2023-q5-overall-deviance-2}
    \begin{split}
        D
          &\propto \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log\left(\frac{y_i}{n_i}\right) + n_i\log\left(\frac{n_i-y_i}{n_i}\right) - y_i\log\left(\frac{n_i-y_i}{n_i}\right) \\ &-  \sum_{i=1}^n \log\left(\binom{n_i}{y_i}\right) + y_i\log\left(\frac{\hat{y}_i}{n_i}\right) + n_i\log\left(\frac{n_i - \hat{y}_i}{n_i}\right) - y_i\log\left(\frac{n_i - \hat{y}_i}{n_i}\right) \\
          &\propto \sum_{i=1}^n y_i \left(\log\left(\frac{y_i}{n_i}\right) - \log\left(\frac{\hat{y}_i}{n_i}\right)\right) + n_i \left(\log\left(\frac{n_i-y_i}{n_i}\right) - \log\left(\frac{n_i-\hat{y}_i}{n_i}\right)\right) \\ &- y_i \left(\log\left(\frac{n_i-y_i}{n_i}\right) - \log\left(\frac{n_i - \hat{y}_i}{n_i}\right)\right) \\
          &\propto \sum_{i=1}^n y_i \log\left(\frac{y_i}{\hat{y}_i}\right) + n_i \log\left(\frac{n_i-y_i}{n_i-\hat{y}_i}\right) - y_i\log\left(\frac{n_i-y_i}{n_i-\hat{y}_i}\right) \\
          &\propto \sum_{i=1}^n y_i \log\left(\frac{y_i}{\hat{y}_i}\right) + (n_i-y_i) \log\left(\frac{n_i-y_i}{n_i-\hat{y}_i}\right) \\
    \end{split}
\end{equation}

Next, we need to show that the \gls{lrt} statistic and Pearson's $\chi$-squared statistic have the same distribution. When comparing the saturated and fitted models, the \gls{lrt} is equivalent to the deviance as defined above. 

\section{Large sample theory}\label{sec:examples-lst}

\subsection{Modes of convergence}

A proof of the continuous mapping theorem for convergence in distribution is as follows. Suppose the event $g(X_n) \in F$ is identical to the event $X_n \in g^{-1}(F)$. For all closed sets $F$ we have the following, where $C$ is a set such that $g$ is continuous at every point and $P(X \in C) = 1$:

\begin{equation}\label{eq:ex-lst-cmt-dist-proof-1}
    g^{-1}(F) \subset \overline{g^{-1}(F)} \subset g^{-1}(F) \cup C^c
\end{equation}

There thus exists a sequence $x_m \to x$ and $g(x_m) \in F$ for every $F$. If $x \in C$ then $g(x_m) \to g(x)$ and $g(x) \in F$, otherwise $x \in C^c$. Next, using the portmanteau lemma:

\begin{equation}\label{eq:ex-lst-cmt-dist-proof-2}
    \lim \sup \mathbb{P}(g(X_n) \in F) \le \lim \sup \mathbb{P}\left(X_n \in \overline{g^{-1}(F)}\right) \le \mathbb{P}\left(X \in \overline{g^{-1}(F)}\right)
\end{equation}

Since $\mathbb{P}(X \in C^c) = 0$ by definition, we have:

\begin{equation}\label{eq:ex-lst-cmt-dist-proof-3}
    \mathbb{P}(X \in g^{-1}(F)) = \mathbb{P}(g(X) \in F)
\end{equation}

Applying the portmanteau lemma once more in the opposite direction allows us to conclude that $g(X_n) \overset{d}{\to} g(X)$. 

In order to prove the continuous mapping theorem for convergence in probability, we begin by defining an arbitrary constant $\epsilon > 0$. Then, for any $\delta > 0$ we define the following set:

\begin{equation}\label{eq:ex-lst-cmt-prob-proof-1}
    B_\delta = \{x \in S | x \notin D_g : \exists y \in S : |x - y| < \delta, |g(x) - g(y)| > \epsilon \} 
\end{equation}

This set is that of continuity points of $g$ such that it is possible to find a point that maps outside the $\epsilon$-neighborhood of $g(x)$ within the $\delta$-neighborhood of $x$. As $\delta \to 0$ this set shrinks to $\varnothing$. Next, we have:

\begin{equation}\label{eq:ex-lst-cmt-prob-proof-2}
    \mathbb{P}(|g(X_n) - g(X)| > \epsilon) \leq \mathbb{P}(|X_n - X| \geq \delta) + \mathbb{P}(X \in B_\delta) + \mathbb{P}(X \in D_g)
\end{equation}

The first term on the right hand side converges to zero as $n \to \infty$ by the definition of convergence in probability, while the second term converges to zero as $\delta \to 0$ by the definition of the empty set. The final term is equal to zero by the assumptions of the theorem. Ergo:

\begin{equation}\label{eq:ex-lst-cmt-prob-proof-3}
    \lim_{n \to \infty} \mathbb{P}(|g(X_n) - g(X)| > \epsilon) = 0
\end{equation}

We have thus proved that $g(X_n) \overset{p}{\to} g(X)$. 

Lastly, for almost sure convergence we know the following thanks to the continuity of $g$:

\begin{equation}\label{eq:ex-lst-cmt-as-proof-1}
    \lim_{n\to\infty} X_n(\omega) = X(\omega) \implies \lim_{n\to\infty} g(X_n(\omega)) = g(X(\omega))
\end{equation}

Therefore:

\begin{equation}\label{eq:ex-lst-cmt-as-proof-2}
    \begin{aligned}
        \mathbb{P}\left(\lim_{n\to\infty} g(X_n) = g(X)\right)
          &= \mathbb{P}\left(\lim_{n\to\infty} g(X_n) = g(X), X \in D_g\right) \\
          &\geq \mathbb{P}\left(\lim_{n\to\infty} X_n = X, X \in D_g\right) = 1 \\
    \end{aligned}
\end{equation}

Thus $g(X_n) \overset{a.s.}{\to} g(X)$

\subsection{The central limit theorem}

Suppose we have a sequence of random variables $X_1, \dots, X_n \overset{\text{IID}}{\sim} F_X$. The biased sample variance is defined as follows:

\begin{equation}\label{eq:ex-lst-clt-proof-1}
    \begin{aligned}
        S^2_n 
          &= n^{-1} \sum_{i=1}^n \left(X_i - \bar{X}_n\right)^2 \\
          &= n^{-1} \sum_{i=1}^n X_i^2 - 2X_i\bar{X}_n + \left(\bar{X}_n\right)^2 \\
          &= n^{-1} \sum_{i=1}^n X_i^2 - 2n^{-1}\bar{X}_n \sum_{i=1}^n X_i + \left(\bar{X}_n\right)^2 \\
          &= \bar{X}^2_n - 2\bar{X}_n\bar{X}_n + \left(\bar{X}_n\right)^2 \\
          &= \bar{X}^2_n - \left(\bar{X}_n\right)^2 \\
    \end{aligned}
\end{equation}

We know that $\bar{X}^2_n$ and $\bar{X}_n$ are consistent estimators for $m_2$ and $m_1$, respectively. In order to obtain the asymptotic distribution of the two statistics we need to compute the variance-covariance matrix of the two quantities. The variance of $\bar{X}_n$ is easy:

\begin{equation}\label{eq:ex-lst-clt-proof-2}
\begin{aligned}
    \text{Var}\left(\bar{X}_n\right)
      &= \mathbb{E}\left[\bar{X}_n^2\right] - \left(\mathbb{E}\left[\bar{X}_n\right]\right)^2 \\
      &= m_2 - m_1^2 \\
\end{aligned}
\end{equation}

Similarly, the variance of $\bar{X}_n^2$ is obtained without much difficulty:

\begin{equation}\label{eq:ex-lst-clt-proof-3}
    \begin{aligned}
        \text{Var}\left(\bar{X}_n^2\right)
          &= \mathbb{E}\left[\bar{X}_n^4\right] - \left(\mathbb{E}\left[\bar{X}_n^2\right]\right)^2 \\
          &= m_4 - m_2^2 \\
    \end{aligned}
\end{equation}

The last - and trickiest - bit is the covariance between the two quantities:

\begin{equation}\label{eq:ex-lst-clt-proof-4}
    \begin{aligned}
        \text{Cov}\left(\bar{X}_n, \bar{X}_n^2\right)
          &= \mathbb{E}\left[\left(\bar{X}_n - \mathbb{E}\left[\bar{X}_n\right]\right)\left(\bar{X}_n^2 - \mathbb{E}\left[\bar{X}_n^2\right]\right)\right] \\
          &= \mathbb{E}\left[\left(\bar{X}_n - m_1\right)\left(\bar{X}_n^2 - m_2\right)\right] \\
          &= \mathbb{E}\left[\bar{X}_n\bar{X}_n^2 - m2\bar{X}_n - m_1\bar{X}_n^2 + m_1m_2\right] \\
          &= m_3 - m_2m_1 - m_1m_2 + m_1m_2 \\
          &= m_3 - m_1m_2 \\
    \end{aligned}
\end{equation}

Thus the asymptotic distribution is given by:

\begin{equation}\label{eq:ex-lst-clt-proof-5}
    \sqrt{n} \left(\begin{pmatrix}
        \bar{X}_n \\
        \bar{X}_n^2 \\
    \end{pmatrix} - \begin{pmatrix}
        m_1 \\
        m_2 \\
    \end{pmatrix}\right) \overset{d}{\to} \mathcal{N}\left(\begin{pmatrix}
        0 \\
        0 \\
    \end{pmatrix}, \begin{pmatrix}
        m_2 - m_1^2 & m_3 - m_2m_1 \\
        m_3 - m_2m_1 & m_4 - m_2^2 \\
    \end{pmatrix}\right)
\end{equation}

\subsection{Delta method}

Consider a sequence of \glspl{rv} $X_1, \dots, X_n \overset{\text{IID}}{\sim} F_X$ with mean $\mu$ and variance $\sigma^2$. Say we have a function $g(y) = 3e^y$, and we are interested in finding its asymptotic distribution when applied to the summary statistic $\hat{\theta}_n= \bar{X}_n$ for $\theta = \mu$. We have the following:

\begin{equation}\label{eq:ex-lst-delta-method-exp-1}
    \begin{aligned}
        g^\prime(y) &= 3e^y \\
        g^\prime(\mu) &= 3e^\mu \\
        \left(g^\prime(\mu)\right)^2 &= 9e^{\mu^2} \\
    \end{aligned}
\end{equation}

Thus the asymptotic distribution is given by:

\begin{equation}\label{eq:ex-lst-delta-method-exp-2}
    \sqrt{n}\left(3e^{\bar{X}_n} - 3e^\mu\right) \overset{d}{\to} \mathcal{N}\left(0, 9e^{\mu^2}\sigma^2\right)
\end{equation}

We can also derive the asymptotic distribution of the logarithm of the sample mean as follows. We define $\theta = \mu$, $\hat{\theta} = \bar{X}_n$, and $g(y) = \log(y)$. 

\begin{equation}\label{eq:ex-lst-delta-method-log-1}
    \begin{aligned}
      g(y) &= \log(y) \\
      g^\prime(y) &= y^{-1} \\
      \left(g^\prime(\mu)\right)^2 &= \mu^{-2} \\
    \end{aligned}
\end{equation}

The asymptotic distribution is thus:

\begin{equation}\label{eq:ex-lst-delta-method-log-2}
\sqrt{n}\left(\log\left(\bar{X}_n\right) - \log(\mu)\right) \overset{d}{\to} \mathcal{N}(0, \mu^{-2}\sigma^2)
\end{equation}

\subsection{U- and V-statistics}

Consider a sequence of \glspl{rv} $Y_1, \dots, Y_n \overset{\text{IID}}{\sim} F_Y$ with mean $\mu$ and variance $\sigma^2$. We define a U-statistic of degree $r = 1$ for the sample mean like so:

\begin{equation}\label{eq:ex-lst-u-statistic-mean}
    \begin{aligned}
      U_n 
        &= \frac{1}{\binom{n}{1}} \sum_{i=1}^n y_i \\
        &= n^{-1} \sum_{i=1}^n y_i \\
    \end{aligned}
\end{equation}

The projection is thus:

\begin{equation}\label{eq:ex-lst-u-statistic-mean-projection}
    \begin{aligned}
      h_1(Y_1)
        &= \mathbb{E}[h(Y_1) | Y_1] \\
        &= \mathbb{E}[Y_1 | Y_1] \\
        &= Y_1 \\
      h_1^c(Y_1)
        &= h_1(Y_1) - \theta \\
        &= Y_1 - \mu \\
    \end{aligned}
\end{equation}

The \gls{if} of the kernel is given by:

\begin{equation}\label{eq:ex-lst-u-statistic-mean-if}
    \begin{aligned}
      IF_U(y_i)
        &= rh_1^c(Y_1) \\
        &= Y_1 - \mu \\
    \end{aligned}
\end{equation}

The asymptotic variance of the U-statistic is equal to the variance of the \gls{if}:

\begin{equation}\label{eq:ex-lst-u-statistic-mean-if-variance}
    \begin{aligned}
      \text{Var}(IF_U(y_i))
        &= \text{Var}(Y_1 - \mu) \\
        &= \text{Var}(Y_1) + \text{Var}(\mu) - 2\text{Cov}(Y_1, \mu) \\
        &= \sigma^2 + 0 - 2(\mathbb{E}[Y_1\mu] - \mathbb{E}[Y_1]\mathbb{E}[\mu]) \\
        &= \sigma^2 - 2(\mu\mu - \mu\mu) \\
        &= \sigma^2 \\
    \end{aligned}
\end{equation}

The asymptotic distribution of the U-statistic matches what we expect from the central limit theorem:

\begin{equation}\label{eq:ex-lst-u-statistic-mean-asymptotics}
    \sqrt{n}(U_n - \mu) \overset{d}{\to} \mathcal{N}(0, \sigma^2)
\end{equation}

Given the same sequence of random variables, suppose instead that we are interested in the asymptotic distribution of $\theta = \mu^2$. Our U-statistic can be defined as follows using the symmetric kernel $h(y_i, y_j) = y_iy_j$:

\begin{equation}\label{eq:ex-lst-u-staistic-m2}
    \begin{aligned}
      U_n 
        &= \frac{1}{\binom{n}{2}} \sum_{i<j}^n y_iy_j \\
        &= \frac{2}{n(n-1)} \sum_{i<j}^n y_iy_j \\
    \end{aligned}
\end{equation}

The projection is then:

\begin{equation}\label{eq:ex-lst-u-statistic-m2-projection}
    \begin{aligned}
      h_1(Y_1, Y_2)
        &= \mathbb{E}[h(Y_1, Y_2) | Y_1] \\
        &= \mathbb{E}[Y_1Y_2 | Y_1] \\
        &= \mu Y_1 \\
      h_1^c(Y_1, Y_2)
        &= h_1(Y_1, Y_2) - \theta \\
        &= \mu Y_1 - \mu^2 \\
    \end{aligned}
\end{equation}

The \gls{if} of the U-statistic is:

\begin{equation}\label{eq:ex-lst-u-statistic-m2-if}
    IF_U(y_i) = 2\mu Y_1 - 2\mu^2 
\end{equation}

Its variance is then:

\begin{equation}\label{eq:ex-lst-u-statistic-m2-if-variance}
    \begin{aligned}
      \text{Var}(IF_U(y_i))
        &= \text{Var}(2\mu Y_1 - 2\mu^2) \\
        &= \text{Var}(2\mu Y_1) + \text{Var}(2\mu^2) - 2\text{Cov}(2\mu Y_1, 2\mu^2) \\
        &= 4\mu^2 \text{Var}(Y_1) + 0 - 6\text{Cov}(\mu Y_1, \mu^2) \\
        &= 4\mu^2\sigma^2 - 6(\mathbb{E}[\mu Y_1 \mu^2] - \mathbb{E}[\mu Y_1]\mathbb{E}[\mu^2]) \\
        &= 4\mu^2\sigma^2 - 6(\mu^3\mathbb{E}[Y_1] - \mu^3\mathbb{E}[Y_1]) \\
        &= 4\mu^2\sigma^2 \\
    \end{aligned}
\end{equation}

The asymptotic distribution is thus (an alternate proof can be found in \cite[Chapter~5.5.2]{serfling_approximation_1980}):

\begin{equation}\label{eq:ex-lst-u-statistic-m2-asymptotics}
    \sqrt{n}(U_n - \mu^2) \overset{d}{\to} \mathcal{N}(0, 4\mu^2\sigma^2)
\end{equation}

Consider now a sequence of \glspl{rv} $X_1 \dots, X_n \sim F_X$. We'd like to develop a U-statistic for the sample variance, which we do using the symmetric kernel $h(x_i, x_j) = \frac{1}{2}(x_i - x_j)^2$:

\begin{equation}\label{eq:ex-lst-u-statistic-variance}
    \begin{aligned}
        U_n
          &= \frac{1}{\binom{n}{2}} \sum_{i=1}^n \sum_{i<j} \frac{1}{2} (X_i - X_j)^2 \\
          &= \frac{2(n-2)!}{n!} \sum_{i=1}^n \sum_{i<j} \frac{1}{2} \left(X_i^2 - 2X_iX_j + X_j^2\right) \\
          &= \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{i<j} X_i^2 - 2X_iX_j + X_j^2 \\
          &= \frac{1}{n(n-1)} \sum_{i=1}^n nX_i^2 -2X_i\sum_{i<j}X_j + \sum_{i<j}X_j^2 \\
          &= \frac{1}{n(n-1)} \sum_{i=1}^n nX_i^2 -2X_in\bar{X}_n + n\bar{X}_n^2 \\
          &= \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \bar{X}_n\right)^2 \\
    \end{aligned}
\end{equation}

The projection is thus as follows; note that we assume \gls{wlog} from here onward that the random variables $X_i$ have been centered and thus have zero mean:

\begin{equation}\label{eq:ex-lst-u-statistic-variance-projection}
    \begin{aligned}
        h_1(X_1, X_2)
          &= \mathbb{E}[h(X_1, X_2)|X_1] \\
          &= \mathbb{E}\left[\frac{1}{2}(X_1 - X_2)^2|X_1\right] \\
          &= \frac{1}{2}\mathbb{E}\left[X_1^2 - 2X_1X_2 + X_2^2|X_1\right] \\
          &= \frac{1}{2} (X_1^2 -2X_1\mu + m_2) \\
          &= \frac{1}{2} (X_1^2 + m_2) \\
        h_1^c(X_1, X_2)
          &= h_1(X_1, X_2) - \theta \\
          &= \frac{1}{2} (X_1^2 + m_2) - \sigma^2 \\
          &= \frac{X_1^2}{2} + \frac{m_2}{2} - (m_2 - m_1^2) \\
          &= \frac{X_1^2}{2} - \frac{m_2}{2} \\
    \end{aligned}
\end{equation}

The \gls{if} of our U-statistic of rank 2 is:

\begin{equation}\label{eq:ex-lst-u-statistic-variance-if}
    \begin{aligned}
        IF_U(X_i, X_j)
          &= rh_1^c(X_1, X_2) \\
          &= 2 \left(\frac{X_1^2}{2} - \frac{m_2}{2}\right) \\
          &= X_1^2 - m_2 \\
    \end{aligned}
\end{equation}

The variance of the \gls{if} is thus:

\begin{equation}\label{eq:ex-lst-u-statistic-variance-if-variance}
    \begin{aligned}
        \text{Var}\left(IF_U(X_i, X_j)\right)
          &= \text{Var}(X_1^2 - m_2) \\
          &= \text{Var}(X_1^2) + \text{Var}(m_2) - 2\text{Cov}(X_1^2, m_2) \\
          &= \mathbb{E}[X_1^4] - \left(\mathbb{E}[X_1^2]\right)^2 - 2\mathbb{E}[(X_1^2 - \mathbb{E}[X_1^2])(m_2 - \mathbb{E}[m_2])] \\
          &= m_4 - m_2^2 \\
    \end{aligned}
\end{equation}

Thus we have arrived at the asymptotic distribution of our U-statistic:

\begin{equation}\label{eq:ex-lst-u-statistic-variance-asymptotics}
    \sqrt{n}(U_n - \sigma^2) \overset{d}{\to} \mathcal{N}(0, m_4 - m_2^2)
\end{equation}

Suppose we observe a sequence of \glspl{rv} $Z_1, \dots, Z_n \sim F_Z$. We're interested in developing a V-statistic for the third raw moment, denoted $\theta = m_3$. Our symmetric kernel will be $h(z_i, z_j, z_k) = z_iz_jz_k$. The V-statistic of rank 3 can then be defined as:

\begin{equation}\label{eq:ex-lst-v-statistic-m3}
    V_n = n^{-3} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n Z_iZ_jZ_k
\end{equation}

The projection is thus:

\begin{equation}\label{eq:ex-lst-v-statistic-m3-projection}
    \begin{aligned}
        h_1(Z_1, Z_2, Z_3) 
          &= \mathbb{E}[h(Z_1, Z_2, Z_3 | Z_1] \\
          &= \mathbb{E}[Z_1Z_2Z_3 | Z_1] \\
          &= Z_1m_2 \\
        h_1^c(Z_1, Z_2, Z_3)
          &= h_1(Z_1, Z_2, Z_3) - \theta \\
          &= Z_1m_2 - m_3 \\
    \end{aligned}
\end{equation}

The \gls{if} for a V-statistic follows the same form as that of a U-statistic:

\begin{equation}\label{eq:ex-lst-v-statistic-m3-if}
    \begin{aligned}
        IF_V(Z_i, Z_j, Z_k)
          &= rh_1^c(Z_1, Z_2, Z_3) \\
          &= 3Z_1m_2 - 3m_3 \\
    \end{aligned}
\end{equation}

The variance of the \gls{if} is then:

\begin{equation}\label{eq:ex-lst-v-statistic-m3-if-variance}
    \begin{aligned}
        \text{Var}\left(IF_V(Z_i, Z_j, Z_k)\right)
          &= \text{Var}(3Z_1m_2 - 3m_3) \\
          &= \text{Var}(3Z_1m_2) + \text{Var}(3m_3) - 2\text{Cov}(3Z_1m_2, 3m_3) \\
          &= 9m_2^2\text{Var}(Z_1) + 0 - 18\text{Cov}(Z_1m_2, m_3) \\
          &= 9m_2^2\sigma^2 - 18\mathbb{E}[(Z_1m_2 - \mathbb{E}[Z_1m_2])(m_3 - \mathbb{E}[m_3])] \\
          &= 9m_2^2\sigma^2 \\
    \end{aligned}
\end{equation}

Thus the asymptotic distribution of our V-statistic is:

\begin{equation}\label{eq:ex-lst-v-statistic-m3-asymptotics}
    \sqrt{n}(V_n - m_3) \overset{d}{\to} \mathcal{N}(0, 9m_2^2\sigma^2)
\end{equation}

Assume now that we observe a sequence of \glspl{rv} $X_1, \dots, X_n \sim F_X$ with mean $\mu$ and variance $\sigma^2$. Assume \gls{wlog} that the variables have been centered i.e., that $m_1 = 0$. We're interested in identifying two U-statistics, one for the third raw moment $m_3$ and one for the variance; we'll denote these $U_n$ for and $U_n^*$, respectively. The first U-statistic is given by:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-m3}
    U_n = \frac{1}{\binom{n}{3}} \sum_{i=1}^n \sum_{i<j} \sum_{j<k} X_iX_jX_k
\end{equation}

The second is as follows; see Equation \ref{eq:ex-lst-u-statistic-variance} for an expanded definition:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-variance}
    U_n^* = \frac{1}{\binom{n}{2}} \sum_{i=1}^n \sum_{i<j} \frac{1}{2}(X_i - X_j)^2
\end{equation}

The projection of the kernel of $U_n$ is given by:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-m3-projection}
    \begin{aligned}
        h_1(X_1, X_2, X_3)
          &= \mathbb{E}[h(X_1, X_2, X_3) | X_1] \\
          &= \mathbb{E}[X_1X_2X_3 | X_1] \\
          &= X_1m_2 \\
        h_1^c(X_1, X_2, X_3)
          &= h_1(X_1, X_2, X_3) - \theta \\
          &= X_1m_2 - m_3 \\
    \end{aligned}
\end{equation}

The \gls{if} is then:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-m3-if}
    \begin{aligned}
        IF_U(X_i, X_j, X_k)
          &= rh_1^c(X_1, X_2, X_3) \\
          &= 3X_1m_2 - 3m_3 \\
    \end{aligned}
\end{equation}

The projection of the kernel of $U_n^*$ is:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-variance-projection}
    \begin{aligned}
        h_1(X_1, X_2)
          &= \mathbb{E}[h(X_1, X_2) | X_1] \\
          &= \mathbb{E}\left[\frac{1}{2}(X_1^2 - 2X_1X_2 + X_2^2\right] \\
          &= \frac{1}{2}X_1^2 - 2X_1m_1 + \frac{1}{2}m_2 \\
          &= \frac{1}{2}X_1^2 + \frac{1}{2}m_2 \\
        h_1^c(X_1, X_2)
          &= h_1(X_1, X_2) - \theta \\
          &= \frac{1}{2}X_1^2 + \frac{1}{2}m_2 - \sigma^2 \\
          &= \frac{1}{2}X_1^2 + \frac{1}{2}m_2 - (m_2 - m_1^2) \\
          &= \frac{1}{2}X_1^2 - \frac{1}{2}m_2 \\
    \end{aligned}
\end{equation}

The corresponding \gls{if} is:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-variance-if}
    \begin{aligned}
        IF_{U^*}(X_i, X_j)
          &= rh_1^c(X_1, X_2) \\
          &= 2 \left(\frac{1}{2}X_1^2 - \frac{1}{2}m_2\right) \\
          &= X_1^2 - m_2 \\
    \end{aligned}
\end{equation}

The variance of the first \gls{if} is defined as:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-m3-if-variance}
    \begin{aligned}
        \text{Var}\left(IF_U(X_i, X_j, X_k)\right)
          &= \text{Var}(3X_1m_2 - 3m_3) \\
          &= \text{Var}(3X_1m_2) + \text{Var}(3m_3) - 2\text{Cov}(3X_1m_2, 3m_3) \\
          &= 9m_2^2\text{Var}(X_1) + 0 - 18\mathbb{E}\left[(X_1m_2 - \mathbb{E}[X_1m_2])(m_3 - \mathbb{E}[m_3])\right] \\
          &= 9m_2^2\sigma^2 \\
    \end{aligned}
\end{equation}

The variance of the second \gls{if} is:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-variance-if-variance}
    \begin{aligned}
        \text{Var}\left(IF_{U^*}(X_i, X_j)\right)
          &= \text{Var}(X_1^2 - m_2) \\
          &= \text{Var}(X_1^2) + \text{Var}(m_2) - 2\text{Cov}(X_1^2, m_2) \\
          &= \mathbb{E}[X_1^4] - \left(\mathbb{E}[X_1^2]\right)^2 + 0 - 2\mathbb{E}\left[(X_1^2 - \mathbb{E}[X_1^2])(m_2 - \mathbb{E}[m_2])\right] \\
          &= m_4 - m_2^2 \\
    \end{aligned}
\end{equation}

Now for the tricky part - the covariance between the two \glspl{if} (keeping in mind that $m_1 = 0$):

\begin{equation}\label{eq:ex-lst-u-statistic-joint-if-covariance}
    \begin{aligned}
        \text{Cov}\left(IF_U, IF_{U^*}\right)
          &= \text{Cov}\left(3X_1m_2 - 3m_3, X_1^2 - m_2\right) \\
          &= \mathbb{E}\left[\left(3X_1m_2 - m_3 - \mathbb{E}[3X_1m_2 - 3m_3]\right)\left(X_1^2 - m_2 - \mathbb{E}[X_1^2 - m_2]\right)\right] \\ 
          &= \mathbb{E}\left[\left(3X_1m_2 - m_3 - 3m_2\mathbb{E}[X_1] + m_3\right)\left(X_1^2 - m_2 - m_2 + m_2\right)\right] \\
          &= \mathbb{E}\left[(3X_1m_2)(X_1^2 - m_2)\right] \\
          &= \mathbb{E}\left[3X_1^3m_2 - 3X_1m_2^2\right] \\
          &= 3m_2\mathbb{E}[X_1^3] - 3m_2^2\mathbb{E}[X_1] \\
          &= 3m_2m_3 \\
    \end{aligned}
\end{equation}

Thus we have arrived at the joint asymptotic distribution of the two U-statistics, assuming of course that the second, third, and fourth moments are finite:

\begin{equation}\label{eq:ex-lst-u-statistic-joint-asymptotics}
    \sqrt{n}\left(\begin{pmatrix}
        U_n \\
        U_n^*
    \end{pmatrix} - \begin{pmatrix}
        m_3 \\
        \sigma^2 \\
    \end{pmatrix}\right) \overset{d}{\to} \mathcal{N}\left(\begin{pmatrix}
        0 \\
        0 \\
    \end{pmatrix}, \begin{pmatrix}
        9m_2^2\sigma^2 & 3m_2m_3 \\
        3m_2m_3 & m_4 - m_2 \\
    \end{pmatrix}\right)
\end{equation}

\subsection{The jackknife}

The following problem is taken verbatim from a 2024 PHC 7066 midterm exam: write down two formulae for the jackknife estimator of variance of a statistic and prove their equality. We start with a few definitions:

\begin{equation}\label{eq:ex-lst-jackknife-defs}
    \begin{aligned}
        \hat{\theta}_{(i)} &= (n-1)^{-1} \sum_{j \neq i}^n x_j \\
        \hat{\theta}_{\text{jack}} &= n^{-1} \sum_{i=1}^n \hat{\theta}_{(i)} \\
        \Tilde{\theta}_i &= n\hat{\theta} - (n-1)\hat{\theta}_{(i)} \\
    \end{aligned}
\end{equation}

The first formula for the jackknife estimator of variance is:

\begin{equation}\label{eq:ex-lst-jackknife-variance-estimator-1}
    \widehat{\text{Var}}_{\text{jack}}\left(\hat{\theta}\right) = \frac{1}{n(n-1)} \sum_{i=1}^n \left(\Tilde{\theta}_i - \hat{\theta}_{\text{jack}}\right)^2
\end{equation}

This can be rewritten as:

\begin{equation}\label{eq:ex-lst-jackknife-variance-estimator-1-reformulated}
    \widehat{\text{Var}}_{\text{jack}}\left(\hat{\theta}\right) = \frac{n-1}{n} \sum_{i=1}^n \left(\hat{\theta}_{(i)} - n^{-1}\sum_{i=1}^n \hat{\theta}_{(i)}\right)^2
\end{equation}

The second formula is:

\begin{equation}\label{eq:ex-lst-jackknife-variance-estimator-2}
    \widehat{\text{Var}}_{\text{jack}}\left(\hat{\theta}\right) = \frac{1}{n(n-1)} \sum_{i=1}^n \left(n\hat{\theta} - (n-1)\hat{\theta}_{(i)} -n^{-1}\sum_{i=1}^n \left(n\hat{\theta} - (n-1)\hat{\theta}_{(i)}\right)\right)^2
\end{equation}

Rearranging the second formula reveals its equality with the first:

\begin{equation}\label{eq:ex-lst-jackknife-variance-estimator-2-reformulated}
    \begin{aligned}
        \widehat{\text{Var}}_{\text{jack}}\left(\hat{\theta}\right)
          &= \frac{1}{n(n-1)} \sum_{i=1}^n \left(n\hat{\theta} - (n-1)\hat{\theta}_{(i)} -n^{-1}\sum_{i=1}^n n\hat{\theta} + \frac{n-1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}\right)^2 \\
          &= \frac{n-1}{n} \sum_{i=1}^n \left(\hat{\theta}_{(i)} - n^{-1}\sum_{i=1}^n \hat{\theta}_{(i)}\right)^2 \\
    \end{aligned}
\end{equation}

\appendix

\chapter{Terminology}\label{chap:appendix}

% align glossary entires correctly
\glsfindwidesttoplevelname
\setglossarystyle{alttree}
% print acronyms in first section
\printglossary[type=\acronymtype]
% prevent bold text from being printed in math section
\renewcommand*{\glsnamefont}[1]{\textmd{#1}}
% print math symbols in second section
\printunsrtglossary[type=symbols]

\chapter{Software Packages} \label{chap:software}

Throughout this document I've included example code snippets showing how to apply theory in R. As such, I've made use of a variety of software packages beyond those specific to modeling that are listed in Tables \ref{table:bayes-software}, \ref{table:gee-software}, \ref{table:glmm-software}, and \ref{table:gam-software}. The rest of the packages are listed below, with links to the source code and brief descriptions of their functionality. 

\begin{table}[h]
\centering
\begin{tabular}{||l l||} 
 \hline
 \textbf{R Package} & \textbf{Functionality} \\ [0.5ex] 
  \hline\hline
  \href{https://cran.r-project.org/web/packages/dplyr/index.html}{\texttt{dplyr}} & Tidy manipulation of \texttt{data.frame} objects \\
  \hline
  \href{https://cran.r-project.org/web/packages/ggplot2/index.html}{\texttt{ggplot2}} & Extremely customizable generation of publication-quality plots \\
  \hline
  \href{https://cran.r-project.org/web/packages/ggh4x/index.html}{\texttt{ggh4x}} & Contains a variety of useful extensions to \texttt{ggplot2} \\
  \hline
  \href{https://cran.r-project.org/web/packages/tidyr/index.html}{\texttt{tidyr}} & Advanced data manipulation including pivoting \\
  \hline
  \href{https://cran.r-project.org/web/packages/purrr/index.html}{\texttt{purrr}} & Functional programming based on the map-reduce paradigm \\
  \hline
  \href{https://cran.r-project.org/web/packages/broom/index.html}{\texttt{broom}} & Summarization of model parameters for a wide variety of model classes \\
  \hline
  \href{https://cran.r-project.org/web/packages/broom.mixed/index.html}{\texttt{broom.mixed}} & The same as \texttt{broom} but for (generalized) linear mixed models \\
  \hline
  \href{https://cran.r-project.org/web/packages/faraway/index.html}{\texttt{faraway}} & Companion package to \cite{faraway_extending_2016} that includes many example datasets \\
  \hline
  \href{https://cran.r-project.org/web/packages/car/index.html}{\texttt{car}} & Implements a variety of functions related to regression model diagnostics \\
  \hline
  \href{https://cran.r-project.org/web/packages/tidymodels/index.html}{\texttt{tidymodels}} & Tidy interface to a wide variety of algorithms for classification and regression \\
  \hline
  \href{https://cran.r-project.org/web/packages/janitor/index.html}{\texttt{janitor}} & Contains functions related to data cleaning / standardization \\
  \hline
  \href{https://cran.r-project.org/web/packages/reticulate/index.html}{\texttt{reticulate}} & Interact with Python through R \\
  \hline
  \href{https://cran.r-project.org/web/packages/marginaleffects/index.html}{\texttt{marginaleffects}} & Tidily create summaries of model parameters and compare models \\
  \hline
  \href{https://cran.r-project.org/web/packages/patchwork/index.html}{\texttt{patchwork}} & Arithmeticallly combine \texttt{ggplot2} objects to create multi-panel figures \\
  \hline
  \href{https://cran.r-project.org/web/packages/readr/index.html}{\texttt{readr}} & Read pretty much any type of text file into R with intelligent type detection \\
  \hline
  \href{https://cran.r-project.org/web/packages/stargazer/index.html}{\texttt{stargazer}} & Create tabular summaries of model parameters for a wide variety of model classes \\
  \hline
  \href{https://cran.r-project.org/web/packages/MASS/index.html}{\texttt{MASS}} & Many functions related to model diagnostics and summarization \\
  \hline
  \href{https://cran.r-project.org/web/packages/pscl/index.html}{\texttt{pscl}} & Implements maximum likelihood estimation of count data models and tests of fit for GLMs \\
  \hline
\end{tabular}
\caption{Useful software packages.}
\label{table:useful-software}
\end{table}

\chapter*{Acknowledgements}

Writing this document wouldn't have been possible without some help. I'd like to thank my advisor Dr. Rhonda Bacher for teaching me to think scientifically, for proofreading my work, and for encouraging me to spend time studying while still making progress on my research. In addition, Dr. Xiaoru Dong has been a great help in telling me what I need to study for, and she proofread my notes as well. Lastly, I thank my friends for their encouragement and continual support. 

\newpage

\printbibliography

\end{document}
